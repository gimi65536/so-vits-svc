{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**改編自〈sovits3.0一键脚本（小狼躺平了，所以是深夜诗人修改版本,已更新32k/48k分支切换）.ipynb〉**\n",
    "\n",
    "https://colab.research.google.com/drive/1rCUOOVG7-XQlVZuWRAj5IpGrMM8t07pE?usp=sharing\n",
    "\n",
    "做了大幅(?)調整使其可用在vast.ai的機台上頭（運作映像為nvidia/cuda:12.0.1-runtime-ubuntu22.04 `00602486357e8c88c07157ddcd8fb2d7`），包含各種版本差異造成的坑。\n",
    "\n",
    "原Repo `innnky/so-vits-svc` 因為某些因素不見了，所以使用我自己的fork，預訓練模型採用 `TachibanaKimika/so-vits-svc-4.0-models` 提供的模型。\n",
    "\n",
    "註解繁中化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0gQcIZ8RsOkn",
    "outputId": "986cf91b-8d2f-4c39-a82b-f950eb2ddab5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar  7 09:20:33 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:03:00.0 Off |                  N/A |\n",
      "|  0%   42C    P8     4W / 420W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# 查看顯示卡\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root = Path('/root')\n",
    "sovits = root / 'so-vits-svc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LS0OPRkL4Pme",
    "outputId": "a52db68a-8feb-45a3-f0e1-165ab3e9d135",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'so-vits-svc' already exists and is not an empty directory.\n",
      "/root/so-vits-svc\n"
     ]
    }
   ],
   "source": [
    "#@title Clone github Repo\n",
    "!git clone https://github.com/gimi65536/so-vits-svc -b 4.0\n",
    "%cd {sovits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXBLkXxL4T1O",
    "outputId": "a4ad72ed-e57a-43a2-9f69-e15c7c5b9dc5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 安裝Dependencies\n",
    "!apt-get install build-essential zip -y\n",
    "!pip install pyworld praat-parselmouth librosa==0.9.2 fairseq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqf3W0d6ify",
    "outputId": "96d4b184-1c57-4a54-888b-5d9ad420838e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-07 09:24:14--  https://huggingface.co/innnky/contentvec/resolve/main/checkpoint_best_legacy_500.pt\n",
      "Resolving huggingface.co (huggingface.co)... 54.235.118.239, 3.231.67.228, 2600:1f18:147f:e800:671:b733:ecf3:a585, ...\n",
      "Connecting to huggingface.co (huggingface.co)|54.235.118.239|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/0a/5f/0a5f89ff9058782f4b7a207d38e0914d63f6f55e8fc1fad0c3bf68b546039f2d/60d936ec5a566776fc392e69ad8b630d14eb588111233fe313436e200a7b187b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27checkpoint_best_legacy_500.pt%3B+filename%3D%22checkpoint_best_legacy_500.pt%22%3B&Expires=1678439717&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzBhLzVmLzBhNWY4OWZmOTA1ODc4MmY0YjdhMjA3ZDM4ZTA5MTRkNjNmNmY1NWU4ZmMxZmFkMGMzYmY2OGI1NDYwMzlmMmQvNjBkOTM2ZWM1YTU2Njc3NmZjMzkyZTY5YWQ4YjYzMGQxNGViNTg4MTExMjMzZmUzMTM0MzZlMjAwYTdiMTg3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2Nzg0Mzk3MTd9fX1dfQ__&Signature=BAvG-sCqZjiIAWGy-huocNdAYnHD92npsJI-GI0yDVAe%7EuzNHEU6wSFkDxc-PMZAfp7At%7EsI-CK-8SFoCn4xt%7EFTKHRfF5tFPTx4pE6jZ%7ESml%7ElHTE8BojLC8aDUCahq47by4ebHE1yzJP7qqVCGjVLdMnwt-jnKNmvewzvoeEyWNUD1QDCNgKS9sshWKGIcATtwHZUoELdmT1rmyol6Zjeb17uQTHb4-przJEsPxtsw6BzqHG2OU15altUT5-lIkt%7EOqK5vamCMHrb8G48ZHJSG9ZkJSxeUo03aV5grJL%7EngzL80CtTxoV3AOXv-vYeayI3D%7Ehc1XOCk5iqHw0WAQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-03-07 09:24:14--  https://cdn-lfs.huggingface.co/repos/0a/5f/0a5f89ff9058782f4b7a207d38e0914d63f6f55e8fc1fad0c3bf68b546039f2d/60d936ec5a566776fc392e69ad8b630d14eb588111233fe313436e200a7b187b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27checkpoint_best_legacy_500.pt%3B+filename%3D%22checkpoint_best_legacy_500.pt%22%3B&Expires=1678439717&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzBhLzVmLzBhNWY4OWZmOTA1ODc4MmY0YjdhMjA3ZDM4ZTA5MTRkNjNmNmY1NWU4ZmMxZmFkMGMzYmY2OGI1NDYwMzlmMmQvNjBkOTM2ZWM1YTU2Njc3NmZjMzkyZTY5YWQ4YjYzMGQxNGViNTg4MTExMjMzZmUzMTM0MzZlMjAwYTdiMTg3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2Nzg0Mzk3MTd9fX1dfQ__&Signature=BAvG-sCqZjiIAWGy-huocNdAYnHD92npsJI-GI0yDVAe%7EuzNHEU6wSFkDxc-PMZAfp7At%7EsI-CK-8SFoCn4xt%7EFTKHRfF5tFPTx4pE6jZ%7ESml%7ElHTE8BojLC8aDUCahq47by4ebHE1yzJP7qqVCGjVLdMnwt-jnKNmvewzvoeEyWNUD1QDCNgKS9sshWKGIcATtwHZUoELdmT1rmyol6Zjeb17uQTHb4-przJEsPxtsw6BzqHG2OU15altUT5-lIkt%7EOqK5vamCMHrb8G48ZHJSG9ZkJSxeUo03aV5grJL%7EngzL80CtTxoV3AOXv-vYeayI3D%7Ehc1XOCk5iqHw0WAQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.67.17.2, 18.67.17.33, 18.67.17.96, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.67.17.2|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1330114945 (1.2G) [binary/octet-stream]\n",
      "Saving to: ‘hubert/checkpoint_best_legacy_500.pt’\n",
      "\n",
      "checkpoint_best_leg 100%[===================>]   1.24G  11.2MB/s    in 1m 57s  \n",
      "\n",
      "2023-03-07 09:26:11 (10.8 MB/s) - ‘hubert/checkpoint_best_legacy_500.pt’ saved [1330114945/1330114945]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 下載必要模型文件\n",
    "!wget -P hubert/ https://huggingface.co/TachibanaKimika/so-vits-svc-4.0-models/resolve/main/others/checkpoint_best_legacy_500.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1qadJBFehMo",
    "tags": []
   },
   "source": [
    "# 資料集預處理預處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBlju6Q3lSM6"
   },
   "source": [
    "該處理腳本可以一次性預處理多位角色，並且生成多角色的filelists以及對應的配置文件\n",
    "\n",
    "只需要將你的資料集按照以下文件結構放到dataset_raw資料夾下\n",
    "\n",
    "\n",
    "```\n",
    "dataset_raw\n",
    "├───speaker0\n",
    "│   ├───xxx1-xxx1.wav\n",
    "│   ├───...\n",
    "│   └───Lxx-0xx8.wav\n",
    "└───speaker1\n",
    "    ├───xx2-0xxx2.wav\n",
    "    ├───...\n",
    "    └───xxx7-xxx007.wav\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 設定訓練資料集備份與還原的路徑，預設為/root\n",
    "DATA_PATH = root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U05CXlAipvJR",
    "outputId": "b8f27c10-56c4-4cca-8153-584100d3d428",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /root/mika.zip\n",
      "   creating: /root/so-vits-svc/dataset_raw/mika_voice/\n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/157766.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/182536.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/183845.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/192622.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/195410.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/195990.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/199358.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/20150.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/206410.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/215752.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/215796.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/225893.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/229728.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/236088.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/244446.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/255426.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/260160.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/260281.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/292902.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/297728.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/32225.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/337536.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/340530.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/359512.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/397689.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/400669.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/401534.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/407600.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/410154.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/425690.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/432528.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/469511.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/475051.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/49584.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/502150.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/512932.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/517476.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/532468.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/550767.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/558568.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/565.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/577806.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/605948.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/608053.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/615422.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/648132.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/654282.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/666840.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/667651.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/667906.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/668781.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/672229.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/699708.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/705166.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/743939.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/756944.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/760191.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/772429.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/785306.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/807828.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/828619.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/835838.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/850726.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/865830.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/917575.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/950179.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/966920.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/966945.wav  \n",
      "  inflating: /root/so-vits-svc/dataset_raw/mika_voice/983491.wav  \n"
     ]
    }
   ],
   "source": [
    "# 資料集名稱（壓縮檔名稱不帶zip）\n",
    "DATASETNAME = \"mika\"  #@param {type:\"string\"}\n",
    "# 壓縮檔路徑\n",
    "ZIP_PATH = root\n",
    "ZIP_NAME = ZIP_PATH / DATASETNAME\n",
    "\n",
    "!unzip -d {sovits / 'dataset_raw'} {ZIP_NAME}.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ThKTzYs5CfL",
    "outputId": "72be4163-7c41-4dd3-e640-e45336e4d03a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset_raw/mika_voice\n",
      "69it [00:00, 264.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# 重取樣到44100Hz\n",
    "!python3 resample.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svITReeL5N8K",
    "outputId": "f10ae845-c205-4f7c-b702-a8ca6624d9be",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 159.01it/s]\n",
      "Writing ./filelists/train.txt\n",
      "100%|███████████████████████████████████████| 65/65 [00:00<00:00, 711827.05it/s]\n",
      "Writing ./filelists/val.txt\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 50840.05it/s]\n",
      "Writing ./filelists/test.txt\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 52758.54it/s]\n",
      "Writing configs/config.json\n"
     ]
    }
   ],
   "source": [
    "# 劃分訓練集、生成配置文件\n",
    "!python3 preprocess_flist_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHUXMi836DMe",
    "outputId": "9695de70-5259-499b-e9df-02985e0b0b3c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69]\n",
      "Loading hubert for content...\n",
      "load model(s) from hubert/checkpoint_best_legacy_500.pt\n",
      "DEBUG:git.cmd:Popen(['git', 'version'], cwd=/root/so-vits-svc, universal_newlines=False, shell=None, istream=None)\n",
      "DEBUG:git.cmd:Popen(['git', 'version'], cwd=/root/so-vits-svc, universal_newlines=False, shell=None, istream=None)\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.hubert_pretraining:current directory is /root/so-vits-svc\n",
      "INFO:fairseq.tasks.hubert_pretraining:HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'metadata', 'fine_tuning': False, 'labels': ['km'], 'label_dir': 'label', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "INFO:fairseq.models.hubert.hubert:HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n",
      "Loaded hubert.\n",
      "100%|███████████████████████████████████████████| 69/69 [00:52<00:00,  1.31it/s]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 生成hubert和f0\n",
    "!python3 preprocess_hubert_f0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wo4OTmTAUXgj",
    "outputId": "def5e777-819e-451d-ea69-05c2ae3ca59e",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: dataset/ (stored 0%)\n",
      "  adding: dataset/44k/ (stored 0%)\n",
      "  adding: dataset/44k/mika_voice/ (stored 0%)\n",
      "  adding: dataset/44k/mika_voice/183845.wav (deflated 16%)\n",
      "  adding: dataset/44k/mika_voice/195410.wav (deflated 6%)\n",
      "  adding: dataset/44k/mika_voice/157766.wav (deflated 60%)\n",
      "  adding: dataset/44k/mika_voice/199358.wav (deflated 4%)\n",
      "  adding: dataset/44k/mika_voice/182536.wav (deflated 17%)\n",
      "  adding: dataset/44k/mika_voice/20150.wav (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/195990.wav (deflated 3%)\n",
      "  adding: dataset/44k/mika_voice/215752.wav (deflated 4%)\n",
      "  adding: dataset/44k/mika_voice/206410.wav (deflated 25%)\n",
      "  adding: dataset/44k/mika_voice/236088.wav (deflated 5%)\n",
      "  adding: dataset/44k/mika_voice/225893.wav (deflated 2%)\n",
      "  adding: dataset/44k/mika_voice/229728.wav (deflated 5%)\n",
      "  adding: dataset/44k/mika_voice/192622.wav (deflated 26%)\n",
      "  adding: dataset/44k/mika_voice/244446.wav (deflated 36%)\n",
      "  adding: dataset/44k/mika_voice/215796.wav (deflated 4%)\n",
      "  adding: dataset/44k/mika_voice/260160.wav (deflated 43%)\n",
      "  adding: dataset/44k/mika_voice/297728.wav (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/32225.wav (deflated 40%)\n",
      "  adding: dataset/44k/mika_voice/337536.wav (deflated 30%)\n",
      "  adding: dataset/44k/mika_voice/292902.wav (deflated 32%)\n",
      "  adding: dataset/44k/mika_voice/255426.wav (deflated 28%)\n",
      "  adding: dataset/44k/mika_voice/397689.wav (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/359512.wav (deflated 2%)\n",
      "  adding: dataset/44k/mika_voice/400669.wav (deflated 6%)\n",
      "  adding: dataset/44k/mika_voice/340530.wav (deflated 23%)\n",
      "  adding: dataset/44k/mika_voice/425690.wav (deflated 1%)\n",
      "  adding: dataset/44k/mika_voice/469511.wav (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/260281.wav (deflated 43%)\n",
      "  adding: dataset/44k/mika_voice/432528.wav (deflated 15%)\n",
      "  adding: dataset/44k/mika_voice/401534.wav (deflated 47%)\n",
      "  adding: dataset/44k/mika_voice/502150.wav (deflated 6%)\n",
      "  adding: dataset/44k/mika_voice/475051.wav (deflated 6%)\n",
      "  adding: dataset/44k/mika_voice/49584.wav (deflated 15%)\n",
      "  adding: dataset/44k/mika_voice/512932.wav (deflated 4%)\n",
      "  adding: dataset/44k/mika_voice/517476.wav (deflated 3%)\n",
      "  adding: dataset/44k/mika_voice/407600.wav (deflated 28%)\n",
      "  adding: dataset/44k/mika_voice/532468.wav (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/605948.wav (deflated 21%)\n",
      "  adding: dataset/44k/mika_voice/565.wav (deflated 18%)\n",
      "  adding: dataset/44k/mika_voice/615422.wav (deflated 32%)\n",
      "  adding: dataset/44k/mika_voice/577806.wav (deflated 39%)\n",
      "  adding: dataset/44k/mika_voice/608053.wav (deflated 34%)\n",
      "  adding: dataset/44k/mika_voice/648132.wav (deflated 3%)\n",
      "  adding: dataset/44k/mika_voice/410154.wav (deflated 44%)\n",
      "  adding: dataset/44k/mika_voice/666840.wav (deflated 3%)\n",
      "  adding: dataset/44k/mika_voice/558568.wav (deflated 22%)\n",
      "  adding: dataset/44k/mika_voice/668781.wav (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/667906.wav (deflated 16%)\n",
      "  adding: dataset/44k/mika_voice/672229.wav (deflated 4%)\n",
      "  adding: dataset/44k/mika_voice/743939.wav (deflated 13%)\n",
      "  adding: dataset/44k/mika_voice/699708.wav (deflated 29%)\n",
      "  adding: dataset/44k/mika_voice/667651.wav (deflated 17%)\n",
      "  adding: dataset/44k/mika_voice/654282.wav (deflated 29%)\n",
      "  adding: dataset/44k/mika_voice/772429.wav (deflated 28%)\n",
      "  adding: dataset/44k/mika_voice/785306.wav (deflated 16%)\n",
      "  adding: dataset/44k/mika_voice/807828.wav (deflated 15%)\n",
      "  adding: dataset/44k/mika_voice/705166.wav (deflated 32%)\n",
      "  adding: dataset/44k/mika_voice/828619.wav (deflated 33%)\n",
      "  adding: dataset/44k/mika_voice/835838.wav (deflated 3%)\n",
      "  adding: dataset/44k/mika_voice/760191.wav (deflated 31%)\n",
      "  adding: dataset/44k/mika_voice/550767.wav (deflated 48%)\n",
      "  adding: dataset/44k/mika_voice/756944.wav (deflated 34%)\n",
      "  adding: dataset/44k/mika_voice/950179.wav (deflated 3%)\n",
      "  adding: dataset/44k/mika_voice/917575.wav (deflated 31%)\n",
      "  adding: dataset/44k/mika_voice/865830.wav (deflated 28%)\n",
      "  adding: dataset/44k/mika_voice/966920.wav (deflated 11%)\n",
      "  adding: dataset/44k/mika_voice/983491.wav (deflated 20%)\n",
      "  adding: dataset/44k/mika_voice/966945.wav (deflated 19%)\n",
      "  adding: dataset/44k/mika_voice/850726.wav (deflated 31%)\n",
      "  adding: dataset/44k/mika_voice/401534.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/401534.wav.f0.npy (deflated 70%)\n",
      "  adding: dataset/44k/mika_voice/608053.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/608053.wav.f0.npy (deflated 62%)\n",
      "  adding: dataset/44k/mika_voice/225893.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/225893.wav.f0.npy (deflated 32%)\n",
      "  adding: dataset/44k/mika_voice/20150.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/20150.wav.f0.npy (deflated 41%)\n",
      "  adding: dataset/44k/mika_voice/215752.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/215752.wav.f0.npy (deflated 50%)\n",
      "  adding: dataset/44k/mika_voice/835838.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/835838.wav.f0.npy (deflated 42%)\n",
      "  adding: dataset/44k/mika_voice/432528.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/432528.wav.f0.npy (deflated 74%)\n",
      "  adding: dataset/44k/mika_voice/532468.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/532468.wav.f0.npy (deflated 43%)\n",
      "  adding: dataset/44k/mika_voice/199358.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/199358.wav.f0.npy (deflated 35%)\n",
      "  adding: dataset/44k/mika_voice/244446.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/244446.wav.f0.npy (deflated 66%)\n",
      "  adding: dataset/44k/mika_voice/699708.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/699708.wav.f0.npy (deflated 50%)\n",
      "  adding: dataset/44k/mika_voice/615422.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/615422.wav.f0.npy (deflated 60%)\n",
      "  adding: dataset/44k/mika_voice/297728.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/297728.wav.f0.npy (deflated 32%)\n",
      "  adding: dataset/44k/mika_voice/292902.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/292902.wav.f0.npy (deflated 56%)\n",
      "  adding: dataset/44k/mika_voice/236088.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/236088.wav.f0.npy (deflated 88%)\n",
      "  adding: dataset/44k/mika_voice/49584.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/49584.wav.f0.npy (deflated 50%)\n",
      "  adding: dataset/44k/mika_voice/605948.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/605948.wav.f0.npy (deflated 52%)\n",
      "  adding: dataset/44k/mika_voice/667651.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/667651.wav.f0.npy (deflated 53%)\n",
      "  adding: dataset/44k/mika_voice/425690.wav.soft.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/425690.wav.f0.npy (deflated 55%)\n",
      "  adding: dataset/44k/mika_voice/756944.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/756944.wav.f0.npy (deflated 88%)\n",
      "  adding: dataset/44k/mika_voice/950179.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/950179.wav.f0.npy (deflated 22%)\n",
      "  adding: dataset/44k/mika_voice/410154.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/410154.wav.f0.npy (deflated 73%)\n",
      "  adding: dataset/44k/mika_voice/337536.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/337536.wav.f0.npy (deflated 54%)\n",
      "  adding: dataset/44k/mika_voice/215796.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/215796.wav.f0.npy (deflated 47%)\n",
      "  adding: dataset/44k/mika_voice/512932.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/512932.wav.f0.npy (deflated 24%)\n",
      "  adding: dataset/44k/mika_voice/192622.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/192622.wav.f0.npy (deflated 59%)\n",
      "  adding: dataset/44k/mika_voice/917575.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/917575.wav.f0.npy (deflated 58%)\n",
      "  adding: dataset/44k/mika_voice/183845.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/183845.wav.f0.npy (deflated 61%)\n",
      "  adding: dataset/44k/mika_voice/359512.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/359512.wav.f0.npy (deflated 67%)\n",
      "  adding: dataset/44k/mika_voice/469511.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/469511.wav.f0.npy (deflated 53%)\n",
      "  adding: dataset/44k/mika_voice/182536.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/182536.wav.f0.npy (deflated 43%)\n",
      "  adding: dataset/44k/mika_voice/157766.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/157766.wav.f0.npy (deflated 74%)\n",
      "  adding: dataset/44k/mika_voice/565.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/565.wav.f0.npy (deflated 59%)\n",
      "  adding: dataset/44k/mika_voice/966920.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/966920.wav.f0.npy (deflated 36%)\n",
      "  adding: dataset/44k/mika_voice/260160.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/260160.wav.f0.npy (deflated 64%)\n",
      "  adding: dataset/44k/mika_voice/772429.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/772429.wav.f0.npy (deflated 56%)\n",
      "  adding: dataset/44k/mika_voice/983491.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/983491.wav.f0.npy (deflated 70%)\n",
      "  adding: dataset/44k/mika_voice/195990.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/195990.wav.f0.npy (deflated 27%)\n",
      "  adding: dataset/44k/mika_voice/229728.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/229728.wav.f0.npy (deflated 51%)\n",
      "  adding: dataset/44k/mika_voice/255426.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/255426.wav.f0.npy (deflated 68%)\n",
      "  adding: dataset/44k/mika_voice/668781.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/668781.wav.f0.npy (deflated 45%)\n",
      "  adding: dataset/44k/mika_voice/260281.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/260281.wav.f0.npy (deflated 69%)\n",
      "  adding: dataset/44k/mika_voice/550767.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/550767.wav.f0.npy (deflated 76%)\n",
      "  adding: dataset/44k/mika_voice/206410.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/206410.wav.f0.npy (deflated 51%)\n",
      "  adding: dataset/44k/mika_voice/966945.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/966945.wav.f0.npy (deflated 56%)\n",
      "  adding: dataset/44k/mika_voice/654282.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/654282.wav.f0.npy (deflated 65%)\n",
      "  adding: dataset/44k/mika_voice/502150.wav.soft.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/502150.wav.f0.npy (deflated 47%)\n",
      "  adding: dataset/44k/mika_voice/865830.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/865830.wav.f0.npy (deflated 62%)\n",
      "  adding: dataset/44k/mika_voice/850726.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/850726.wav.f0.npy (deflated 61%)\n",
      "  adding: dataset/44k/mika_voice/577806.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/577806.wav.f0.npy (deflated 67%)\n",
      "  adding: dataset/44k/mika_voice/672229.wav.soft.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/672229.wav.f0.npy (deflated 52%)\n",
      "  adding: dataset/44k/mika_voice/667906.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/667906.wav.f0.npy (deflated 41%)\n",
      "  adding: dataset/44k/mika_voice/517476.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/517476.wav.f0.npy (deflated 53%)\n",
      "  adding: dataset/44k/mika_voice/558568.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/558568.wav.f0.npy (deflated 57%)\n",
      "  adding: dataset/44k/mika_voice/195410.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/195410.wav.f0.npy (deflated 34%)\n",
      "  adding: dataset/44k/mika_voice/407600.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/407600.wav.f0.npy (deflated 58%)\n",
      "  adding: dataset/44k/mika_voice/400669.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/400669.wav.f0.npy (deflated 67%)\n",
      "  adding: dataset/44k/mika_voice/743939.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/743939.wav.f0.npy (deflated 52%)\n",
      "  adding: dataset/44k/mika_voice/340530.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/340530.wav.f0.npy (deflated 55%)\n",
      "  adding: dataset/44k/mika_voice/397689.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/397689.wav.f0.npy (deflated 46%)\n",
      "  adding: dataset/44k/mika_voice/32225.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/32225.wav.f0.npy (deflated 56%)\n",
      "  adding: dataset/44k/mika_voice/648132.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/648132.wav.f0.npy (deflated 38%)\n",
      "  adding: dataset/44k/mika_voice/475051.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/475051.wav.f0.npy (deflated 35%)\n",
      "  adding: dataset/44k/mika_voice/705166.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/705166.wav.f0.npy (deflated 60%)\n",
      "  adding: dataset/44k/mika_voice/785306.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/785306.wav.f0.npy (deflated 51%)\n",
      "  adding: dataset/44k/mika_voice/666840.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/666840.wav.f0.npy (deflated 24%)\n",
      "  adding: dataset/44k/mika_voice/760191.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/760191.wav.f0.npy (deflated 61%)\n",
      "  adding: dataset/44k/mika_voice/807828.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/807828.wav.f0.npy (deflated 47%)\n",
      "  adding: dataset/44k/mika_voice/828619.wav.soft.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/828619.wav.f0.npy (deflated 67%)\n",
      "  adding: dataset/44k/mika_voice/615422.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/359512.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/666840.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/532468.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/983491.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/654282.spec.pt (deflated 11%)\n",
      "  adding: dataset/44k/mika_voice/756944.spec.pt (deflated 11%)\n",
      "  adding: dataset/44k/mika_voice/20150.spec.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/502150.spec.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/236088.spec.pt (deflated 12%)\n",
      "  adding: dataset/44k/mika_voice/195990.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/199358.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/469511.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/605948.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/337536.spec.pt (deflated 11%)\n",
      "  adding: dataset/44k/mika_voice/425690.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/966920.spec.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/672229.spec.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/835838.spec.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/297728.spec.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/608053.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/192622.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/410154.spec.pt (deflated 12%)\n",
      "  adding: dataset/44k/mika_voice/49584.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/565.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/260160.spec.pt (deflated 13%)\n",
      "  adding: dataset/44k/mika_voice/917575.spec.pt (deflated 11%)\n",
      "  adding: dataset/44k/mika_voice/512932.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/785306.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/215752.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/183845.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/195410.spec.pt (deflated 11%)\n",
      "  adding: dataset/44k/mika_voice/400669.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/807828.spec.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/667906.spec.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/206410.spec.pt (deflated 11%)\n",
      "  adding: dataset/44k/mika_voice/397689.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/182536.spec.pt (deflated 11%)\n",
      "  adding: dataset/44k/mika_voice/292902.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/255426.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/772429.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/577806.spec.pt (deflated 11%)\n",
      "  adding: dataset/44k/mika_voice/157766.spec.pt (deflated 14%)\n",
      "  adding: dataset/44k/mika_voice/401534.spec.pt (deflated 13%)\n",
      "  adding: dataset/44k/mika_voice/865830.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/760191.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/648132.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/225893.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/517476.spec.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/828619.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/699708.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/229728.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/743939.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/32225.spec.pt (deflated 12%)\n",
      "  adding: dataset/44k/mika_voice/850726.spec.pt (deflated 11%)\n",
      "  adding: dataset/44k/mika_voice/668781.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/260281.spec.pt (deflated 12%)\n",
      "  adding: dataset/44k/mika_voice/966945.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/550767.spec.pt (deflated 13%)\n",
      "  adding: dataset/44k/mika_voice/407600.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/558568.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/432528.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/950179.spec.pt (deflated 10%)\n",
      "  adding: dataset/44k/mika_voice/215796.spec.pt (deflated 8%)\n",
      "  adding: dataset/44k/mika_voice/667651.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/340530.spec.pt (deflated 9%)\n",
      "  adding: dataset/44k/mika_voice/244446.spec.pt (deflated 11%)\n"
     ]
    }
   ],
   "source": [
    "# 至此，數據集預處理製作完畢，將資料集和相關文件保存到so-vits-svc資料夾中並自行下載，方便下次訓練使用\n",
    "\n",
    "# 壓縮dataset資料夾\n",
    "!zip -r dataset.zip dataset\n",
    "# 自訂壓縮檔名稱（不含zip）\n",
    "dataset_name_drive = \"44kmika_dataset\"\n",
    "DATASET_PATH_DRIVE = DATA_PATH / dataset_name_drive\n",
    "!mkdir -p {DATASET_PATH_DRIVE}\n",
    "\n",
    "!cp dataset.zip \"{DATASET_PATH_DRIVE}\"\n",
    "!cp configs/config.json \"{DATASET_PATH_DRIVE}\"\n",
    "!cp filelists/train.txt \"{DATASET_PATH_DRIVE}\"\n",
    "!cp filelists/test.txt \"{DATASET_PATH_DRIVE}\"\n",
    "!cp filelists/val.txt \"{DATASET_PATH_DRIVE}\"\n",
    "\n",
    "# 自行下載壓縮檔{DATA_PATH / dataset_name_drive}.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2G6v_6zblWK",
    "outputId": "4668210c-bf35-435e-b01a-3567d3364f4e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /root/48kmika_dataset/dataset.zip\n",
      "replace /content/so-vits-svc/dataset/48k/mika_voice/183845.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "# 已經預處理過資料集的話，就可以跳過預處理部分，直接解壓縮處理過的資料以及配置文件\n",
    "\n",
    "# 壓縮檔名稱（不含zip）\n",
    "back_up_name = \"44kmika_dataset\"\n",
    "BACK_UP_DATASET_PATH = DATA_PATH / back_up_name\n",
    "!unzip {BACK_UP_DATASET_PATH / 'dataset.zip'} -d .\n",
    "!cp {BACK_UP_DATASET_PATH / 'config.json'} configs/config.json \n",
    "!cp {BACK_UP_DATASET_PATH / 'val.txt'} filelists/val.txt\n",
    "!cp {BACK_UP_DATASET_PATH / 'train.txt'} filelists/train.txt\n",
    "!cp {BACK_UP_DATASET_PATH / 'test.txt'} filelists/test.txt\n",
    "\n",
    "# 將之前訓練過的模型G_xx.pth與D_xx.pth放到 logs/44k 底下……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENoH-pShel7w"
   },
   "source": [
    "# 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l8J2ubh9KV5J",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-07 09:39:05--  https://huggingface.co/innnky/sovits_pretrained/resolve/main/sovits4/G_0.pth\n",
      "Resolving huggingface.co (huggingface.co)... 54.235.118.239, 3.231.67.228, 2600:1f18:147f:e800:671:b733:ecf3:a585, ...\n",
      "Connecting to huggingface.co (huggingface.co)|54.235.118.239|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/b8/68/b8680b5490e63bf5f140618c3ee2e5ccd0731956a87ab87a228c08398cd8e03c/20a327c54e5731bed377bd38404bc32ab98e66a1b2777b0af4cc034d4d6914b0?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27G_0.pth%3B+filename%3D%22G_0.pth%22%3B&Expires=1678441146&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2I4LzY4L2I4NjgwYjU0OTBlNjNiZjVmMTQwNjE4YzNlZTJlNWNjZDA3MzE5NTZhODdhYjg3YTIyOGMwODM5OGNkOGUwM2MvMjBhMzI3YzU0ZTU3MzFiZWQzNzdiZDM4NDA0YmMzMmFiOThlNjZhMWIyNzc3YjBhZjRjYzAzNGQ0ZDY5MTRiMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2Nzg0NDExNDZ9fX1dfQ__&Signature=oGpUOItJqLiWdV5VTnMFdFlUB6lc6z9OQ4IMSB%7EHtiPyIf4kUUiYFiZbE6gTInC5tlHRVPqbjBjGqwRBNJ37Izp-0jhqQfjYcGtsdScNHKlIek1QEMbv1L2o0KLgJsoZg-Z8Iq9sdgVHJ8LgNarTFVHMswV1T9Mp3M0wDoD2h9FM3LeV6hWIaWeN%7EU1QX3DZLpPcIqcEy5HPJczRPAkcpFjAihpOaz4h6YkZbXnudVm7BtPVfiFG8uZwOtPH45XSAUtbL93mI4u6uhUqysGDhY72KdYjGFwDTmtk8NIRtuGReE2SmwEPYlDdHccSrla6WIAO0chTs%7EZLXt4x0iObGA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-03-07 09:39:05--  https://cdn-lfs.huggingface.co/repos/b8/68/b8680b5490e63bf5f140618c3ee2e5ccd0731956a87ab87a228c08398cd8e03c/20a327c54e5731bed377bd38404bc32ab98e66a1b2777b0af4cc034d4d6914b0?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27G_0.pth%3B+filename%3D%22G_0.pth%22%3B&Expires=1678441146&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2I4LzY4L2I4NjgwYjU0OTBlNjNiZjVmMTQwNjE4YzNlZTJlNWNjZDA3MzE5NTZhODdhYjg3YTIyOGMwODM5OGNkOGUwM2MvMjBhMzI3YzU0ZTU3MzFiZWQzNzdiZDM4NDA0YmMzMmFiOThlNjZhMWIyNzc3YjBhZjRjYzAzNGQ0ZDY5MTRiMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2Nzg0NDExNDZ9fX1dfQ__&Signature=oGpUOItJqLiWdV5VTnMFdFlUB6lc6z9OQ4IMSB%7EHtiPyIf4kUUiYFiZbE6gTInC5tlHRVPqbjBjGqwRBNJ37Izp-0jhqQfjYcGtsdScNHKlIek1QEMbv1L2o0KLgJsoZg-Z8Iq9sdgVHJ8LgNarTFVHMswV1T9Mp3M0wDoD2h9FM3LeV6hWIaWeN%7EU1QX3DZLpPcIqcEy5HPJczRPAkcpFjAihpOaz4h6YkZbXnudVm7BtPVfiFG8uZwOtPH45XSAUtbL93mI4u6uhUqysGDhY72KdYjGFwDTmtk8NIRtuGReE2SmwEPYlDdHccSrla6WIAO0chTs%7EZLXt4x0iObGA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.67.17.126, 18.67.17.96, 18.67.17.2, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.67.17.126|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 180628517 (172M) [binary/octet-stream]\n",
      "Saving to: ‘logs/44k/G_0.pth’\n",
      "\n",
      "G_0.pth             100%[===================>] 172.26M  11.2MB/s    in 17s     \n",
      "\n",
      "2023-03-07 09:39:22 (10.4 MB/s) - ‘logs/44k/G_0.pth’ saved [180628517/180628517]\n",
      "\n",
      "--2023-03-07 09:39:23--  https://huggingface.co/innnky/sovits_pretrained/resolve/main/sovits4/D_0.pth\n",
      "Resolving huggingface.co (huggingface.co)... 3.231.67.228, 54.235.118.239, 2600:1f18:147f:e850:e203:c458:10cd:fc3c, ...\n",
      "Connecting to huggingface.co (huggingface.co)|3.231.67.228|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/b8/68/b8680b5490e63bf5f140618c3ee2e5ccd0731956a87ab87a228c08398cd8e03c/635be5c3409aaf3eec4135a1f5a771595683f3a6461ffc5bdea43441e50269a9?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27D_0.pth%3B+filename%3D%22D_0.pth%22%3B&Expires=1678441163&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2I4LzY4L2I4NjgwYjU0OTBlNjNiZjVmMTQwNjE4YzNlZTJlNWNjZDA3MzE5NTZhODdhYjg3YTIyOGMwODM5OGNkOGUwM2MvNjM1YmU1YzM0MDlhYWYzZWVjNDEzNWExZjVhNzcxNTk1NjgzZjNhNjQ2MWZmYzViZGVhNDM0NDFlNTAyNjlhOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2Nzg0NDExNjN9fX1dfQ__&Signature=XhMFs0vBSbSXj0pxY%7ERGTgcDQNTXiMzmfSjq6eGGzia8goN3j7sgbIit28t8RhrgORc0UuFRu8OJ1v%7ErxYz-lCfupd9NhQCEr0kIQj8blA8A2zt8n4ds72H9kV2HwpCApZiyJzE9%7EL3ByHmaQh0GteyCIIUnGN7AxGaUJNtFo2dsgt%7E59OzyzoP2wY9QKWcefZH8vlq2zmszD3uF%7EbePYB5ZAZEPbU0B80b5gjCkOYVAF21Yh72CBiRwrO7njiczB30-m8a10uzjL3DUY7XNrb8WJsMK9raN7JYKbHZCx0viHKS2SENcYIoLK7Tr09IOZfW9w3segb6N8kQd7Mo30w__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-03-07 09:39:23--  https://cdn-lfs.huggingface.co/repos/b8/68/b8680b5490e63bf5f140618c3ee2e5ccd0731956a87ab87a228c08398cd8e03c/635be5c3409aaf3eec4135a1f5a771595683f3a6461ffc5bdea43441e50269a9?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27D_0.pth%3B+filename%3D%22D_0.pth%22%3B&Expires=1678441163&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2I4LzY4L2I4NjgwYjU0OTBlNjNiZjVmMTQwNjE4YzNlZTJlNWNjZDA3MzE5NTZhODdhYjg3YTIyOGMwODM5OGNkOGUwM2MvNjM1YmU1YzM0MDlhYWYzZWVjNDEzNWExZjVhNzcxNTk1NjgzZjNhNjQ2MWZmYzViZGVhNDM0NDFlNTAyNjlhOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2Nzg0NDExNjN9fX1dfQ__&Signature=XhMFs0vBSbSXj0pxY%7ERGTgcDQNTXiMzmfSjq6eGGzia8goN3j7sgbIit28t8RhrgORc0UuFRu8OJ1v%7ErxYz-lCfupd9NhQCEr0kIQj8blA8A2zt8n4ds72H9kV2HwpCApZiyJzE9%7EL3ByHmaQh0GteyCIIUnGN7AxGaUJNtFo2dsgt%7E59OzyzoP2wY9QKWcefZH8vlq2zmszD3uF%7EbePYB5ZAZEPbU0B80b5gjCkOYVAF21Yh72CBiRwrO7njiczB30-m8a10uzjL3DUY7XNrb8WJsMK9raN7JYKbHZCx0viHKS2SENcYIoLK7Tr09IOZfW9w3segb6N8kQd7Mo30w__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.67.17.33, 18.67.17.2, 18.67.17.96, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.67.17.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 187018591 (178M) [binary/octet-stream]\n",
      "Saving to: ‘logs/44k/D_0.pth’\n",
      "\n",
      "D_0.pth             100%[===================>] 178.35M  11.2MB/s    in 16s     \n",
      "\n",
      "2023-03-07 09:39:39 (11.0 MB/s) - ‘logs/44k/D_0.pth’ saved [187018591/187018591]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 如果是首次訓練，則需事先下載預訓練模型\n",
    "!wget -P logs/44k/ https://huggingface.co/TachibanaKimika/so-vits-svc-4.0-models/resolve/main/others/G_0.pth\n",
    "!wget -P logs/44k/ https://huggingface.co/TachibanaKimika/so-vits-svc-4.0-models/resolve/main/others/D_0.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:44k:{'train': {'log_interval': 200, 'eval_interval': 800, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 6, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 10240, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 512, 'port': '8001', 'keep_ckpts': 0}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 44100, 'filter_length': 2048, 'hop_length': 512, 'win_length': 2048, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': 22050}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [8, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 256, 'n_speakers': 200}, 'spk': {'mika_voice': 0}, 'model_dir': './logs/44k'}\n",
      "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "./logs/44k/G_0.pth\n",
      "error, emb_g.weight is not in the checkpoint\n",
      "INFO:44k:emb_g.weight is not in the checkpoint\n",
      "load \n",
      "INFO:44k:Loaded checkpoint './logs/44k/G_0.pth' (iteration 0)\n",
      "./logs/44k/D_0.pth\n",
      "load \n",
      "INFO:44k:Loaded checkpoint './logs/44k/D_0.pth' (iteration 0)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:204: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [32, 1, 4], strides() = [4, 1, 1]\n",
      "bucket_view.sizes() = [32, 1, 4], strides() = [4, 4, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "INFO:44k:Train Epoch: 1 [0%]\n",
      "INFO:44k:Losses: [2.9138317108154297, 2.067965269088745, 11.388880729675293, 35.48935317993164, 6.664247512817383], step: 0, lr: 0.0001\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "INFO:44k:Saving model and optimizer state at iteration 1 to ./logs/44k/G_0.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1 to ./logs/44k/D_0.pth\n",
      "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
      "INFO:44k:====> Epoch: 1, cost 56.18 s\n",
      "INFO:44k:====> Epoch: 2, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 3, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 4, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 5, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 6, cost 16.17 s\n",
      "INFO:44k:====> Epoch: 7, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 8, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 9, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 10, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 11, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 12, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 13, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 14, cost 17.08 s\n",
      "INFO:44k:====> Epoch: 15, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 16, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 17, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 18, cost 16.44 s\n",
      "INFO:44k:Train Epoch: 19 [18%]\n",
      "INFO:44k:Losses: [2.3970155715942383, 2.305896520614624, 9.596781730651855, 12.560172080993652, 1.187543511390686], step: 200, lr: 9.977523890319963e-05\n",
      "INFO:44k:====> Epoch: 19, cost 17.87 s\n",
      "INFO:44k:====> Epoch: 20, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 21, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 22, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 23, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 24, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 25, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 26, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 27, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 28, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 29, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 30, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 31, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 32, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 33, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 34, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 35, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 36, cost 16.39 s\n",
      "INFO:44k:Train Epoch: 37 [36%]\n",
      "INFO:44k:Losses: [2.554015636444092, 2.374305009841919, 11.773859024047852, 17.32969856262207, 0.7483566999435425], step: 400, lr: 9.95509829819056e-05\n",
      "INFO:44k:====> Epoch: 37, cost 17.58 s\n",
      "INFO:44k:====> Epoch: 38, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 39, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 40, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 41, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 42, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 43, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 44, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 45, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 46, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 47, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 48, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 49, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 50, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 51, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 52, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 53, cost 16.20 s\n",
      "INFO:44k:====> Epoch: 54, cost 16.46 s\n",
      "INFO:44k:Train Epoch: 55 [55%]\n",
      "INFO:44k:Losses: [2.3504014015197754, 2.234138011932373, 10.484521865844727, 17.427165985107422, 1.0214942693710327], step: 600, lr: 9.932723110067987e-05\n",
      "INFO:44k:====> Epoch: 55, cost 17.48 s\n",
      "INFO:44k:====> Epoch: 56, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 57, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 58, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 59, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 60, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 61, cost 16.19 s\n",
      "INFO:44k:====> Epoch: 62, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 63, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 64, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 65, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 66, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 67, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 68, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 69, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 70, cost 16.20 s\n",
      "INFO:44k:====> Epoch: 71, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 72, cost 16.64 s\n",
      "INFO:44k:Train Epoch: 73 [73%]\n",
      "INFO:44k:Losses: [2.0961663722991943, 2.6512391567230225, 17.764820098876953, 24.953292846679688, 0.6168485283851624], step: 800, lr: 9.910398212663652e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 73 to ./logs/44k/G_800.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 73 to ./logs/44k/D_800.pth\n",
      "INFO:44k:====> Epoch: 73, cost 27.27 s\n",
      "INFO:44k:====> Epoch: 74, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 75, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 76, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 77, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 78, cost 16.12 s\n",
      "INFO:44k:====> Epoch: 79, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 80, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 81, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 82, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 83, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 84, cost 16.11 s\n",
      "INFO:44k:====> Epoch: 85, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 86, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 87, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 88, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 89, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 90, cost 16.29 s\n",
      "INFO:44k:Train Epoch: 91 [91%]\n",
      "INFO:44k:Losses: [2.3933465480804443, 2.101125478744507, 11.844046592712402, 18.998998641967773, 0.43067505955696106], step: 1000, lr: 9.888123492943583e-05\n",
      "INFO:44k:====> Epoch: 91, cost 17.65 s\n",
      "INFO:44k:====> Epoch: 92, cost 16.77 s\n",
      "INFO:44k:====> Epoch: 93, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 94, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 95, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 96, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 97, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 98, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 99, cost 16.15 s\n",
      "INFO:44k:====> Epoch: 100, cost 16.14 s\n",
      "INFO:44k:====> Epoch: 101, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 102, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 103, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 104, cost 16.13 s\n",
      "INFO:44k:====> Epoch: 105, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 106, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 107, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 108, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 109, cost 16.53 s\n",
      "INFO:44k:Train Epoch: 110 [9%]\n",
      "INFO:44k:Losses: [2.6100080013275146, 2.2732961177825928, 7.169862270355225, 16.325586318969727, 0.8306676149368286], step: 1200, lr: 9.864665600773098e-05\n",
      "INFO:44k:====> Epoch: 110, cost 17.74 s\n",
      "INFO:44k:====> Epoch: 111, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 112, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 113, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 114, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 115, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 116, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 117, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 118, cost 16.84 s\n",
      "INFO:44k:====> Epoch: 119, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 120, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 121, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 122, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 123, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 124, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 125, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 126, cost 17.04 s\n",
      "INFO:44k:====> Epoch: 127, cost 16.66 s\n",
      "INFO:44k:Train Epoch: 128 [27%]\n",
      "INFO:44k:Losses: [2.2920353412628174, 2.602768898010254, 17.054277420043945, 23.118574142456055, 0.7678318619728088], step: 1400, lr: 9.842493670173108e-05\n",
      "INFO:44k:====> Epoch: 128, cost 17.46 s\n",
      "INFO:44k:====> Epoch: 129, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 130, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 131, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 132, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 133, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 134, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 135, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 136, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 137, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 138, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 139, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 140, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 141, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 142, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 143, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 144, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 145, cost 16.28 s\n",
      "INFO:44k:Train Epoch: 146 [45%]\n",
      "INFO:44k:Losses: [1.9581904411315918, 2.766242265701294, 20.324586868286133, 24.266225814819336, 1.0265908241271973], step: 1600, lr: 9.820371573447515e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 146 to ./logs/44k/G_1600.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 146 to ./logs/44k/D_1600.pth\n",
      "INFO:44k:====> Epoch: 146, cost 27.05 s\n",
      "INFO:44k:====> Epoch: 147, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 148, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 149, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 150, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 151, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 152, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 153, cost 16.10 s\n",
      "INFO:44k:====> Epoch: 154, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 155, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 156, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 157, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 158, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 159, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 160, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 161, cost 16.15 s\n",
      "INFO:44k:====> Epoch: 162, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 163, cost 16.42 s\n",
      "INFO:44k:Train Epoch: 164 [64%]\n",
      "INFO:44k:Losses: [2.1765289306640625, 2.633765697479248, 15.542930603027344, 24.38896942138672, 0.6963006258010864], step: 1800, lr: 9.798299198589162e-05\n",
      "INFO:44k:====> Epoch: 164, cost 17.89 s\n",
      "INFO:44k:====> Epoch: 165, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 166, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 167, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 168, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 169, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 170, cost 16.14 s\n",
      "INFO:44k:====> Epoch: 171, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 172, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 173, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 174, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 175, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 176, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 177, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 178, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 179, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 180, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 181, cost 16.58 s\n",
      "INFO:44k:Train Epoch: 182 [82%]\n",
      "INFO:44k:Losses: [2.4909117221832275, 2.084465742111206, 10.175512313842773, 18.90746307373047, 0.9048758745193481], step: 2000, lr: 9.776276433842631e-05\n",
      "INFO:44k:====> Epoch: 182, cost 17.36 s\n",
      "INFO:44k:====> Epoch: 183, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 184, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 185, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 186, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 187, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 188, cost 16.79 s\n",
      "INFO:44k:====> Epoch: 189, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 190, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 191, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 192, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 193, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 194, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 195, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 196, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 197, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 198, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 199, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 200, cost 16.29 s\n",
      "INFO:44k:Train Epoch: 201 [0%]\n",
      "INFO:44k:Losses: [2.291978597640991, 2.379352569580078, 16.511568069458008, 20.27473258972168, 0.6612323522567749], step: 2200, lr: 9.753083879807726e-05\n",
      "INFO:44k:====> Epoch: 201, cost 17.45 s\n",
      "INFO:44k:====> Epoch: 202, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 203, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 204, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 205, cost 16.77 s\n",
      "INFO:44k:====> Epoch: 206, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 207, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 208, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 209, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 210, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 211, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 212, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 213, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 214, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 215, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 216, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 217, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 218, cost 16.60 s\n",
      "INFO:44k:Train Epoch: 219 [18%]\n",
      "INFO:44k:Losses: [2.2017061710357666, 2.426271438598633, 16.90521812438965, 20.74007797241211, 0.9171450138092041], step: 2400, lr: 9.731162741507607e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 219 to ./logs/44k/G_2400.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 219 to ./logs/44k/D_2400.pth\n",
      "INFO:44k:====> Epoch: 219, cost 27.09 s\n",
      "INFO:44k:====> Epoch: 220, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 221, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 222, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 223, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 224, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 225, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 226, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 227, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 228, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 229, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 230, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 231, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 232, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 233, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 234, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 235, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 236, cost 16.35 s\n",
      "INFO:44k:Train Epoch: 237 [36%]\n",
      "INFO:44k:Losses: [2.8005218505859375, 2.438260555267334, 9.691117286682129, 19.571834564208984, 0.64513099193573], step: 2600, lr: 9.709290873398365e-05\n",
      "INFO:44k:====> Epoch: 237, cost 17.46 s\n",
      "INFO:44k:====> Epoch: 238, cost 16.13 s\n",
      "INFO:44k:====> Epoch: 239, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 240, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 241, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 242, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 243, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 244, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 245, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 246, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 247, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 248, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 249, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 250, cost 16.17 s\n",
      "INFO:44k:====> Epoch: 251, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 252, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 253, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 254, cost 16.51 s\n",
      "INFO:44k:Train Epoch: 255 [55%]\n",
      "INFO:44k:Losses: [2.8504247665405273, 2.1059987545013428, 4.13408899307251, 14.024803161621094, 0.5440775752067566], step: 2800, lr: 9.687468164739773e-05\n",
      "INFO:44k:====> Epoch: 255, cost 17.58 s\n",
      "INFO:44k:====> Epoch: 256, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 257, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 258, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 259, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 260, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 261, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 262, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 263, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 264, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 265, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 266, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 267, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 268, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 269, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 270, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 271, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 272, cost 16.06 s\n",
      "INFO:44k:Train Epoch: 273 [73%]\n",
      "INFO:44k:Losses: [2.4840869903564453, 2.2133095264434814, 15.397595405578613, 20.255125045776367, 0.44869235157966614], step: 3000, lr: 9.665694505040515e-05\n",
      "INFO:44k:====> Epoch: 273, cost 17.30 s\n",
      "INFO:44k:====> Epoch: 274, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 275, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 276, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 277, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 278, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 279, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 280, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 281, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 282, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 283, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 284, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 285, cost 16.17 s\n",
      "INFO:44k:====> Epoch: 286, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 287, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 288, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 289, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 290, cost 16.52 s\n",
      "INFO:44k:Train Epoch: 291 [91%]\n",
      "INFO:44k:Losses: [2.289743185043335, 2.4823460578918457, 14.326077461242676, 21.587160110473633, 0.4598323106765747], step: 3200, lr: 9.643969784057613e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 291 to ./logs/44k/G_3200.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 291 to ./logs/44k/D_3200.pth\n",
      "INFO:44k:====> Epoch: 291, cost 26.93 s\n",
      "INFO:44k:====> Epoch: 292, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 293, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 294, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 295, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 296, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 297, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 298, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 299, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 300, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 301, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 302, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 303, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 304, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 305, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 306, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 307, cost 16.08 s\n",
      "INFO:44k:====> Epoch: 308, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 309, cost 16.19 s\n",
      "INFO:44k:Train Epoch: 310 [9%]\n",
      "INFO:44k:Losses: [2.695251703262329, 2.1575751304626465, 6.244716644287109, 12.816254615783691, 0.5780749917030334], step: 3400, lr: 9.621091105059392e-05\n",
      "INFO:44k:====> Epoch: 310, cost 17.64 s\n",
      "INFO:44k:====> Epoch: 311, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 312, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 313, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 314, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 315, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 316, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 317, cost 16.13 s\n",
      "INFO:44k:====> Epoch: 318, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 319, cost 16.17 s\n",
      "INFO:44k:====> Epoch: 320, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 321, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 322, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 323, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 324, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 325, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 326, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 327, cost 16.67 s\n",
      "INFO:44k:Train Epoch: 328 [27%]\n",
      "INFO:44k:Losses: [1.9511756896972656, 3.4109673500061035, 17.48959732055664, 20.14829444885254, 0.8219402432441711], step: 3600, lr: 9.599466635167497e-05\n",
      "INFO:44k:====> Epoch: 328, cost 17.56 s\n",
      "INFO:44k:====> Epoch: 329, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 330, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 331, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 332, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 333, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 334, cost 16.82 s\n",
      "INFO:44k:====> Epoch: 335, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 336, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 337, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 338, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 339, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 340, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 341, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 342, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 343, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 344, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 345, cost 16.58 s\n",
      "INFO:44k:Train Epoch: 346 [45%]\n",
      "INFO:44k:Losses: [2.0733656883239746, 2.8553872108459473, 21.07773780822754, 20.60511589050293, 0.5407649874687195], step: 3800, lr: 9.577890768671308e-05\n",
      "INFO:44k:====> Epoch: 346, cost 17.44 s\n",
      "INFO:44k:====> Epoch: 347, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 348, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 349, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 350, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 351, cost 16.07 s\n",
      "INFO:44k:====> Epoch: 352, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 353, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 354, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 355, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 356, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 357, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 358, cost 17.31 s\n",
      "INFO:44k:====> Epoch: 359, cost 32.27 s\n",
      "INFO:44k:====> Epoch: 360, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 361, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 362, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 363, cost 16.31 s\n",
      "INFO:44k:Train Epoch: 364 [64%]\n",
      "INFO:44k:Losses: [2.158217191696167, 2.706528663635254, 14.6389799118042, 21.9600830078125, 1.121217131614685], step: 4000, lr: 9.556363396329299e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 364 to ./logs/44k/G_4000.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 364 to ./logs/44k/D_4000.pth\n",
      "INFO:44k:====> Epoch: 364, cost 27.09 s\n",
      "INFO:44k:====> Epoch: 365, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 366, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 367, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 368, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 369, cost 16.17 s\n",
      "INFO:44k:====> Epoch: 370, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 371, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 372, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 373, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 374, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 375, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 376, cost 16.05 s\n",
      "INFO:44k:====> Epoch: 377, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 378, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 379, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 380, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 381, cost 16.82 s\n",
      "INFO:44k:Train Epoch: 382 [82%]\n",
      "INFO:44k:Losses: [2.360044240951538, 2.7036752700805664, 9.45746898651123, 18.819089889526367, 0.702570915222168], step: 4200, lr: 9.534884409145477e-05\n",
      "INFO:44k:====> Epoch: 382, cost 17.81 s\n",
      "INFO:44k:====> Epoch: 383, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 384, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 385, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 386, cost 16.10 s\n",
      "INFO:44k:====> Epoch: 387, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 388, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 389, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 390, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 391, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 392, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 393, cost 16.20 s\n",
      "INFO:44k:====> Epoch: 394, cost 16.19 s\n",
      "INFO:44k:====> Epoch: 395, cost 16.74 s\n",
      "INFO:44k:====> Epoch: 396, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 397, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 398, cost 16.86 s\n",
      "INFO:44k:====> Epoch: 399, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 400, cost 16.26 s\n",
      "INFO:44k:Train Epoch: 401 [0%]\n",
      "INFO:44k:Losses: [2.050278663635254, 2.503829002380371, 15.740026473999023, 20.63587188720703, 1.1157203912734985], step: 4400, lr: 9.512264516656537e-05\n",
      "INFO:44k:====> Epoch: 401, cost 17.52 s\n",
      "INFO:44k:====> Epoch: 402, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 403, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 404, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 405, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 406, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 407, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 408, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 409, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 410, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 411, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 412, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 413, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 414, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 415, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 416, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 417, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 418, cost 16.38 s\n",
      "INFO:44k:Train Epoch: 419 [18%]\n",
      "INFO:44k:Losses: [2.3598172664642334, 2.3360772132873535, 18.987903594970703, 17.70714569091797, 0.5518074631690979], step: 4600, lr: 9.490884646598347e-05\n",
      "INFO:44k:====> Epoch: 419, cost 17.58 s\n",
      "INFO:44k:====> Epoch: 420, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 421, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 422, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 423, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 424, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 425, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 426, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 427, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 428, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 429, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 430, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 431, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 432, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 433, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 434, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 435, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 436, cost 16.60 s\n",
      "INFO:44k:Train Epoch: 437 [36%]\n",
      "INFO:44k:Losses: [2.4258460998535156, 2.9277286529541016, 12.039257049560547, 20.4736270904541, 1.1235235929489136], step: 4800, lr: 9.469552830170594e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 437 to ./logs/44k/G_4800.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 437 to ./logs/44k/D_4800.pth\n",
      "INFO:44k:====> Epoch: 437, cost 27.15 s\n",
      "INFO:44k:====> Epoch: 438, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 439, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 440, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 441, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 442, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 443, cost 16.74 s\n",
      "INFO:44k:====> Epoch: 444, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 445, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 446, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 447, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 448, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 449, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 450, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 451, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 452, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 453, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 454, cost 16.61 s\n",
      "INFO:44k:Train Epoch: 455 [55%]\n",
      "INFO:44k:Losses: [2.448087215423584, 2.150088310241699, 9.363846778869629, 16.0568904876709, 0.2908785045146942], step: 5000, lr: 9.448268959367411e-05\n",
      "INFO:44k:====> Epoch: 455, cost 17.35 s\n",
      "INFO:44k:====> Epoch: 456, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 457, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 458, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 459, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 460, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 461, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 462, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 463, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 464, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 465, cost 16.84 s\n",
      "INFO:44k:====> Epoch: 466, cost 16.99 s\n",
      "INFO:44k:====> Epoch: 467, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 468, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 469, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 470, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 471, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 472, cost 16.43 s\n",
      "INFO:44k:Train Epoch: 473 [73%]\n",
      "INFO:44k:Losses: [1.9932080507278442, 2.8139119148254395, 15.114035606384277, 19.475513458251953, 0.8279477953910828], step: 5200, lr: 9.427032926425684e-05\n",
      "INFO:44k:====> Epoch: 473, cost 17.56 s\n",
      "INFO:44k:====> Epoch: 474, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 475, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 476, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 477, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 478, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 479, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 480, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 481, cost 16.91 s\n",
      "INFO:44k:====> Epoch: 482, cost 21.14 s\n",
      "INFO:44k:====> Epoch: 483, cost 17.21 s\n",
      "INFO:44k:====> Epoch: 484, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 485, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 486, cost 18.36 s\n",
      "INFO:44k:====> Epoch: 487, cost 21.59 s\n",
      "INFO:44k:====> Epoch: 488, cost 19.51 s\n",
      "INFO:44k:====> Epoch: 489, cost 19.15 s\n",
      "INFO:44k:====> Epoch: 490, cost 18.85 s\n",
      "INFO:44k:Train Epoch: 491 [91%]\n",
      "INFO:44k:Losses: [2.58504581451416, 1.9233243465423584, 8.780613899230957, 13.464418411254883, 0.48538488149642944], step: 5400, lr: 9.405844623824521e-05\n",
      "INFO:44k:====> Epoch: 491, cost 17.19 s\n",
      "INFO:44k:====> Epoch: 492, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 493, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 494, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 495, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 496, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 497, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 498, cost 16.12 s\n",
      "INFO:44k:====> Epoch: 499, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 500, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 501, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 502, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 503, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 504, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 505, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 506, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 507, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 508, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 509, cost 16.09 s\n",
      "INFO:44k:Train Epoch: 510 [9%]\n",
      "INFO:44k:Losses: [2.5096640586853027, 2.426612615585327, 10.133868217468262, 18.06202507019043, 0.6816455721855164], step: 5600, lr: 9.383530856291636e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 510 to ./logs/44k/G_5600.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 510 to ./logs/44k/D_5600.pth\n",
      "INFO:44k:====> Epoch: 510, cost 27.01 s\n",
      "INFO:44k:====> Epoch: 511, cost 17.45 s\n",
      "INFO:44k:====> Epoch: 512, cost 20.45 s\n",
      "INFO:44k:====> Epoch: 513, cost 19.23 s\n",
      "INFO:44k:====> Epoch: 514, cost 19.17 s\n",
      "INFO:44k:====> Epoch: 515, cost 18.89 s\n",
      "INFO:44k:====> Epoch: 516, cost 17.15 s\n",
      "INFO:44k:====> Epoch: 517, cost 16.81 s\n",
      "INFO:44k:====> Epoch: 518, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 519, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 520, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 521, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 522, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 523, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 524, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 525, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 526, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 527, cost 16.42 s\n",
      "INFO:44k:Train Epoch: 528 [27%]\n",
      "INFO:44k:Losses: [1.6460939645767212, 2.8554439544677734, 19.46371078491211, 18.907567977905273, 0.1157488226890564], step: 5800, lr: 9.362440329420433e-05\n",
      "INFO:44k:====> Epoch: 528, cost 17.63 s\n",
      "INFO:44k:====> Epoch: 529, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 530, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 531, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 532, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 533, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 534, cost 17.62 s\n",
      "INFO:44k:====> Epoch: 535, cost 20.85 s\n",
      "INFO:44k:====> Epoch: 536, cost 19.09 s\n",
      "INFO:44k:====> Epoch: 537, cost 19.13 s\n",
      "INFO:44k:====> Epoch: 538, cost 18.92 s\n",
      "INFO:44k:====> Epoch: 539, cost 17.73 s\n",
      "INFO:44k:====> Epoch: 540, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 541, cost 16.12 s\n",
      "INFO:44k:====> Epoch: 542, cost 16.20 s\n",
      "INFO:44k:====> Epoch: 543, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 544, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 545, cost 16.52 s\n",
      "INFO:44k:Train Epoch: 546 [45%]\n",
      "INFO:44k:Losses: [1.837077021598816, 2.9129700660705566, 18.376001358032227, 19.273395538330078, 0.8558246493339539], step: 6000, lr: 9.341397205848746e-05\n",
      "INFO:44k:====> Epoch: 546, cost 18.65 s\n",
      "INFO:44k:====> Epoch: 547, cost 21.19 s\n",
      "INFO:44k:====> Epoch: 548, cost 18.89 s\n",
      "INFO:44k:====> Epoch: 549, cost 19.12 s\n",
      "INFO:44k:====> Epoch: 550, cost 19.29 s\n",
      "INFO:44k:====> Epoch: 551, cost 16.94 s\n",
      "INFO:44k:====> Epoch: 552, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 553, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 554, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 555, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 556, cost 16.08 s\n",
      "INFO:44k:====> Epoch: 557, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 558, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 559, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 560, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 561, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 562, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 563, cost 16.36 s\n",
      "INFO:44k:Train Epoch: 564 [64%]\n",
      "INFO:44k:Losses: [1.9057602882385254, 2.951598644256592, 18.7841854095459, 22.562419891357422, 1.1081596612930298], step: 6200, lr: 9.320401379032397e-05\n",
      "INFO:44k:====> Epoch: 564, cost 17.48 s\n",
      "INFO:44k:====> Epoch: 565, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 566, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 567, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 568, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 569, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 570, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 571, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 572, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 573, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 574, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 575, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 576, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 577, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 578, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 579, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 580, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 581, cost 16.38 s\n",
      "INFO:44k:Train Epoch: 582 [82%]\n",
      "INFO:44k:Losses: [2.6058127880096436, 2.2887392044067383, 11.117876052856445, 18.445871353149414, 0.758009672164917], step: 6400, lr: 9.299452742666683e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 582 to ./logs/44k/G_6400.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 582 to ./logs/44k/D_6400.pth\n",
      "INFO:44k:====> Epoch: 582, cost 27.23 s\n",
      "INFO:44k:====> Epoch: 583, cost 16.19 s\n",
      "INFO:44k:====> Epoch: 584, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 585, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 586, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 587, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 588, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 589, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 590, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 591, cost 16.87 s\n",
      "INFO:44k:====> Epoch: 592, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 593, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 594, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 595, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 596, cost 16.19 s\n",
      "INFO:44k:====> Epoch: 597, cost 16.07 s\n",
      "INFO:44k:====> Epoch: 598, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 599, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 600, cost 16.24 s\n",
      "INFO:44k:Train Epoch: 601 [0%]\n",
      "INFO:44k:Losses: [2.1244425773620605, 2.7699716091156006, 12.952387809753418, 19.070104598999023, 0.6114813089370728], step: 6600, lr: 9.277391371786995e-05\n",
      "INFO:44k:====> Epoch: 601, cost 17.72 s\n",
      "INFO:44k:====> Epoch: 602, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 603, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 604, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 605, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 606, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 607, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 608, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 609, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 610, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 611, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 612, cost 16.82 s\n",
      "INFO:44k:====> Epoch: 613, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 614, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 615, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 616, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 617, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 618, cost 16.32 s\n",
      "INFO:44k:Train Epoch: 619 [18%]\n",
      "INFO:44k:Losses: [2.148380756378174, 2.627739906311035, 18.02496337890625, 18.438844680786133, 0.5030256509780884], step: 6800, lr: 9.2565394051853e-05\n",
      "INFO:44k:====> Epoch: 619, cost 17.47 s\n",
      "INFO:44k:====> Epoch: 620, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 621, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 622, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 623, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 624, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 625, cost 16.20 s\n",
      "INFO:44k:====> Epoch: 626, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 627, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 628, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 629, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 630, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 631, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 632, cost 17.69 s\n",
      "INFO:44k:====> Epoch: 633, cost 18.45 s\n",
      "INFO:44k:====> Epoch: 634, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 635, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 636, cost 16.30 s\n",
      "INFO:44k:Train Epoch: 637 [36%]\n",
      "INFO:44k:Losses: [2.1383628845214844, 3.2334370613098145, 15.86087417602539, 15.203852653503418, 0.6550137996673584], step: 7000, lr: 9.235734305692444e-05\n",
      "INFO:44k:====> Epoch: 637, cost 17.70 s\n",
      "INFO:44k:====> Epoch: 638, cost 16.90 s\n",
      "INFO:44k:====> Epoch: 639, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 640, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 641, cost 16.81 s\n",
      "INFO:44k:====> Epoch: 642, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 643, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 644, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 645, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 646, cost 16.77 s\n",
      "INFO:44k:====> Epoch: 647, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 648, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 649, cost 16.82 s\n",
      "INFO:44k:====> Epoch: 650, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 651, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 652, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 653, cost 16.87 s\n",
      "INFO:44k:====> Epoch: 654, cost 16.42 s\n",
      "INFO:44k:Train Epoch: 655 [55%]\n",
      "INFO:44k:Losses: [2.127194881439209, 2.9262917041778564, 14.683670997619629, 19.481367111206055, 0.6922681331634521], step: 7200, lr: 9.214975967969402e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 655 to ./logs/44k/G_7200.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 655 to ./logs/44k/D_7200.pth\n",
      "INFO:44k:====> Epoch: 655, cost 27.58 s\n",
      "INFO:44k:====> Epoch: 656, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 657, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 658, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 659, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 660, cost 17.06 s\n",
      "INFO:44k:====> Epoch: 661, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 662, cost 16.79 s\n",
      "INFO:44k:====> Epoch: 663, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 664, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 665, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 666, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 667, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 668, cost 16.91 s\n",
      "INFO:44k:====> Epoch: 669, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 670, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 671, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 672, cost 16.45 s\n",
      "INFO:44k:Train Epoch: 673 [73%]\n",
      "INFO:44k:Losses: [2.21840238571167, 2.5150082111358643, 15.468832969665527, 17.94184684753418, 0.8317580819129944], step: 7400, lr: 9.194264286913901e-05\n",
      "INFO:44k:====> Epoch: 673, cost 17.71 s\n",
      "INFO:44k:====> Epoch: 674, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 675, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 676, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 677, cost 16.83 s\n",
      "INFO:44k:====> Epoch: 678, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 679, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 680, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 681, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 682, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 683, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 684, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 685, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 686, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 687, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 688, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 689, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 690, cost 16.84 s\n",
      "INFO:44k:Train Epoch: 691 [91%]\n",
      "INFO:44k:Losses: [2.0985801219940186, 2.596353054046631, 13.371600151062012, 18.800058364868164, 0.7836315631866455], step: 7600, lr: 9.173599157659907e-05\n",
      "INFO:44k:====> Epoch: 691, cost 17.72 s\n",
      "INFO:44k:====> Epoch: 692, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 693, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 694, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 695, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 696, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 697, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 698, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 699, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 700, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 701, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 702, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 703, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 704, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 705, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 706, cost 16.99 s\n",
      "INFO:44k:====> Epoch: 707, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 708, cost 17.22 s\n",
      "INFO:44k:====> Epoch: 709, cost 16.34 s\n",
      "INFO:44k:Train Epoch: 710 [9%]\n",
      "INFO:44k:Losses: [2.3677895069122314, 2.6030585765838623, 11.619811058044434, 15.887225151062012, 0.3631344139575958], step: 7800, lr: 9.151836353017629e-05\n",
      "INFO:44k:====> Epoch: 710, cost 17.48 s\n",
      "INFO:44k:====> Epoch: 711, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 712, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 713, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 714, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 715, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 716, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 717, cost 16.96 s\n",
      "INFO:44k:====> Epoch: 718, cost 17.02 s\n",
      "INFO:44k:====> Epoch: 719, cost 19.03 s\n",
      "INFO:44k:====> Epoch: 720, cost 21.30 s\n",
      "INFO:44k:====> Epoch: 721, cost 19.10 s\n",
      "INFO:44k:====> Epoch: 722, cost 19.06 s\n",
      "INFO:44k:====> Epoch: 723, cost 18.76 s\n",
      "INFO:44k:====> Epoch: 724, cost 18.61 s\n",
      "INFO:44k:====> Epoch: 725, cost 20.35 s\n",
      "INFO:44k:====> Epoch: 726, cost 16.86 s\n",
      "INFO:44k:====> Epoch: 727, cost 16.48 s\n",
      "INFO:44k:Train Epoch: 728 [27%]\n",
      "INFO:44k:Losses: [1.857804298400879, 2.994304895401001, 17.707277297973633, 16.159311294555664, 0.4802815616130829], step: 8000, lr: 9.13126658525321e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 728 to ./logs/44k/G_8000.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 728 to ./logs/44k/D_8000.pth\n",
      "INFO:44k:====> Epoch: 728, cost 27.59 s\n",
      "INFO:44k:====> Epoch: 729, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 730, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 731, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 732, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 733, cost 16.82 s\n",
      "INFO:44k:====> Epoch: 734, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 735, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 736, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 737, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 738, cost 17.32 s\n",
      "INFO:44k:====> Epoch: 739, cost 20.84 s\n",
      "INFO:44k:====> Epoch: 740, cost 18.94 s\n",
      "INFO:44k:====> Epoch: 741, cost 19.10 s\n",
      "INFO:44k:====> Epoch: 742, cost 19.11 s\n",
      "INFO:44k:====> Epoch: 743, cost 18.35 s\n",
      "INFO:44k:====> Epoch: 744, cost 16.91 s\n",
      "INFO:44k:====> Epoch: 745, cost 17.33 s\n",
      "INFO:44k:Train Epoch: 746 [45%]\n",
      "INFO:44k:Losses: [2.0488080978393555, 2.556539535522461, 19.142192840576172, 19.79071617126465, 0.7712822556495667], step: 8200, lr: 9.110743050324427e-05\n",
      "INFO:44k:====> Epoch: 746, cost 18.44 s\n",
      "INFO:44k:====> Epoch: 747, cost 17.24 s\n",
      "INFO:44k:====> Epoch: 748, cost 17.29 s\n",
      "INFO:44k:====> Epoch: 749, cost 18.33 s\n",
      "INFO:44k:====> Epoch: 750, cost 21.21 s\n",
      "INFO:44k:====> Epoch: 751, cost 19.18 s\n",
      "INFO:44k:====> Epoch: 752, cost 19.16 s\n",
      "INFO:44k:====> Epoch: 753, cost 18.92 s\n",
      "INFO:44k:====> Epoch: 754, cost 17.22 s\n",
      "INFO:44k:====> Epoch: 755, cost 16.81 s\n",
      "INFO:44k:====> Epoch: 756, cost 17.16 s\n",
      "INFO:44k:====> Epoch: 757, cost 17.10 s\n",
      "INFO:44k:====> Epoch: 758, cost 17.02 s\n",
      "INFO:44k:====> Epoch: 759, cost 16.81 s\n",
      "INFO:44k:====> Epoch: 760, cost 18.03 s\n",
      "INFO:44k:====> Epoch: 761, cost 20.87 s\n",
      "INFO:44k:====> Epoch: 762, cost 19.28 s\n",
      "INFO:44k:====> Epoch: 763, cost 19.69 s\n",
      "INFO:44k:Train Epoch: 764 [64%]\n",
      "INFO:44k:Losses: [2.0638926029205322, 3.022202730178833, 13.883447647094727, 19.376848220825195, 1.0753350257873535], step: 8400, lr: 9.09026564431785e-05\n",
      "INFO:44k:====> Epoch: 764, cost 20.35 s\n",
      "INFO:44k:====> Epoch: 765, cost 17.30 s\n",
      "INFO:44k:====> Epoch: 766, cost 17.12 s\n",
      "INFO:44k:====> Epoch: 767, cost 16.94 s\n",
      "INFO:44k:====> Epoch: 768, cost 16.88 s\n",
      "INFO:44k:====> Epoch: 769, cost 17.10 s\n",
      "INFO:44k:====> Epoch: 770, cost 17.09 s\n",
      "INFO:44k:====> Epoch: 771, cost 18.39 s\n",
      "INFO:44k:====> Epoch: 772, cost 21.15 s\n",
      "INFO:44k:====> Epoch: 773, cost 18.80 s\n",
      "INFO:44k:====> Epoch: 774, cost 18.77 s\n",
      "INFO:44k:====> Epoch: 775, cost 18.78 s\n",
      "INFO:44k:====> Epoch: 776, cost 17.19 s\n",
      "INFO:44k:====> Epoch: 777, cost 17.14 s\n",
      "INFO:44k:====> Epoch: 778, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 779, cost 17.09 s\n",
      "INFO:44k:====> Epoch: 780, cost 17.33 s\n",
      "INFO:44k:====> Epoch: 781, cost 17.11 s\n",
      "INFO:44k:Train Epoch: 782 [82%]\n",
      "INFO:44k:Losses: [2.1382594108581543, 2.8122775554656982, 13.52231216430664, 19.874473571777344, 0.729682445526123], step: 8600, lr: 9.069834263553609e-05\n",
      "INFO:44k:====> Epoch: 782, cost 19.98 s\n",
      "INFO:44k:====> Epoch: 783, cost 21.52 s\n",
      "INFO:44k:====> Epoch: 784, cost 19.07 s\n",
      "INFO:44k:====> Epoch: 785, cost 19.37 s\n",
      "INFO:44k:====> Epoch: 786, cost 18.90 s\n",
      "INFO:44k:====> Epoch: 787, cost 17.23 s\n",
      "INFO:44k:====> Epoch: 788, cost 16.92 s\n",
      "INFO:44k:====> Epoch: 789, cost 17.18 s\n",
      "INFO:44k:====> Epoch: 790, cost 17.00 s\n",
      "INFO:44k:====> Epoch: 791, cost 16.85 s\n",
      "INFO:44k:====> Epoch: 792, cost 17.42 s\n",
      "INFO:44k:====> Epoch: 793, cost 19.89 s\n",
      "INFO:44k:====> Epoch: 794, cost 21.03 s\n",
      "INFO:44k:====> Epoch: 795, cost 19.02 s\n",
      "INFO:44k:====> Epoch: 796, cost 19.10 s\n",
      "INFO:44k:====> Epoch: 797, cost 18.77 s\n",
      "INFO:44k:====> Epoch: 798, cost 17.12 s\n",
      "INFO:44k:====> Epoch: 799, cost 17.21 s\n",
      "INFO:44k:====> Epoch: 800, cost 16.78 s\n",
      "INFO:44k:Train Epoch: 801 [0%]\n",
      "INFO:44k:Losses: [1.7147941589355469, 2.977583885192871, 15.07776165008545, 19.3795223236084, 0.6608377695083618], step: 8800, lr: 9.048317623484297e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 801 to ./logs/44k/G_8800.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 801 to ./logs/44k/D_8800.pth\n",
      "INFO:44k:====> Epoch: 801, cost 28.31 s\n",
      "INFO:44k:====> Epoch: 802, cost 17.18 s\n",
      "INFO:44k:====> Epoch: 803, cost 18.91 s\n",
      "INFO:44k:====> Epoch: 804, cost 21.54 s\n",
      "INFO:44k:====> Epoch: 805, cost 19.23 s\n",
      "INFO:44k:====> Epoch: 806, cost 19.54 s\n",
      "INFO:44k:====> Epoch: 807, cost 18.32 s\n",
      "INFO:44k:====> Epoch: 808, cost 17.01 s\n",
      "INFO:44k:====> Epoch: 809, cost 17.00 s\n",
      "INFO:44k:====> Epoch: 810, cost 16.82 s\n",
      "INFO:44k:====> Epoch: 811, cost 17.31 s\n",
      "INFO:44k:====> Epoch: 812, cost 17.19 s\n",
      "INFO:44k:====> Epoch: 813, cost 17.11 s\n",
      "INFO:44k:====> Epoch: 814, cost 18.78 s\n",
      "INFO:44k:====> Epoch: 815, cost 21.65 s\n",
      "INFO:44k:====> Epoch: 816, cost 19.32 s\n",
      "INFO:44k:====> Epoch: 817, cost 19.20 s\n",
      "INFO:44k:====> Epoch: 818, cost 18.36 s\n",
      "INFO:44k:Train Epoch: 819 [18%]\n",
      "INFO:44k:Losses: [2.220856189727783, 2.623861074447632, 11.940094947814941, 13.826963424682617, 0.6129260063171387], step: 9000, lr: 9.027980525551768e-05\n",
      "INFO:44k:====> Epoch: 819, cost 18.38 s\n",
      "INFO:44k:====> Epoch: 820, cost 17.52 s\n",
      "INFO:44k:====> Epoch: 821, cost 16.90 s\n",
      "INFO:44k:====> Epoch: 822, cost 17.01 s\n",
      "INFO:44k:====> Epoch: 823, cost 17.08 s\n",
      "INFO:44k:====> Epoch: 824, cost 17.28 s\n",
      "INFO:44k:====> Epoch: 825, cost 20.80 s\n",
      "INFO:44k:====> Epoch: 826, cost 19.09 s\n",
      "INFO:44k:====> Epoch: 827, cost 19.10 s\n",
      "INFO:44k:====> Epoch: 828, cost 19.24 s\n",
      "INFO:44k:====> Epoch: 829, cost 18.25 s\n",
      "INFO:44k:====> Epoch: 830, cost 17.21 s\n",
      "INFO:44k:====> Epoch: 831, cost 16.87 s\n",
      "INFO:44k:====> Epoch: 832, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 833, cost 16.85 s\n",
      "INFO:44k:====> Epoch: 834, cost 17.09 s\n",
      "INFO:44k:====> Epoch: 835, cost 17.51 s\n",
      "INFO:44k:====> Epoch: 836, cost 20.38 s\n",
      "INFO:44k:Train Epoch: 837 [36%]\n",
      "INFO:44k:Losses: [2.306040048599243, 2.7981061935424805, 12.739213943481445, 18.146963119506836, 0.6701725125312805], step: 9200, lr: 9.007689137503609e-05\n",
      "INFO:44k:====> Epoch: 837, cost 20.65 s\n",
      "INFO:44k:====> Epoch: 838, cost 19.18 s\n",
      "INFO:44k:====> Epoch: 839, cost 18.86 s\n",
      "INFO:44k:====> Epoch: 840, cost 17.54 s\n",
      "INFO:44k:====> Epoch: 841, cost 17.12 s\n",
      "INFO:44k:====> Epoch: 842, cost 17.00 s\n",
      "INFO:44k:====> Epoch: 843, cost 16.90 s\n",
      "INFO:44k:====> Epoch: 844, cost 17.13 s\n",
      "INFO:44k:====> Epoch: 845, cost 17.66 s\n",
      "INFO:44k:====> Epoch: 846, cost 18.83 s\n",
      "INFO:44k:====> Epoch: 847, cost 21.05 s\n",
      "INFO:44k:====> Epoch: 848, cost 19.77 s\n",
      "INFO:44k:====> Epoch: 849, cost 18.74 s\n",
      "INFO:44k:====> Epoch: 850, cost 18.92 s\n",
      "INFO:44k:====> Epoch: 851, cost 17.75 s\n",
      "INFO:44k:====> Epoch: 852, cost 16.99 s\n",
      "INFO:44k:====> Epoch: 853, cost 16.90 s\n",
      "INFO:44k:====> Epoch: 854, cost 17.09 s\n",
      "INFO:44k:Train Epoch: 855 [55%]\n",
      "INFO:44k:Losses: [2.4179391860961914, 2.3781518936157227, 12.232877731323242, 15.684900283813477, 0.636916995048523], step: 9400, lr: 8.987443356601786e-05\n",
      "INFO:44k:====> Epoch: 855, cost 18.13 s\n",
      "INFO:44k:====> Epoch: 856, cost 17.36 s\n",
      "INFO:44k:====> Epoch: 857, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 858, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 859, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 860, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 861, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 862, cost 16.86 s\n",
      "INFO:44k:====> Epoch: 863, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 864, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 865, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 866, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 867, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 868, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 869, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 870, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 871, cost 17.48 s\n",
      "INFO:44k:====> Epoch: 872, cost 16.36 s\n",
      "INFO:44k:Train Epoch: 873 [73%]\n",
      "INFO:44k:Losses: [2.066769599914551, 2.9135313034057617, 16.652193069458008, 18.08171844482422, 1.107097864151001], step: 9600, lr: 8.967243080339174e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 873 to ./logs/44k/G_9600.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 873 to ./logs/44k/D_9600.pth\n",
      "INFO:44k:====> Epoch: 873, cost 27.91 s\n",
      "INFO:44k:====> Epoch: 874, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 875, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 876, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 877, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 878, cost 16.87 s\n",
      "INFO:44k:====> Epoch: 879, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 880, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 881, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 882, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 883, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 884, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 885, cost 16.81 s\n",
      "INFO:44k:====> Epoch: 886, cost 19.23 s\n",
      "INFO:44k:====> Epoch: 887, cost 20.46 s\n",
      "INFO:44k:====> Epoch: 888, cost 19.09 s\n",
      "INFO:44k:====> Epoch: 889, cost 19.38 s\n",
      "INFO:44k:====> Epoch: 890, cost 19.35 s\n",
      "INFO:44k:Train Epoch: 891 [91%]\n",
      "INFO:44k:Losses: [2.7232346534729004, 2.3244433403015137, 13.511751174926758, 15.600542068481445, 0.3073091506958008], step: 9800, lr: 8.947088206439049e-05\n",
      "INFO:44k:====> Epoch: 891, cost 18.69 s\n",
      "INFO:44k:====> Epoch: 892, cost 17.08 s\n",
      "INFO:44k:====> Epoch: 893, cost 17.20 s\n",
      "INFO:44k:====> Epoch: 894, cost 17.59 s\n",
      "INFO:44k:====> Epoch: 895, cost 17.03 s\n",
      "INFO:44k:====> Epoch: 896, cost 17.18 s\n",
      "INFO:44k:====> Epoch: 897, cost 19.12 s\n",
      "INFO:44k:====> Epoch: 898, cost 21.57 s\n",
      "INFO:44k:====> Epoch: 899, cost 19.03 s\n",
      "INFO:44k:====> Epoch: 900, cost 18.72 s\n",
      "INFO:44k:====> Epoch: 901, cost 18.70 s\n",
      "INFO:44k:====> Epoch: 902, cost 16.92 s\n",
      "INFO:44k:====> Epoch: 903, cost 17.34 s\n",
      "INFO:44k:====> Epoch: 904, cost 16.89 s\n",
      "INFO:44k:====> Epoch: 905, cost 17.10 s\n",
      "INFO:44k:====> Epoch: 906, cost 17.02 s\n",
      "INFO:44k:====> Epoch: 907, cost 17.39 s\n",
      "INFO:44k:====> Epoch: 908, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 909, cost 16.41 s\n",
      "INFO:44k:Train Epoch: 910 [9%]\n",
      "INFO:44k:Losses: [2.375126838684082, 2.577892541885376, 11.332347869873047, 18.19108009338379, 0.5181016325950623], step: 10000, lr: 8.925862760525449e-05\n",
      "INFO:44k:====> Epoch: 910, cost 17.64 s\n",
      "INFO:44k:====> Epoch: 911, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 912, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 913, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 914, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 915, cost 16.90 s\n",
      "INFO:44k:====> Epoch: 916, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 917, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 918, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 919, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 920, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 921, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 922, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 923, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 924, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 925, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 926, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 927, cost 16.21 s\n",
      "INFO:44k:Train Epoch: 928 [27%]\n",
      "INFO:44k:Losses: [1.8312268257141113, 3.046644449234009, 17.294391632080078, 16.570844650268555, 0.3642748296260834], step: 10200, lr: 8.90580089348599e-05\n",
      "INFO:44k:====> Epoch: 928, cost 17.94 s\n",
      "INFO:44k:====> Epoch: 929, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 930, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 931, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 932, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 933, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 934, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 935, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 936, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 937, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 938, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 939, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 940, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 941, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 942, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 943, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 944, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 945, cost 16.52 s\n",
      "INFO:44k:Train Epoch: 946 [45%]\n",
      "INFO:44k:Losses: [1.9037281274795532, 3.0022451877593994, 17.657669067382812, 20.19171714782715, 0.8138209581375122], step: 10400, lr: 8.885784117718933e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 946 to ./logs/44k/G_10400.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 946 to ./logs/44k/D_10400.pth\n",
      "INFO:44k:====> Epoch: 946, cost 27.83 s\n",
      "INFO:44k:====> Epoch: 947, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 948, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 949, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 950, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 951, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 952, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 953, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 954, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 955, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 956, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 957, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 958, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 959, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 960, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 961, cost 16.85 s\n",
      "INFO:44k:====> Epoch: 962, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 963, cost 17.07 s\n",
      "INFO:44k:Train Epoch: 964 [64%]\n",
      "INFO:44k:Losses: [2.1963717937469482, 2.6080262660980225, 9.616792678833008, 15.358768463134766, 0.9631187319755554], step: 10600, lr: 8.865812331876634e-05\n",
      "INFO:44k:====> Epoch: 964, cost 17.79 s\n",
      "INFO:44k:====> Epoch: 965, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 966, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 967, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 968, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 969, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 970, cost 21.20 s\n",
      "INFO:44k:====> Epoch: 971, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 972, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 973, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 974, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 975, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 976, cost 17.07 s\n",
      "INFO:44k:====> Epoch: 977, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 978, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 979, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 980, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 981, cost 16.32 s\n",
      "INFO:44k:Train Epoch: 982 [82%]\n",
      "INFO:44k:Losses: [2.4863765239715576, 2.192009687423706, 9.860481262207031, 14.94776725769043, 0.8717955350875854], step: 10800, lr: 8.845885434839242e-05\n",
      "INFO:44k:====> Epoch: 982, cost 17.38 s\n",
      "INFO:44k:====> Epoch: 983, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 984, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 985, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 986, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 987, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 988, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 989, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 990, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 991, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 992, cost 16.88 s\n",
      "INFO:44k:====> Epoch: 993, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 994, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 995, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 996, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 997, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 998, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 999, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1000, cost 16.47 s\n",
      "INFO:44k:Train Epoch: 1001 [0%]\n",
      "INFO:44k:Losses: [1.966120958328247, 2.7423977851867676, 15.377431869506836, 15.609330177307129, 1.2127399444580078], step: 11000, lr: 8.824900075298475e-05\n",
      "INFO:44k:====> Epoch: 1001, cost 17.81 s\n",
      "INFO:44k:====> Epoch: 1002, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1003, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1004, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 1005, cost 16.94 s\n",
      "INFO:44k:====> Epoch: 1006, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1007, cost 16.90 s\n",
      "INFO:44k:====> Epoch: 1008, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 1009, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1010, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1011, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 1012, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1013, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1014, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 1015, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 1016, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1017, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 1018, cost 16.31 s\n",
      "INFO:44k:Train Epoch: 1019 [18%]\n",
      "INFO:44k:Losses: [2.232866048812866, 2.975647449493408, 10.70394515991211, 14.733695983886719, 0.5054775476455688], step: 11200, lr: 8.805065133097696e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 1019 to ./logs/44k/G_11200.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1019 to ./logs/44k/D_11200.pth\n",
      "INFO:44k:====> Epoch: 1019, cost 27.54 s\n",
      "INFO:44k:====> Epoch: 1020, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1021, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 1022, cost 16.96 s\n",
      "INFO:44k:====> Epoch: 1023, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1024, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 1025, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1026, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1027, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1028, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1029, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1030, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 1031, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 1032, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 1033, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1034, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1035, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1036, cost 16.41 s\n",
      "INFO:44k:Train Epoch: 1037 [36%]\n",
      "INFO:44k:Losses: [2.1627421379089355, 2.7179207801818848, 14.065890312194824, 15.870307922363281, 0.9095109105110168], step: 11400, lr: 8.785274772130558e-05\n",
      "INFO:44k:====> Epoch: 1037, cost 17.67 s\n",
      "INFO:44k:====> Epoch: 1038, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1039, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 1040, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 1041, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1042, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1043, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1044, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1045, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1046, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1047, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 1048, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 1049, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1050, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1051, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1052, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 1053, cost 16.93 s\n",
      "INFO:44k:====> Epoch: 1054, cost 16.52 s\n",
      "INFO:44k:Train Epoch: 1055 [55%]\n",
      "INFO:44k:Losses: [2.402200937271118, 2.3022892475128174, 8.87155532836914, 11.970394134521484, 0.36508938670158386], step: 11600, lr: 8.765528892195788e-05\n",
      "INFO:44k:====> Epoch: 1055, cost 17.74 s\n",
      "INFO:44k:====> Epoch: 1056, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 1057, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1058, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 1059, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1060, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 1061, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 1062, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1063, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1064, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 1065, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1066, cost 16.83 s\n",
      "INFO:44k:====> Epoch: 1067, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1068, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1069, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1070, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1071, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1072, cost 16.31 s\n",
      "INFO:44k:Train Epoch: 1073 [73%]\n",
      "INFO:44k:Losses: [2.1444554328918457, 2.8430140018463135, 13.014286041259766, 18.39870262145996, 0.6348608136177063], step: 11800, lr: 8.745827393317333e-05\n",
      "INFO:44k:====> Epoch: 1073, cost 17.69 s\n",
      "INFO:44k:====> Epoch: 1074, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1075, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1076, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 1077, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1078, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1079, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1080, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1081, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1082, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1083, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 1084, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 1085, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1086, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 1087, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1088, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1089, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1090, cost 16.59 s\n",
      "INFO:44k:Train Epoch: 1091 [91%]\n",
      "INFO:44k:Losses: [2.2238826751708984, 2.364816188812256, 11.773441314697266, 18.256542205810547, 0.5815343260765076], step: 12000, lr: 8.726170175743843e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 1091 to ./logs/44k/G_12000.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1091 to ./logs/44k/D_12000.pth\n",
      "INFO:44k:====> Epoch: 1091, cost 27.38 s\n",
      "INFO:44k:====> Epoch: 1092, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1093, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1094, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1095, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1096, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1097, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 1098, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 1099, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 1100, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1101, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1102, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 1103, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1104, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1105, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1106, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 1107, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1108, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1109, cost 16.37 s\n",
      "INFO:44k:Train Epoch: 1110 [9%]\n",
      "INFO:44k:Losses: [2.17917537689209, 2.8409247398376465, 11.136545181274414, 13.835039138793945, 0.6075876355171204], step: 12200, lr: 8.705468820305681e-05\n",
      "INFO:44k:====> Epoch: 1110, cost 17.47 s\n",
      "INFO:44k:====> Epoch: 1111, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 1112, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 1113, cost 17.21 s\n",
      "INFO:44k:====> Epoch: 1114, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1115, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1116, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1117, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1118, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 1119, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1120, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1121, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1122, cost 16.98 s\n",
      "INFO:44k:====> Epoch: 1123, cost 19.30 s\n",
      "INFO:44k:====> Epoch: 1124, cost 21.24 s\n",
      "INFO:44k:====> Epoch: 1125, cost 19.10 s\n",
      "INFO:44k:====> Epoch: 1126, cost 18.92 s\n",
      "INFO:44k:====> Epoch: 1127, cost 18.79 s\n",
      "INFO:44k:Train Epoch: 1128 [27%]\n",
      "INFO:44k:Losses: [1.7461488246917725, 3.143373727798462, 19.648338317871094, 16.345735549926758, 0.514045238494873], step: 12400, lr: 8.68590231310355e-05\n",
      "INFO:44k:====> Epoch: 1128, cost 18.37 s\n",
      "INFO:44k:====> Epoch: 1129, cost 17.14 s\n",
      "INFO:44k:====> Epoch: 1130, cost 17.36 s\n",
      "INFO:44k:====> Epoch: 1131, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 1132, cost 17.12 s\n",
      "INFO:44k:====> Epoch: 1133, cost 17.19 s\n",
      "INFO:44k:====> Epoch: 1134, cost 18.95 s\n",
      "INFO:44k:====> Epoch: 1135, cost 20.87 s\n",
      "INFO:44k:====> Epoch: 1136, cost 19.24 s\n",
      "INFO:44k:====> Epoch: 1137, cost 19.49 s\n",
      "INFO:44k:====> Epoch: 1138, cost 18.60 s\n",
      "INFO:44k:====> Epoch: 1139, cost 17.10 s\n",
      "INFO:44k:====> Epoch: 1140, cost 17.07 s\n",
      "INFO:44k:====> Epoch: 1141, cost 16.88 s\n",
      "INFO:44k:====> Epoch: 1142, cost 16.86 s\n",
      "INFO:44k:====> Epoch: 1143, cost 17.20 s\n",
      "INFO:44k:====> Epoch: 1144, cost 17.53 s\n",
      "INFO:44k:====> Epoch: 1145, cost 19.55 s\n",
      "INFO:44k:Train Epoch: 1146 [45%]\n",
      "INFO:44k:Losses: [1.5566167831420898, 3.396817445755005, 21.952558517456055, 20.10303497314453, 0.5880789756774902], step: 12600, lr: 8.666379783797609e-05\n",
      "INFO:44k:====> Epoch: 1146, cost 22.05 s\n",
      "INFO:44k:====> Epoch: 1147, cost 19.19 s\n",
      "INFO:44k:====> Epoch: 1148, cost 19.19 s\n",
      "INFO:44k:====> Epoch: 1149, cost 18.50 s\n",
      "INFO:44k:====> Epoch: 1150, cost 17.03 s\n",
      "INFO:44k:====> Epoch: 1151, cost 17.13 s\n",
      "INFO:44k:====> Epoch: 1152, cost 17.15 s\n",
      "INFO:44k:====> Epoch: 1153, cost 16.86 s\n",
      "INFO:44k:====> Epoch: 1154, cost 16.98 s\n",
      "INFO:44k:====> Epoch: 1155, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 1156, cost 16.88 s\n",
      "INFO:44k:====> Epoch: 1157, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 1158, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1159, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 1160, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1161, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 1162, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 1163, cost 16.40 s\n",
      "INFO:44k:Train Epoch: 1164 [64%]\n",
      "INFO:44k:Losses: [1.9824700355529785, 2.724477529525757, 12.856143951416016, 14.898927688598633, 0.6525904536247253], step: 12800, lr: 8.646901133542656e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 1164 to ./logs/44k/G_12800.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1164 to ./logs/44k/D_12800.pth\n",
      "INFO:44k:====> Epoch: 1164, cost 27.14 s\n",
      "INFO:44k:====> Epoch: 1165, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1166, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1167, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1168, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 1169, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1170, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1171, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1172, cost 16.92 s\n",
      "INFO:44k:====> Epoch: 1173, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1174, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 1175, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 1176, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1177, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 1178, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1179, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1180, cost 16.87 s\n",
      "INFO:44k:====> Epoch: 1181, cost 16.74 s\n",
      "INFO:44k:Train Epoch: 1182 [82%]\n",
      "INFO:44k:Losses: [2.4028518199920654, 2.371931791305542, 10.167177200317383, 16.377769470214844, 0.7092981934547424], step: 13000, lr: 8.627466263715663e-05\n",
      "INFO:44k:====> Epoch: 1182, cost 17.61 s\n",
      "INFO:44k:====> Epoch: 1183, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1184, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 1185, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1186, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 1187, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 1188, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1189, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1190, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1191, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1192, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1193, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1194, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1195, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1196, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1197, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1198, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1199, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 1200, cost 16.43 s\n",
      "INFO:44k:Train Epoch: 1201 [0%]\n",
      "INFO:44k:Losses: [1.8106417655944824, 3.015918254852295, 19.326841354370117, 18.321792602539062, 0.6059449315071106], step: 13200, lr: 8.60699906653076e-05\n",
      "INFO:44k:====> Epoch: 1201, cost 17.63 s\n",
      "INFO:44k:====> Epoch: 1202, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1203, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1204, cost 16.83 s\n",
      "INFO:44k:====> Epoch: 1205, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 1206, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 1207, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1208, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1209, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1210, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 1211, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1212, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1213, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 1214, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 1215, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1216, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1217, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1218, cost 16.69 s\n",
      "INFO:44k:Train Epoch: 1219 [18%]\n",
      "INFO:44k:Losses: [2.363300085067749, 2.509683609008789, 11.64130687713623, 16.52367401123047, 0.5942844152450562], step: 13400, lr: 8.587653881027227e-05\n",
      "INFO:44k:====> Epoch: 1219, cost 17.63 s\n",
      "INFO:44k:====> Epoch: 1220, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 1221, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1222, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1223, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1224, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1225, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1226, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 1227, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1228, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1229, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 1230, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 1231, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 1232, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1233, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 1234, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 1235, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 1236, cost 16.38 s\n",
      "INFO:44k:Train Epoch: 1237 [36%]\n",
      "INFO:44k:Losses: [2.299691677093506, 2.6876604557037354, 10.50378131866455, 16.186847686767578, 0.546255886554718], step: 13600, lr: 8.568352175974806e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 1237 to ./logs/44k/G_13600.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1237 to ./logs/44k/D_13600.pth\n",
      "INFO:44k:====> Epoch: 1237, cost 27.19 s\n",
      "INFO:44k:====> Epoch: 1238, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1239, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 1240, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1241, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1242, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1243, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 1244, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1245, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1246, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1247, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1248, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 1249, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 1250, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 1251, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1252, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1253, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 1254, cost 16.42 s\n",
      "INFO:44k:Train Epoch: 1255 [55%]\n",
      "INFO:44k:Losses: [2.1813302040100098, 2.7216320037841797, 13.725725173950195, 13.56513500213623, 0.6739184260368347], step: 13800, lr: 8.549093853646363e-05\n",
      "INFO:44k:====> Epoch: 1255, cost 17.84 s\n",
      "INFO:44k:====> Epoch: 1256, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 1257, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1258, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1259, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 1260, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1261, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1262, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1263, cost 17.02 s\n",
      "INFO:44k:====> Epoch: 1264, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 1265, cost 16.88 s\n",
      "INFO:44k:====> Epoch: 1266, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1267, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1268, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1269, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 1270, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 1271, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1272, cost 16.45 s\n",
      "INFO:44k:Train Epoch: 1273 [73%]\n",
      "INFO:44k:Losses: [2.4295012950897217, 2.573712110519409, 17.774499893188477, 18.368505477905273, 0.3802652955055237], step: 14000, lr: 8.529878816534412e-05\n",
      "INFO:44k:====> Epoch: 1273, cost 17.61 s\n",
      "INFO:44k:====> Epoch: 1274, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 1275, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1276, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1277, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1278, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1279, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1280, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1281, cost 16.84 s\n",
      "INFO:44k:====> Epoch: 1282, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 1283, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 1284, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1285, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 1286, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1287, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1288, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 1289, cost 16.95 s\n",
      "INFO:44k:====> Epoch: 1290, cost 16.45 s\n",
      "INFO:44k:Train Epoch: 1291 [91%]\n",
      "INFO:44k:Losses: [2.2852649688720703, 2.732875347137451, 11.484050750732422, 16.949188232421875, 0.729483425617218], step: 14200, lr: 8.510706967350623e-05\n",
      "INFO:44k:====> Epoch: 1291, cost 17.38 s\n",
      "INFO:44k:====> Epoch: 1292, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1293, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1294, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 1295, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1296, cost 16.74 s\n",
      "INFO:44k:====> Epoch: 1297, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1298, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1299, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 1300, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1301, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 1302, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1303, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 1304, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1305, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 1306, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1307, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 1308, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 1309, cost 16.40 s\n",
      "INFO:44k:Train Epoch: 1310 [9%]\n",
      "INFO:44k:Losses: [1.9235247373580933, 3.2484686374664307, 18.683076858520508, 20.06601905822754, 0.37211140990257263], step: 14400, lr: 8.490516761749211e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 1310 to ./logs/44k/G_14400.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1310 to ./logs/44k/D_14400.pth\n",
      "INFO:44k:====> Epoch: 1310, cost 27.59 s\n",
      "INFO:44k:====> Epoch: 1311, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1312, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1313, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1314, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 1315, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1316, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1317, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1318, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1319, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1320, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1321, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1322, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 1323, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1324, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1325, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1326, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 1327, cost 16.60 s\n",
      "INFO:44k:Train Epoch: 1328 [27%]\n",
      "INFO:44k:Losses: [1.9069513082504272, 3.1285438537597656, 18.092132568359375, 14.918892860412598, 0.4011695384979248], step: 14600, lr: 8.47143338315148e-05\n",
      "INFO:44k:====> Epoch: 1328, cost 17.55 s\n",
      "INFO:44k:====> Epoch: 1329, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1330, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1331, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 1332, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1333, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 1334, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 1335, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 1336, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1337, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1338, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1339, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1340, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1341, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1342, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1343, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1344, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 1345, cost 16.79 s\n",
      "INFO:44k:Train Epoch: 1346 [45%]\n",
      "INFO:44k:Losses: [1.6296117305755615, 3.460381507873535, 24.129043579101562, 19.50334930419922, 0.6550222635269165], step: 14800, lr: 8.452392896564795e-05\n",
      "INFO:44k:====> Epoch: 1346, cost 17.73 s\n",
      "INFO:44k:====> Epoch: 1347, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1348, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1349, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1350, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1351, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1352, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1353, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1354, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 1355, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1356, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1357, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1358, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1359, cost 16.86 s\n",
      "INFO:44k:====> Epoch: 1360, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 1361, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 1362, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1363, cost 16.73 s\n",
      "INFO:44k:Train Epoch: 1364 [64%]\n",
      "INFO:44k:Losses: [2.035282850265503, 2.750056266784668, 11.027095794677734, 14.809677124023438, 0.7430037260055542], step: 15000, lr: 8.433395205584596e-05\n",
      "INFO:44k:====> Epoch: 1364, cost 17.51 s\n",
      "INFO:44k:====> Epoch: 1365, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 1366, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1367, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1368, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1369, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 1370, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1371, cost 18.08 s\n",
      "INFO:44k:====> Epoch: 1372, cost 20.79 s\n",
      "INFO:44k:====> Epoch: 1373, cost 19.13 s\n",
      "INFO:44k:====> Epoch: 1374, cost 19.00 s\n",
      "INFO:44k:====> Epoch: 1375, cost 18.61 s\n",
      "INFO:44k:====> Epoch: 1376, cost 17.26 s\n",
      "INFO:44k:====> Epoch: 1377, cost 17.02 s\n",
      "INFO:44k:====> Epoch: 1378, cost 16.77 s\n",
      "INFO:44k:====> Epoch: 1379, cost 17.06 s\n",
      "INFO:44k:====> Epoch: 1380, cost 17.28 s\n",
      "INFO:44k:====> Epoch: 1381, cost 17.70 s\n",
      "INFO:44k:Train Epoch: 1382 [82%]\n",
      "INFO:44k:Losses: [2.1553800106048584, 2.601508855819702, 12.644271850585938, 15.378202438354492, 0.4967799186706543], step: 15200, lr: 8.41444021402301e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 1382 to ./logs/44k/G_15200.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1382 to ./logs/44k/D_15200.pth\n",
      "INFO:44k:====> Epoch: 1382, cost 31.23 s\n",
      "INFO:44k:====> Epoch: 1383, cost 19.13 s\n",
      "INFO:44k:====> Epoch: 1384, cost 19.05 s\n",
      "INFO:44k:====> Epoch: 1385, cost 19.25 s\n",
      "INFO:44k:====> Epoch: 1386, cost 18.04 s\n",
      "INFO:44k:====> Epoch: 1387, cost 17.22 s\n",
      "INFO:44k:====> Epoch: 1388, cost 17.25 s\n",
      "INFO:44k:====> Epoch: 1389, cost 17.34 s\n",
      "INFO:44k:====> Epoch: 1390, cost 17.09 s\n",
      "INFO:44k:====> Epoch: 1391, cost 17.08 s\n",
      "INFO:44k:====> Epoch: 1392, cost 18.20 s\n",
      "INFO:44k:====> Epoch: 1393, cost 20.89 s\n",
      "INFO:44k:====> Epoch: 1394, cost 19.60 s\n",
      "INFO:44k:====> Epoch: 1395, cost 19.04 s\n",
      "INFO:44k:====> Epoch: 1396, cost 18.99 s\n",
      "INFO:44k:====> Epoch: 1397, cost 17.13 s\n",
      "INFO:44k:====> Epoch: 1398, cost 16.96 s\n",
      "INFO:44k:====> Epoch: 1399, cost 16.93 s\n",
      "INFO:44k:====> Epoch: 1400, cost 16.78 s\n",
      "INFO:44k:Train Epoch: 1401 [0%]\n",
      "INFO:44k:Losses: [1.680711030960083, 2.9990711212158203, 18.028139114379883, 17.391386032104492, 0.20729655027389526], step: 15400, lr: 8.394478384930123e-05\n",
      "INFO:44k:====> Epoch: 1401, cost 18.22 s\n",
      "INFO:44k:====> Epoch: 1402, cost 16.99 s\n",
      "INFO:44k:====> Epoch: 1403, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1404, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1405, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1406, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1407, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1408, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 1409, cost 16.77 s\n",
      "INFO:44k:====> Epoch: 1410, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1411, cost 16.86 s\n",
      "INFO:44k:====> Epoch: 1412, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1413, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1414, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1415, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1416, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 1417, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 1418, cost 16.54 s\n",
      "INFO:44k:Train Epoch: 1419 [18%]\n",
      "INFO:44k:Losses: [2.571213483810425, 2.066740036010742, 10.113330841064453, 13.519542694091797, 0.7249157428741455], step: 15600, lr: 8.375610863241483e-05\n",
      "INFO:44k:====> Epoch: 1419, cost 17.92 s\n",
      "INFO:44k:====> Epoch: 1420, cost 16.79 s\n",
      "INFO:44k:====> Epoch: 1421, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 1422, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 1423, cost 16.90 s\n",
      "INFO:44k:====> Epoch: 1424, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 1425, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1426, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1427, cost 16.79 s\n",
      "INFO:44k:====> Epoch: 1428, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1429, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1430, cost 16.12 s\n",
      "INFO:44k:====> Epoch: 1431, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 1432, cost 17.12 s\n",
      "INFO:44k:====> Epoch: 1433, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 1434, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 1435, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1436, cost 16.42 s\n",
      "INFO:44k:Train Epoch: 1437 [36%]\n",
      "INFO:44k:Losses: [2.453779935836792, 2.3062493801116943, 10.34794807434082, 14.418344497680664, 0.5951688289642334], step: 15800, lr: 8.35678574840153e-05\n",
      "INFO:44k:====> Epoch: 1437, cost 17.92 s\n",
      "INFO:44k:====> Epoch: 1438, cost 16.79 s\n",
      "INFO:44k:====> Epoch: 1439, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1440, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1441, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 1442, cost 16.19 s\n",
      "INFO:44k:====> Epoch: 1443, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1444, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 1445, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1446, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1447, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1448, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1449, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1450, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1451, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1452, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 1453, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1454, cost 16.47 s\n",
      "INFO:44k:Train Epoch: 1455 [55%]\n",
      "INFO:44k:Losses: [2.0312888622283936, 3.2892022132873535, 14.171488761901855, 18.5543155670166, 0.6097925901412964], step: 16000, lr: 8.338002945096165e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 1455 to ./logs/44k/G_16000.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1455 to ./logs/44k/D_16000.pth\n",
      "INFO:44k:====> Epoch: 1455, cost 27.52 s\n",
      "INFO:44k:====> Epoch: 1456, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1457, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 1458, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 1459, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1460, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1461, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1462, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1463, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 1464, cost 16.74 s\n",
      "INFO:44k:====> Epoch: 1465, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 1466, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1467, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1468, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 1469, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1470, cost 16.74 s\n",
      "INFO:44k:====> Epoch: 1471, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 1472, cost 16.50 s\n",
      "INFO:44k:Train Epoch: 1473 [73%]\n",
      "INFO:44k:Losses: [2.0497920513153076, 2.715271234512329, 15.979769706726074, 13.374838829040527, 0.6857991814613342], step: 16200, lr: 8.319262358225517e-05\n",
      "INFO:44k:====> Epoch: 1473, cost 17.56 s\n",
      "INFO:44k:====> Epoch: 1474, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 1475, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 1476, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1477, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1478, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1479, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 1480, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1481, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1482, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1483, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1484, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1485, cost 16.74 s\n",
      "INFO:44k:====> Epoch: 1486, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1487, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1488, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1489, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 1490, cost 16.69 s\n",
      "INFO:44k:Train Epoch: 1491 [91%]\n",
      "INFO:44k:Losses: [2.456942319869995, 2.3193936347961426, 10.840392112731934, 13.553230285644531, 0.2742190957069397], step: 16400, lr: 8.300563892903466e-05\n",
      "INFO:44k:====> Epoch: 1491, cost 17.46 s\n",
      "INFO:44k:====> Epoch: 1492, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1493, cost 16.96 s\n",
      "INFO:44k:====> Epoch: 1494, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1495, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1496, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 1497, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 1498, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1499, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 1500, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1501, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1502, cost 16.89 s\n",
      "INFO:44k:====> Epoch: 1503, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1504, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1505, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1506, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 1507, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 1508, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1509, cost 16.39 s\n",
      "INFO:44k:Train Epoch: 1510 [9%]\n",
      "INFO:44k:Losses: [2.4686508178710938, 2.4753336906433105, 11.20032024383545, 13.251326560974121, 0.5828890204429626], step: 16600, lr: 8.28087221602535e-05\n",
      "INFO:44k:====> Epoch: 1510, cost 17.80 s\n",
      "INFO:44k:====> Epoch: 1511, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 1512, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1513, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1514, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 1515, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1516, cost 16.88 s\n",
      "INFO:44k:====> Epoch: 1517, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1518, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1519, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1520, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1521, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1522, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1523, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1524, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 1525, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 1526, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 1527, cost 16.60 s\n",
      "INFO:44k:Train Epoch: 1528 [27%]\n",
      "INFO:44k:Losses: [1.7882351875305176, 3.294661045074463, 16.775386810302734, 11.856157302856445, -0.14047856628894806], step: 16800, lr: 8.26226003680797e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 1528 to ./logs/44k/G_16800.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1528 to ./logs/44k/D_16800.pth\n",
      "INFO:44k:====> Epoch: 1528, cost 27.98 s\n",
      "INFO:44k:====> Epoch: 1529, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 1530, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1531, cost 16.93 s\n",
      "INFO:44k:====> Epoch: 1532, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 1533, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 1534, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 1535, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1536, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1537, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1538, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1539, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1540, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 1541, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1542, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 1543, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1544, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1545, cost 16.51 s\n",
      "INFO:44k:Train Epoch: 1546 [45%]\n",
      "INFO:44k:Losses: [1.6015039682388306, 3.389880657196045, 26.195114135742188, 19.017391204833984, 0.8436632752418518], step: 17000, lr: 8.243689690528738e-05\n",
      "INFO:44k:====> Epoch: 1546, cost 17.67 s\n",
      "INFO:44k:====> Epoch: 1547, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1548, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1549, cost 16.89 s\n",
      "INFO:44k:====> Epoch: 1550, cost 16.20 s\n",
      "INFO:44k:====> Epoch: 1551, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1552, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1553, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1554, cost 16.87 s\n",
      "INFO:44k:====> Epoch: 1555, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1556, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1557, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1558, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1559, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1560, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 1561, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1562, cost 16.88 s\n",
      "INFO:44k:====> Epoch: 1563, cost 16.78 s\n",
      "INFO:44k:Train Epoch: 1564 [64%]\n",
      "INFO:44k:Losses: [2.1300711631774902, 2.7744407653808594, 18.165977478027344, 15.303974151611328, 0.8990393280982971], step: 17200, lr: 8.225161083163482e-05\n",
      "INFO:44k:====> Epoch: 1564, cost 17.50 s\n",
      "INFO:44k:====> Epoch: 1565, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1566, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1567, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1568, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1569, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 1570, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1571, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 1572, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1573, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1574, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 1575, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1576, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1577, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1578, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1579, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1580, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1581, cost 16.49 s\n",
      "INFO:44k:Train Epoch: 1582 [82%]\n",
      "INFO:44k:Losses: [2.4242095947265625, 2.506119966506958, 13.032617568969727, 13.360821723937988, 0.7954717874526978], step: 17400, lr: 8.206674120899363e-05\n",
      "INFO:44k:====> Epoch: 1582, cost 17.49 s\n",
      "INFO:44k:====> Epoch: 1583, cost 16.82 s\n",
      "INFO:44k:====> Epoch: 1584, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1585, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 1586, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1587, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 1588, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 1589, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1590, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1591, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1592, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1593, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 1594, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1595, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1596, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1597, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 1598, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 1599, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1600, cost 16.35 s\n",
      "INFO:44k:Train Epoch: 1601 [0%]\n",
      "INFO:44k:Losses: [1.683402180671692, 2.8495333194732666, 15.169134140014648, 16.448572158813477, 0.7457111477851868], step: 17600, lr: 8.18720518154563e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 1601 to ./logs/44k/G_17600.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1601 to ./logs/44k/D_17600.pth\n",
      "INFO:44k:====> Epoch: 1601, cost 27.64 s\n",
      "INFO:44k:====> Epoch: 1602, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 1603, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 1604, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1605, cost 16.83 s\n",
      "INFO:44k:====> Epoch: 1606, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 1607, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1608, cost 21.75 s\n",
      "INFO:44k:====> Epoch: 1609, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1610, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1611, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1612, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1613, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1614, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1615, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1616, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 1617, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1618, cost 18.01 s\n",
      "INFO:44k:Train Epoch: 1619 [18%]\n",
      "INFO:44k:Losses: [2.2293190956115723, 2.6239542961120605, 15.452676773071289, 17.664079666137695, 0.4599636197090149], step: 17800, lr: 8.16880352938229e-05\n",
      "INFO:44k:====> Epoch: 1619, cost 22.00 s\n",
      "INFO:44k:====> Epoch: 1620, cost 19.17 s\n",
      "INFO:44k:====> Epoch: 1621, cost 18.98 s\n",
      "INFO:44k:====> Epoch: 1622, cost 19.16 s\n",
      "INFO:44k:====> Epoch: 1623, cost 17.33 s\n",
      "INFO:44k:====> Epoch: 1624, cost 17.14 s\n",
      "INFO:44k:====> Epoch: 1625, cost 16.97 s\n",
      "INFO:44k:====> Epoch: 1626, cost 16.90 s\n",
      "INFO:44k:====> Epoch: 1627, cost 16.94 s\n",
      "INFO:44k:====> Epoch: 1628, cost 16.86 s\n",
      "INFO:44k:====> Epoch: 1629, cost 18.68 s\n",
      "INFO:44k:====> Epoch: 1630, cost 21.26 s\n",
      "INFO:44k:====> Epoch: 1631, cost 18.80 s\n",
      "INFO:44k:====> Epoch: 1632, cost 19.05 s\n",
      "INFO:44k:====> Epoch: 1633, cost 19.02 s\n",
      "INFO:44k:====> Epoch: 1634, cost 17.20 s\n",
      "INFO:44k:====> Epoch: 1635, cost 16.92 s\n",
      "INFO:44k:====> Epoch: 1636, cost 16.85 s\n",
      "INFO:44k:Train Epoch: 1637 [36%]\n",
      "INFO:44k:Losses: [1.7810649871826172, 3.666639566421509, 12.361296653747559, 14.617694854736328, 0.8845414519309998], step: 18000, lr: 8.15044323697418e-05\n",
      "INFO:44k:====> Epoch: 1637, cost 18.02 s\n",
      "INFO:44k:====> Epoch: 1638, cost 16.86 s\n",
      "INFO:44k:====> Epoch: 1639, cost 16.99 s\n",
      "INFO:44k:====> Epoch: 1640, cost 18.42 s\n",
      "INFO:44k:====> Epoch: 1641, cost 21.34 s\n",
      "INFO:44k:====> Epoch: 1642, cost 19.10 s\n",
      "INFO:44k:====> Epoch: 1643, cost 19.17 s\n",
      "INFO:44k:====> Epoch: 1644, cost 19.43 s\n",
      "INFO:44k:====> Epoch: 1645, cost 17.45 s\n",
      "INFO:44k:====> Epoch: 1646, cost 16.87 s\n",
      "INFO:44k:====> Epoch: 1647, cost 16.90 s\n",
      "INFO:44k:====> Epoch: 1648, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 1649, cost 16.79 s\n",
      "INFO:44k:====> Epoch: 1650, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 1651, cost 18.45 s\n",
      "INFO:44k:====> Epoch: 1652, cost 21.23 s\n",
      "INFO:44k:====> Epoch: 1653, cost 19.20 s\n",
      "INFO:44k:====> Epoch: 1654, cost 19.06 s\n",
      "INFO:44k:Train Epoch: 1655 [55%]\n",
      "INFO:44k:Losses: [2.088559150695801, 2.8154218196868896, 12.210609436035156, 12.855610847473145, 0.6796357035636902], step: 18200, lr: 8.132124211360665e-05\n",
      "INFO:44k:====> Epoch: 1655, cost 20.23 s\n",
      "INFO:44k:====> Epoch: 1656, cost 17.04 s\n",
      "INFO:44k:====> Epoch: 1657, cost 17.10 s\n",
      "INFO:44k:====> Epoch: 1658, cost 17.19 s\n",
      "INFO:44k:====> Epoch: 1659, cost 16.88 s\n",
      "INFO:44k:====> Epoch: 1660, cost 17.02 s\n",
      "INFO:44k:====> Epoch: 1661, cost 17.08 s\n",
      "INFO:44k:====> Epoch: 1662, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1663, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1664, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1665, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1666, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 1667, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1668, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 1669, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 1670, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1671, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1672, cost 16.34 s\n",
      "INFO:44k:Train Epoch: 1673 [73%]\n",
      "INFO:44k:Losses: [1.9779648780822754, 2.9397122859954834, 20.638765335083008, 17.84583854675293, 1.0010172128677368], step: 18400, lr: 8.113846359790042e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 1673 to ./logs/44k/G_18400.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1673 to ./logs/44k/D_18400.pth\n",
      "INFO:44k:====> Epoch: 1673, cost 27.30 s\n",
      "INFO:44k:====> Epoch: 1674, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1675, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1676, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 1677, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1678, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1679, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 1680, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1681, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 1682, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1683, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1684, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1685, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 1686, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1687, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1688, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 1689, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 1690, cost 16.59 s\n",
      "INFO:44k:Train Epoch: 1691 [91%]\n",
      "INFO:44k:Losses: [2.5490610599517822, 2.3060355186462402, 9.008833885192871, 10.912601470947266, 0.9209908246994019], step: 18600, lr: 8.09560958971908e-05\n",
      "INFO:44k:====> Epoch: 1691, cost 17.59 s\n",
      "INFO:44k:====> Epoch: 1692, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1693, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1694, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 1695, cost 16.95 s\n",
      "INFO:44k:====> Epoch: 1696, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1697, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1698, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1699, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1700, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1701, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 1702, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1703, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1704, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 1705, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 1706, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 1707, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1708, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1709, cost 16.59 s\n",
      "INFO:44k:Train Epoch: 1710 [9%]\n",
      "INFO:44k:Losses: [2.1703970432281494, 2.8839988708496094, 15.819353103637695, 16.505510330200195, 0.6533812284469604], step: 18800, lr: 8.07640413208645e-05\n",
      "INFO:44k:====> Epoch: 1710, cost 17.62 s\n",
      "INFO:44k:====> Epoch: 1711, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1712, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1713, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1714, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1715, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 1716, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1717, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1718, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1719, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1720, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1721, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 1722, cost 16.79 s\n",
      "INFO:44k:====> Epoch: 1723, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 1724, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 1725, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 1726, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1727, cost 16.40 s\n",
      "INFO:44k:Train Epoch: 1728 [27%]\n",
      "INFO:44k:Losses: [1.6795823574066162, 3.0995819568634033, 19.35353660583496, 16.58014678955078, 0.693340539932251], step: 19000, lr: 8.058251517577144e-05\n",
      "INFO:44k:====> Epoch: 1728, cost 17.60 s\n",
      "INFO:44k:====> Epoch: 1729, cost 16.87 s\n",
      "INFO:44k:====> Epoch: 1730, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1731, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1732, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1733, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1734, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 1735, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1736, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 1737, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1738, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 1739, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 1740, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1741, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1742, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1743, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1744, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1745, cost 16.58 s\n",
      "INFO:44k:Train Epoch: 1746 [45%]\n",
      "INFO:44k:Losses: [2.029045820236206, 2.732814073562622, 16.925119400024414, 16.840864181518555, 1.1968058347702026], step: 19200, lr: 8.040139703083303e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 1746 to ./logs/44k/G_19200.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1746 to ./logs/44k/D_19200.pth\n",
      "INFO:44k:====> Epoch: 1746, cost 27.52 s\n",
      "INFO:44k:====> Epoch: 1747, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1748, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 1749, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1750, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1751, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1752, cost 16.79 s\n",
      "INFO:44k:====> Epoch: 1753, cost 17.06 s\n",
      "INFO:44k:====> Epoch: 1754, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1755, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1756, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1757, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1758, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1759, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1760, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 1761, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 1762, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1763, cost 16.81 s\n",
      "INFO:44k:Train Epoch: 1764 [64%]\n",
      "INFO:44k:Losses: [2.0051138401031494, 2.924245595932007, 18.331636428833008, 18.200279235839844, 0.7993584275245667], step: 19400, lr: 8.02206859690237e-05\n",
      "INFO:44k:====> Epoch: 1764, cost 17.73 s\n",
      "INFO:44k:====> Epoch: 1765, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1766, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1767, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1768, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 1769, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 1770, cost 16.91 s\n",
      "INFO:44k:====> Epoch: 1771, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 1772, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 1773, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1774, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1775, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1776, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1777, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1778, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1779, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 1780, cost 16.81 s\n",
      "INFO:44k:====> Epoch: 1781, cost 16.61 s\n",
      "INFO:44k:Train Epoch: 1782 [82%]\n",
      "INFO:44k:Losses: [1.9487091302871704, 2.8636112213134766, 19.213754653930664, 17.452152252197266, 0.6980310082435608], step: 19600, lr: 8.004038107537895e-05\n",
      "INFO:44k:====> Epoch: 1782, cost 17.57 s\n",
      "INFO:44k:====> Epoch: 1783, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 1784, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 1785, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 1786, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 1787, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 1788, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1789, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1790, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1791, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 1792, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 1793, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1794, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1795, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1796, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1797, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 1798, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 1799, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1800, cost 16.54 s\n",
      "INFO:44k:Train Epoch: 1801 [0%]\n",
      "INFO:44k:Losses: [1.8195292949676514, 3.147456407546997, 16.953947067260742, 15.518011093139648, 0.7466073036193848], step: 19800, lr: 7.98504988768111e-05\n",
      "INFO:44k:====> Epoch: 1801, cost 17.42 s\n",
      "INFO:44k:====> Epoch: 1802, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 1803, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1804, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1805, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1806, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 1807, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 1808, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1809, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1810, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 1811, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1812, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1813, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 1814, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1815, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1816, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1817, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 1818, cost 16.48 s\n",
      "INFO:44k:Train Epoch: 1819 [18%]\n",
      "INFO:44k:Losses: [2.2735390663146973, 2.5911741256713867, 11.333407402038574, 16.149494171142578, 0.9556602239608765], step: 20000, lr: 7.967102601973497e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 1819 to ./logs/44k/G_20000.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1819 to ./logs/44k/D_20000.pth\n",
      "INFO:44k:====> Epoch: 1819, cost 27.29 s\n",
      "INFO:44k:====> Epoch: 1820, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1821, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1822, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1823, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1824, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1825, cost 16.82 s\n",
      "INFO:44k:====> Epoch: 1826, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1827, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 1828, cost 16.85 s\n",
      "INFO:44k:====> Epoch: 1829, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 1830, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1831, cost 16.93 s\n",
      "INFO:44k:====> Epoch: 1832, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1833, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1834, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 1835, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1836, cost 16.40 s\n",
      "INFO:44k:Train Epoch: 1837 [36%]\n",
      "INFO:44k:Losses: [2.467754602432251, 2.6389997005462646, 17.05974769592285, 15.271516799926758, 1.0387433767318726], step: 20200, lr: 7.949195654782087e-05\n",
      "INFO:44k:====> Epoch: 1837, cost 17.72 s\n",
      "INFO:44k:====> Epoch: 1838, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1839, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1840, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1841, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1842, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1843, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1844, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1845, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 1846, cost 16.92 s\n",
      "INFO:44k:====> Epoch: 1847, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1848, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 1849, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1850, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1851, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 1852, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1853, cost 17.08 s\n",
      "INFO:44k:====> Epoch: 1854, cost 16.74 s\n",
      "INFO:44k:Train Epoch: 1855 [55%]\n",
      "INFO:44k:Losses: [2.1993350982666016, 2.820392370223999, 15.467907905578613, 19.347803115844727, 0.5799201130867004], step: 20400, lr: 7.93132895544159e-05\n",
      "INFO:44k:====> Epoch: 1855, cost 17.67 s\n",
      "INFO:44k:====> Epoch: 1856, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1857, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 1858, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 1859, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1860, cost 17.17 s\n",
      "INFO:44k:====> Epoch: 1861, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1862, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 1863, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1864, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1865, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1866, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1867, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1868, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 1869, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1870, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1871, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 1872, cost 16.62 s\n",
      "INFO:44k:Train Epoch: 1873 [73%]\n",
      "INFO:44k:Losses: [1.9982798099517822, 3.2911744117736816, 15.203025817871094, 15.224002838134766, 0.6288458108901978], step: 20600, lr: 7.913502413490494e-05\n",
      "INFO:44k:====> Epoch: 1873, cost 17.60 s\n",
      "INFO:44k:====> Epoch: 1874, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 1875, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1876, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 1877, cost 17.73 s\n",
      "INFO:44k:====> Epoch: 1878, cost 20.53 s\n",
      "INFO:44k:====> Epoch: 1879, cost 19.33 s\n",
      "INFO:44k:====> Epoch: 1880, cost 19.51 s\n",
      "INFO:44k:====> Epoch: 1881, cost 18.98 s\n",
      "INFO:44k:====> Epoch: 1882, cost 17.64 s\n",
      "INFO:44k:====> Epoch: 1883, cost 17.32 s\n",
      "INFO:44k:====> Epoch: 1884, cost 17.01 s\n",
      "INFO:44k:====> Epoch: 1885, cost 16.74 s\n",
      "INFO:44k:====> Epoch: 1886, cost 17.03 s\n",
      "INFO:44k:====> Epoch: 1887, cost 17.13 s\n",
      "INFO:44k:====> Epoch: 1888, cost 18.29 s\n",
      "INFO:44k:====> Epoch: 1889, cost 20.70 s\n",
      "INFO:44k:====> Epoch: 1890, cost 19.72 s\n",
      "INFO:44k:Train Epoch: 1891 [91%]\n",
      "INFO:44k:Losses: [1.966862678527832, 3.0080411434173584, 14.602372169494629, 14.550869941711426, 0.558994710445404], step: 20800, lr: 7.895715938670606e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 1891 to ./logs/44k/G_20800.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1891 to ./logs/44k/D_20800.pth\n",
      "INFO:44k:====> Epoch: 1891, cost 30.02 s\n",
      "INFO:44k:====> Epoch: 1892, cost 18.41 s\n",
      "INFO:44k:====> Epoch: 1893, cost 17.41 s\n",
      "INFO:44k:====> Epoch: 1894, cost 16.84 s\n",
      "INFO:44k:====> Epoch: 1895, cost 17.06 s\n",
      "INFO:44k:====> Epoch: 1896, cost 17.08 s\n",
      "INFO:44k:====> Epoch: 1897, cost 17.14 s\n",
      "INFO:44k:====> Epoch: 1898, cost 17.54 s\n",
      "INFO:44k:====> Epoch: 1899, cost 20.73 s\n",
      "INFO:44k:====> Epoch: 1900, cost 19.66 s\n",
      "INFO:44k:====> Epoch: 1901, cost 19.64 s\n",
      "INFO:44k:====> Epoch: 1902, cost 19.18 s\n",
      "INFO:44k:====> Epoch: 1903, cost 17.75 s\n",
      "INFO:44k:====> Epoch: 1904, cost 17.08 s\n",
      "INFO:44k:====> Epoch: 1905, cost 16.91 s\n",
      "INFO:44k:====> Epoch: 1906, cost 17.00 s\n",
      "INFO:44k:====> Epoch: 1907, cost 16.77 s\n",
      "INFO:44k:====> Epoch: 1908, cost 16.81 s\n",
      "INFO:44k:====> Epoch: 1909, cost 16.58 s\n",
      "INFO:44k:Train Epoch: 1910 [9%]\n",
      "INFO:44k:Losses: [1.9948737621307373, 2.9463324546813965, 13.366646766662598, 13.406036376953125, 0.2335377037525177], step: 21000, lr: 7.876984694746491e-05\n",
      "INFO:44k:====> Epoch: 1910, cost 17.58 s\n",
      "INFO:44k:====> Epoch: 1911, cost 16.92 s\n",
      "INFO:44k:====> Epoch: 1912, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1913, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1914, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1915, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1916, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1917, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1918, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 1919, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 1920, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1921, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 1922, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 1923, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 1924, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 1925, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1926, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1927, cost 17.02 s\n",
      "INFO:44k:Train Epoch: 1928 [27%]\n",
      "INFO:44k:Losses: [1.9543246030807495, 2.8608365058898926, 15.159077644348145, 11.74249267578125, 0.34432557225227356], step: 21200, lr: 7.859280297551778e-05\n",
      "INFO:44k:====> Epoch: 1928, cost 17.53 s\n",
      "INFO:44k:====> Epoch: 1929, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1930, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 1931, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1932, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1933, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1934, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 1935, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 1936, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 1937, cost 17.00 s\n",
      "INFO:44k:====> Epoch: 1938, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1939, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1940, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1941, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1942, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1943, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 1944, cost 16.94 s\n",
      "INFO:44k:====> Epoch: 1945, cost 16.58 s\n",
      "INFO:44k:Train Epoch: 1946 [45%]\n",
      "INFO:44k:Losses: [1.9270191192626953, 2.9691734313964844, 18.14400863647461, 15.923992156982422, 0.6761314868927002], step: 21400, lr: 7.84161569295438e-05\n",
      "INFO:44k:====> Epoch: 1946, cost 17.56 s\n",
      "INFO:44k:====> Epoch: 1947, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1948, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 1949, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1950, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 1951, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 1952, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 1953, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1954, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 1955, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 1956, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 1957, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1958, cost 17.02 s\n",
      "INFO:44k:====> Epoch: 1959, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1960, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1961, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1962, cost 16.20 s\n",
      "INFO:44k:====> Epoch: 1963, cost 16.31 s\n",
      "INFO:44k:Train Epoch: 1964 [64%]\n",
      "INFO:44k:Losses: [1.9648644924163818, 2.6880927085876465, 14.714820861816406, 14.598225593566895, 0.38754570484161377], step: 21600, lr: 7.823990791516023e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 1964 to ./logs/44k/G_21600.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 1964 to ./logs/44k/D_21600.pth\n",
      "INFO:44k:====> Epoch: 1964, cost 27.28 s\n",
      "INFO:44k:====> Epoch: 1965, cost 16.77 s\n",
      "INFO:44k:====> Epoch: 1966, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1967, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 1968, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 1969, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 1970, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 1971, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 1972, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 1973, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1974, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 1975, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1976, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1977, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 1978, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 1979, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 1980, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 1981, cost 16.56 s\n",
      "INFO:44k:Train Epoch: 1982 [82%]\n",
      "INFO:44k:Losses: [1.944831132888794, 3.2133984565734863, 19.284250259399414, 19.762165069580078, 0.689096987247467], step: 21800, lr: 7.80640550399945e-05\n",
      "INFO:44k:====> Epoch: 1982, cost 17.75 s\n",
      "INFO:44k:====> Epoch: 1983, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 1984, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 1985, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1986, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 1987, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 1988, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 1989, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 1990, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 1991, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 1992, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 1993, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 1994, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 1995, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 1996, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 1997, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 1998, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 1999, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 2000, cost 16.76 s\n",
      "INFO:44k:Train Epoch: 2001 [0%]\n",
      "INFO:44k:Losses: [1.817067265510559, 2.9759538173675537, 12.67503547668457, 13.598769187927246, 0.6802644729614258], step: 22000, lr: 7.787886133900303e-05\n",
      "INFO:44k:====> Epoch: 2001, cost 17.29 s\n",
      "INFO:44k:====> Epoch: 2002, cost 16.14 s\n",
      "INFO:44k:====> Epoch: 2003, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 2004, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 2005, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2006, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2007, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 2008, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 2009, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2010, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 2011, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2012, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 2013, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 2014, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2015, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 2016, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2017, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 2018, cost 16.94 s\n",
      "INFO:44k:Train Epoch: 2019 [18%]\n",
      "INFO:44k:Losses: [2.268314838409424, 2.518857717514038, 12.025994300842285, 13.020925521850586, 0.6045273542404175], step: 22200, lr: 7.770381995608182e-05\n",
      "INFO:44k:====> Epoch: 2019, cost 17.79 s\n",
      "INFO:44k:====> Epoch: 2020, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2021, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2022, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2023, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2024, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2025, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 2026, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 2027, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 2028, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2029, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2030, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 2031, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 2032, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 2033, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2034, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 2035, cost 16.93 s\n",
      "INFO:44k:====> Epoch: 2036, cost 16.57 s\n",
      "INFO:44k:Train Epoch: 2037 [36%]\n",
      "INFO:44k:Losses: [2.242410898208618, 2.6292219161987305, 14.463107109069824, 13.88279914855957, 0.7232567667961121], step: 22400, lr: 7.752917199809274e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 2037 to ./logs/44k/G_22400.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 2037 to ./logs/44k/D_22400.pth\n",
      "INFO:44k:====> Epoch: 2037, cost 27.11 s\n",
      "INFO:44k:====> Epoch: 2038, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 2039, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 2040, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2041, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 2042, cost 16.98 s\n",
      "INFO:44k:====> Epoch: 2043, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2044, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2045, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2046, cost 16.86 s\n",
      "INFO:44k:====> Epoch: 2047, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 2048, cost 16.90 s\n",
      "INFO:44k:====> Epoch: 2049, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 2050, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 2051, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2052, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 2053, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2054, cost 16.26 s\n",
      "INFO:44k:Train Epoch: 2055 [55%]\n",
      "INFO:44k:Losses: [2.125457763671875, 2.9914042949676514, 13.433268547058105, 14.10296630859375, 0.47907754778862], step: 22600, lr: 7.735491658076955e-05\n",
      "INFO:44k:====> Epoch: 2055, cost 17.78 s\n",
      "INFO:44k:====> Epoch: 2056, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2057, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 2058, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 2059, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 2060, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 2061, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2062, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 2063, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2064, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2065, cost 16.74 s\n",
      "INFO:44k:====> Epoch: 2066, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2067, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 2068, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 2069, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2070, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 2071, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 2072, cost 16.41 s\n",
      "INFO:44k:Train Epoch: 2073 [73%]\n",
      "INFO:44k:Losses: [1.9232306480407715, 3.3928942680358887, 17.302942276000977, 15.365618705749512, 0.7291216850280762], step: 22800, lr: 7.718105282183356e-05\n",
      "INFO:44k:====> Epoch: 2073, cost 17.91 s\n",
      "INFO:44k:====> Epoch: 2074, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 2075, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 2076, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 2077, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2078, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 2079, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 2080, cost 16.77 s\n",
      "INFO:44k:====> Epoch: 2081, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 2082, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2083, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 2084, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 2085, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2086, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 2087, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 2088, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 2089, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 2090, cost 16.24 s\n",
      "INFO:44k:Train Epoch: 2091 [91%]\n",
      "INFO:44k:Losses: [2.0552167892456055, 2.708672046661377, 18.266111373901367, 18.397390365600586, 0.4491499960422516], step: 23000, lr: 7.70075798409891e-05\n",
      "INFO:44k:====> Epoch: 2091, cost 17.81 s\n",
      "INFO:44k:====> Epoch: 2092, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2093, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 2094, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 2095, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2096, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 2097, cost 16.17 s\n",
      "INFO:44k:====> Epoch: 2098, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 2099, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 2100, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 2101, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2102, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2103, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 2104, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2105, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 2106, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 2107, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 2108, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 2109, cost 16.59 s\n",
      "INFO:44k:Train Epoch: 2110 [9%]\n",
      "INFO:44k:Losses: [2.1232974529266357, 2.768850326538086, 12.3143949508667, 10.11052131652832, 0.6515412330627441], step: 23200, lr: 7.682489244782409e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 2110 to ./logs/44k/G_23200.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 2110 to ./logs/44k/D_23200.pth\n",
      "INFO:44k:====> Epoch: 2110, cost 27.07 s\n",
      "INFO:44k:====> Epoch: 2111, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2112, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2113, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 2114, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 2115, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2116, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2117, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2118, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 2119, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2120, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 2121, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 2122, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 2123, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 2124, cost 17.21 s\n",
      "INFO:44k:====> Epoch: 2125, cost 19.09 s\n",
      "INFO:44k:====> Epoch: 2126, cost 21.29 s\n",
      "INFO:44k:====> Epoch: 2127, cost 18.74 s\n",
      "INFO:44k:Train Epoch: 2128 [27%]\n",
      "INFO:44k:Losses: [1.6198112964630127, 3.1534664630889893, 21.046276092529297, 14.970998764038086, 0.6922535300254822], step: 23400, lr: 7.665221997694265e-05\n",
      "INFO:44k:====> Epoch: 2128, cost 20.36 s\n",
      "INFO:44k:====> Epoch: 2129, cost 18.81 s\n",
      "INFO:44k:====> Epoch: 2130, cost 17.08 s\n",
      "INFO:44k:====> Epoch: 2131, cost 16.97 s\n",
      "INFO:44k:====> Epoch: 2132, cost 16.88 s\n",
      "INFO:44k:====> Epoch: 2133, cost 17.00 s\n",
      "INFO:44k:====> Epoch: 2134, cost 17.23 s\n",
      "INFO:44k:====> Epoch: 2135, cost 17.51 s\n",
      "INFO:44k:====> Epoch: 2136, cost 18.74 s\n",
      "INFO:44k:====> Epoch: 2137, cost 21.47 s\n",
      "INFO:44k:====> Epoch: 2138, cost 19.30 s\n",
      "INFO:44k:====> Epoch: 2139, cost 19.54 s\n",
      "INFO:44k:====> Epoch: 2140, cost 18.68 s\n",
      "INFO:44k:====> Epoch: 2141, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 2142, cost 16.99 s\n",
      "INFO:44k:====> Epoch: 2143, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 2144, cost 17.16 s\n",
      "INFO:44k:====> Epoch: 2145, cost 17.11 s\n",
      "INFO:44k:Train Epoch: 2146 [45%]\n",
      "INFO:44k:Losses: [1.8958849906921387, 2.933988571166992, 19.262527465820312, 16.675174713134766, 0.8361150026321411], step: 23600, lr: 7.647993560660063e-05\n",
      "INFO:44k:====> Epoch: 2146, cost 18.32 s\n",
      "INFO:44k:====> Epoch: 2147, cost 19.15 s\n",
      "INFO:44k:====> Epoch: 2148, cost 21.60 s\n",
      "INFO:44k:====> Epoch: 2149, cost 19.30 s\n",
      "INFO:44k:====> Epoch: 2150, cost 18.93 s\n",
      "INFO:44k:====> Epoch: 2151, cost 19.06 s\n",
      "INFO:44k:====> Epoch: 2152, cost 16.96 s\n",
      "INFO:44k:====> Epoch: 2153, cost 17.35 s\n",
      "INFO:44k:====> Epoch: 2154, cost 16.94 s\n",
      "INFO:44k:====> Epoch: 2155, cost 16.98 s\n",
      "INFO:44k:====> Epoch: 2156, cost 17.21 s\n",
      "INFO:44k:====> Epoch: 2157, cost 17.55 s\n",
      "INFO:44k:====> Epoch: 2158, cost 19.38 s\n",
      "INFO:44k:====> Epoch: 2159, cost 21.25 s\n",
      "INFO:44k:====> Epoch: 2160, cost 19.57 s\n",
      "INFO:44k:====> Epoch: 2161, cost 18.99 s\n",
      "INFO:44k:====> Epoch: 2162, cost 18.94 s\n",
      "INFO:44k:====> Epoch: 2163, cost 17.58 s\n",
      "INFO:44k:Train Epoch: 2164 [64%]\n",
      "INFO:44k:Losses: [1.7630150318145752, 3.3827338218688965, 18.211162567138672, 16.65087127685547, 0.48243942856788635], step: 23800, lr: 7.630803846449899e-05\n",
      "INFO:44k:====> Epoch: 2164, cost 18.49 s\n",
      "INFO:44k:====> Epoch: 2165, cost 16.85 s\n",
      "INFO:44k:====> Epoch: 2166, cost 16.95 s\n",
      "INFO:44k:====> Epoch: 2167, cost 17.01 s\n",
      "INFO:44k:====> Epoch: 2168, cost 16.92 s\n",
      "INFO:44k:====> Epoch: 2169, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2170, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2171, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 2172, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2173, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2174, cost 16.81 s\n",
      "INFO:44k:====> Epoch: 2175, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 2176, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2177, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2178, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 2179, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2180, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 2181, cost 16.39 s\n",
      "INFO:44k:Train Epoch: 2182 [82%]\n",
      "INFO:44k:Losses: [2.2374844551086426, 2.6225223541259766, 12.515046119689941, 16.27166748046875, 0.517853319644928], step: 24000, lr: 7.613652768029932e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 2182 to ./logs/44k/G_24000.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 2182 to ./logs/44k/D_24000.pth\n",
      "INFO:44k:====> Epoch: 2182, cost 27.23 s\n",
      "INFO:44k:====> Epoch: 2183, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 2184, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2185, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 2186, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2187, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 2188, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 2189, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2190, cost 16.81 s\n",
      "INFO:44k:====> Epoch: 2191, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 2192, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 2193, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2194, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2195, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2196, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2197, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 2198, cost 17.02 s\n",
      "INFO:44k:====> Epoch: 2199, cost 16.86 s\n",
      "INFO:44k:====> Epoch: 2200, cost 16.59 s\n",
      "INFO:44k:Train Epoch: 2201 [0%]\n",
      "INFO:44k:Losses: [1.8341655731201172, 2.9235129356384277, 16.684192657470703, 16.078092575073242, 0.9219750165939331], step: 24200, lr: 7.595590671032112e-05\n",
      "INFO:44k:====> Epoch: 2201, cost 17.39 s\n",
      "INFO:44k:====> Epoch: 2202, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2203, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2204, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 2205, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 2206, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 2207, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2208, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 2209, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 2210, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 2211, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2212, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 2213, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 2214, cost 16.79 s\n",
      "INFO:44k:====> Epoch: 2215, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2216, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2217, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 2218, cost 16.60 s\n",
      "INFO:44k:Train Epoch: 2219 [18%]\n",
      "INFO:44k:Losses: [2.096736431121826, 2.7038934230804443, 12.403197288513184, 15.101995468139648, 0.7599070072174072], step: 24400, lr: 7.578518738131433e-05\n",
      "INFO:44k:====> Epoch: 2219, cost 17.57 s\n",
      "INFO:44k:====> Epoch: 2220, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 2221, cost 16.90 s\n",
      "INFO:44k:====> Epoch: 2222, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 2223, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2224, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2225, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2226, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2227, cost 16.77 s\n",
      "INFO:44k:====> Epoch: 2228, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 2229, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 2230, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2231, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 2232, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2233, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2234, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2235, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2236, cost 16.39 s\n",
      "INFO:44k:Train Epoch: 2237 [36%]\n",
      "INFO:44k:Losses: [2.197993755340576, 3.1254935264587402, 24.126577377319336, 18.04669952392578, 0.49299412965774536], step: 24600, lr: 7.561485176294383e-05\n",
      "INFO:44k:====> Epoch: 2237, cost 17.77 s\n",
      "INFO:44k:====> Epoch: 2238, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 2239, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 2240, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2241, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 2242, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 2243, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2244, cost 16.89 s\n",
      "INFO:44k:====> Epoch: 2245, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 2246, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2247, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 2248, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2249, cost 16.84 s\n",
      "INFO:44k:====> Epoch: 2250, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2251, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2252, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 2253, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2254, cost 16.84 s\n",
      "INFO:44k:Train Epoch: 2255 [55%]\n",
      "INFO:44k:Losses: [2.419489622116089, 2.3064870834350586, 10.3142671585083, 10.734006881713867, 0.46090540289878845], step: 24800, lr: 7.544489899277746e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 2255 to ./logs/44k/G_24800.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 2255 to ./logs/44k/D_24800.pth\n",
      "INFO:44k:====> Epoch: 2255, cost 27.26 s\n",
      "INFO:44k:====> Epoch: 2256, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2257, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2258, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 2259, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2260, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2261, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2262, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 2263, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2264, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 2265, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 2266, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2267, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2268, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 2269, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 2270, cost 16.74 s\n",
      "INFO:44k:====> Epoch: 2271, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2272, cost 16.28 s\n",
      "INFO:44k:Train Epoch: 2273 [73%]\n",
      "INFO:44k:Losses: [2.0673375129699707, 2.9833602905273438, 14.966350555419922, 16.282554626464844, 0.8258103132247925], step: 25000, lr: 7.527532821032133e-05\n",
      "INFO:44k:====> Epoch: 2273, cost 17.90 s\n",
      "INFO:44k:====> Epoch: 2274, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 2275, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 2276, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2277, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 2278, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2279, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 2280, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2281, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 2282, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2283, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 2284, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2285, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2286, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 2287, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 2288, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 2289, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2290, cost 16.45 s\n",
      "INFO:44k:Train Epoch: 2291 [91%]\n",
      "INFO:44k:Losses: [2.4522719383239746, 2.3191919326782227, 8.234220504760742, 9.1287202835083, 0.9410688281059265], step: 25200, lr: 7.510613855701572e-05\n",
      "INFO:44k:====> Epoch: 2291, cost 17.75 s\n",
      "INFO:44k:====> Epoch: 2292, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2293, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 2294, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2295, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2296, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2297, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2298, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2299, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 2300, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 2301, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2302, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 2303, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2304, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 2305, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 2306, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2307, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 2308, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2309, cost 16.55 s\n",
      "INFO:44k:Train Epoch: 2310 [9%]\n",
      "INFO:44k:Losses: [1.9581162929534912, 3.2418417930603027, 13.401790618896484, 12.875435829162598, 0.32043567299842834], step: 25400, lr: 7.492796201008351e-05\n",
      "INFO:44k:====> Epoch: 2310, cost 17.37 s\n",
      "INFO:44k:====> Epoch: 2311, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 2312, cost 16.84 s\n",
      "INFO:44k:====> Epoch: 2313, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2314, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 2315, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 2316, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 2317, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 2318, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2319, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2320, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 2321, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2322, cost 16.82 s\n",
      "INFO:44k:====> Epoch: 2323, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2324, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 2325, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 2326, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2327, cost 16.24 s\n",
      "INFO:44k:Train Epoch: 2328 [27%]\n",
      "INFO:44k:Losses: [1.9246394634246826, 3.057398796081543, 14.696534156799316, 11.663135528564453, 0.6991317868232727], step: 25600, lr: 7.475955310085947e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 2328 to ./logs/44k/G_25600.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 2328 to ./logs/44k/D_25600.pth\n",
      "INFO:44k:====> Epoch: 2328, cost 27.63 s\n",
      "INFO:44k:====> Epoch: 2329, cost 16.81 s\n",
      "INFO:44k:====> Epoch: 2330, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2331, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2332, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2333, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 2334, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2335, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2336, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 2337, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 2338, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 2339, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2340, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2341, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2342, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 2343, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2344, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 2345, cost 16.58 s\n",
      "INFO:44k:Train Epoch: 2346 [45%]\n",
      "INFO:44k:Losses: [1.7017803192138672, 3.1964073181152344, 23.21696662902832, 16.825437545776367, 0.29137980937957764], step: 25800, lr: 7.459152270934688e-05\n",
      "INFO:44k:====> Epoch: 2346, cost 17.62 s\n",
      "INFO:44k:====> Epoch: 2347, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2348, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 2349, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2350, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 2351, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 2352, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2353, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 2354, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 2355, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2356, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 2357, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 2358, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2359, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 2360, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2361, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2362, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 2363, cost 16.60 s\n",
      "INFO:44k:Train Epoch: 2364 [64%]\n",
      "INFO:44k:Losses: [2.1969618797302246, 2.920013427734375, 17.26597785949707, 15.427511215209961, 1.1578434705734253], step: 26000, lr: 7.442386998478524e-05\n",
      "INFO:44k:====> Epoch: 2364, cost 17.53 s\n",
      "INFO:44k:====> Epoch: 2365, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2366, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2367, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 2368, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 2369, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2370, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 2371, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2372, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2373, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2374, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 2375, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2376, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 2377, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 2378, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2379, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2380, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2381, cost 16.40 s\n",
      "INFO:44k:Train Epoch: 2382 [82%]\n",
      "INFO:44k:Losses: [2.186188220977783, 2.56101393699646, 12.008105278015137, 16.37343406677246, 0.3715427815914154], step: 26200, lr: 7.425659407832615e-05\n",
      "INFO:44k:====> Epoch: 2382, cost 17.43 s\n",
      "INFO:44k:====> Epoch: 2383, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 2384, cost 17.49 s\n",
      "INFO:44k:====> Epoch: 2385, cost 20.51 s\n",
      "INFO:44k:====> Epoch: 2386, cost 19.45 s\n",
      "INFO:44k:====> Epoch: 2387, cost 19.27 s\n",
      "INFO:44k:====> Epoch: 2388, cost 19.40 s\n",
      "INFO:44k:====> Epoch: 2389, cost 17.94 s\n",
      "INFO:44k:====> Epoch: 2390, cost 16.98 s\n",
      "INFO:44k:====> Epoch: 2391, cost 16.89 s\n",
      "INFO:44k:====> Epoch: 2392, cost 16.94 s\n",
      "INFO:44k:====> Epoch: 2393, cost 17.23 s\n",
      "INFO:44k:====> Epoch: 2394, cost 16.94 s\n",
      "INFO:44k:====> Epoch: 2395, cost 17.96 s\n",
      "INFO:44k:====> Epoch: 2396, cost 20.87 s\n",
      "INFO:44k:====> Epoch: 2397, cost 19.00 s\n",
      "INFO:44k:====> Epoch: 2398, cost 19.16 s\n",
      "INFO:44k:====> Epoch: 2399, cost 18.91 s\n",
      "INFO:44k:====> Epoch: 2400, cost 18.32 s\n",
      "INFO:44k:Train Epoch: 2401 [0%]\n",
      "INFO:44k:Losses: [1.7441145181655884, 3.0364127159118652, 20.141586303710938, 16.290098190307617, 0.7964913249015808], step: 26400, lr: 7.408043293126123e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 2401 to ./logs/44k/G_26400.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 2401 to ./logs/44k/D_26400.pth\n",
      "INFO:44k:====> Epoch: 2401, cost 27.76 s\n",
      "INFO:44k:====> Epoch: 2402, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 2403, cost 16.83 s\n",
      "INFO:44k:====> Epoch: 2404, cost 17.18 s\n",
      "INFO:44k:====> Epoch: 2405, cost 17.29 s\n",
      "INFO:44k:====> Epoch: 2406, cost 20.92 s\n",
      "INFO:44k:====> Epoch: 2407, cost 19.13 s\n",
      "INFO:44k:====> Epoch: 2408, cost 19.27 s\n",
      "INFO:44k:====> Epoch: 2409, cost 19.23 s\n",
      "INFO:44k:====> Epoch: 2410, cost 18.41 s\n",
      "INFO:44k:====> Epoch: 2411, cost 17.36 s\n",
      "INFO:44k:====> Epoch: 2412, cost 17.03 s\n",
      "INFO:44k:====> Epoch: 2413, cost 17.33 s\n",
      "INFO:44k:====> Epoch: 2414, cost 16.93 s\n",
      "INFO:44k:====> Epoch: 2415, cost 16.97 s\n",
      "INFO:44k:====> Epoch: 2416, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 2417, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2418, cost 16.52 s\n",
      "INFO:44k:Train Epoch: 2419 [18%]\n",
      "INFO:44k:Losses: [1.8591498136520386, 3.0860705375671387, 15.683472633361816, 13.576869010925293, 0.587769627571106], step: 26600, lr: 7.391392893769045e-05\n",
      "INFO:44k:====> Epoch: 2419, cost 17.70 s\n",
      "INFO:44k:====> Epoch: 2420, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2421, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 2422, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 2423, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2424, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2425, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2426, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 2427, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2428, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2429, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2430, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2431, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2432, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 2433, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2434, cost 16.77 s\n",
      "INFO:44k:====> Epoch: 2435, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2436, cost 16.29 s\n",
      "INFO:44k:Train Epoch: 2437 [36%]\n",
      "INFO:44k:Losses: [2.167290449142456, 2.9377732276916504, 15.715912818908691, 17.925800323486328, 0.6670124530792236], step: 26800, lr: 7.374779918032184e-05\n",
      "INFO:44k:====> Epoch: 2437, cost 17.73 s\n",
      "INFO:44k:====> Epoch: 2438, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 2439, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2440, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2441, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2442, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2443, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2444, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2445, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2446, cost 16.94 s\n",
      "INFO:44k:====> Epoch: 2447, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2448, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2449, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 2450, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2451, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2452, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2453, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2454, cost 16.99 s\n",
      "INFO:44k:Train Epoch: 2455 [55%]\n",
      "INFO:44k:Losses: [2.1310696601867676, 2.9620938301086426, 15.218119621276855, 15.806031227111816, 0.5311431288719177], step: 27000, lr: 7.358204281801799e-05\n",
      "INFO:44k:====> Epoch: 2455, cost 17.78 s\n",
      "INFO:44k:====> Epoch: 2456, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2457, cost 16.84 s\n",
      "INFO:44k:====> Epoch: 2458, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 2459, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2460, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2461, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2462, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 2463, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2464, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 2465, cost 16.95 s\n",
      "INFO:44k:====> Epoch: 2466, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2467, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 2468, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 2469, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 2470, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 2471, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2472, cost 16.87 s\n",
      "INFO:44k:Train Epoch: 2473 [73%]\n",
      "INFO:44k:Losses: [1.708878993988037, 3.6220474243164062, 20.011796951293945, 18.397502899169922, 0.5784605145454407], step: 27200, lr: 7.341665901153209e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 2473 to ./logs/44k/G_27200.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 2473 to ./logs/44k/D_27200.pth\n",
      "INFO:44k:====> Epoch: 2473, cost 27.03 s\n",
      "INFO:44k:====> Epoch: 2474, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 2475, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 2476, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 2477, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2478, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2479, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2480, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 2481, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2482, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2483, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2484, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 2485, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2486, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 2487, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 2488, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 2489, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2490, cost 16.43 s\n",
      "INFO:44k:Train Epoch: 2491 [91%]\n",
      "INFO:44k:Losses: [1.9685680866241455, 3.0713396072387695, 7.522976398468018, 4.7037200927734375, 0.34556302428245544], step: 27400, lr: 7.325164692350357e-05\n",
      "INFO:44k:====> Epoch: 2491, cost 17.63 s\n",
      "INFO:44k:====> Epoch: 2492, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 2493, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2494, cost 16.90 s\n",
      "INFO:44k:====> Epoch: 2495, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 2496, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2497, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2498, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 2499, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 2500, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2501, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2502, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2503, cost 16.97 s\n",
      "INFO:44k:====> Epoch: 2504, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 2505, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2506, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2507, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2508, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2509, cost 16.72 s\n",
      "INFO:44k:Train Epoch: 2510 [9%]\n",
      "INFO:44k:Losses: [2.204436779022217, 2.7044100761413574, 13.011174201965332, 16.0811824798584, 0.2549687922000885], step: 27600, lr: 7.307786984273912e-05\n",
      "INFO:44k:====> Epoch: 2510, cost 18.00 s\n",
      "INFO:44k:====> Epoch: 2511, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2512, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2513, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2514, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2515, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2516, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 2517, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 2518, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 2519, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 2520, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2521, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 2522, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 2523, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2524, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2525, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 2526, cost 16.77 s\n",
      "INFO:44k:====> Epoch: 2527, cost 16.26 s\n",
      "INFO:44k:Train Epoch: 2528 [27%]\n",
      "INFO:44k:Losses: [1.8429865837097168, 3.7553796768188477, 15.581141471862793, 15.719144821166992, 0.6568546891212463], step: 27800, lr: 7.29136192209622e-05\n",
      "INFO:44k:====> Epoch: 2528, cost 17.72 s\n",
      "INFO:44k:====> Epoch: 2529, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2530, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 2531, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 2532, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2533, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 2534, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 2535, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 2536, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2537, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 2538, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 2539, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2540, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2541, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 2542, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 2543, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2544, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 2545, cost 16.50 s\n",
      "INFO:44k:Train Epoch: 2546 [45%]\n",
      "INFO:44k:Losses: [1.697243571281433, 2.9896976947784424, 18.730297088623047, 14.50584888458252, 0.6331541538238525], step: 28000, lr: 7.27497377706843e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 2546 to ./logs/44k/G_28000.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 2546 to ./logs/44k/D_28000.pth\n",
      "INFO:44k:====> Epoch: 2546, cost 27.43 s\n",
      "INFO:44k:====> Epoch: 2547, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2548, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2549, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2550, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2551, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 2552, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 2553, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 2554, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 2555, cost 16.81 s\n",
      "INFO:44k:====> Epoch: 2556, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 2557, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2558, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 2559, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 2560, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 2561, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2562, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 2563, cost 16.43 s\n",
      "INFO:44k:Train Epoch: 2564 [64%]\n",
      "INFO:44k:Losses: [2.1370186805725098, 3.2248942852020264, 17.766942977905273, 13.84028434753418, 0.8024378418922424], step: 28200, lr: 7.25862246621515e-05\n",
      "INFO:44k:====> Epoch: 2564, cost 18.08 s\n",
      "INFO:44k:====> Epoch: 2565, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2566, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 2567, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 2568, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 2569, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2570, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 2571, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2572, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 2573, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2574, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2575, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 2576, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 2577, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 2578, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2579, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2580, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 2581, cost 16.43 s\n",
      "INFO:44k:Train Epoch: 2582 [82%]\n",
      "INFO:44k:Losses: [1.9798636436462402, 2.96359920501709, 9.039764404296875, 12.693292617797852, 0.6779991388320923], step: 28400, lr: 7.242307906747484e-05\n",
      "INFO:44k:====> Epoch: 2582, cost 17.47 s\n",
      "INFO:44k:====> Epoch: 2583, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 2584, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 2585, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 2586, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 2587, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 2588, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 2589, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 2590, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2591, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2592, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2593, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2594, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 2595, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2596, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2597, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 2598, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2599, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2600, cost 16.37 s\n",
      "INFO:44k:Train Epoch: 2601 [0%]\n",
      "INFO:44k:Losses: [1.5898019075393677, 3.0161423683166504, 17.577871322631836, 16.844850540161133, -0.11048673093318939], step: 28600, lr: 7.225126762310609e-05\n",
      "INFO:44k:====> Epoch: 2601, cost 17.42 s\n",
      "INFO:44k:====> Epoch: 2602, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 2603, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2604, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2605, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2606, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 2607, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2608, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2609, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2610, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2611, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 2612, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 2613, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 2614, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2615, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2616, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2617, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 2618, cost 16.77 s\n",
      "INFO:44k:Train Epoch: 2619 [18%]\n",
      "INFO:44k:Losses: [1.9579787254333496, 3.020908832550049, 15.5763578414917, 14.500609397888184, 0.6673437356948853], step: 28800, lr: 7.208887488154422e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 2619 to ./logs/44k/G_28800.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 2619 to ./logs/44k/D_28800.pth\n",
      "INFO:44k:====> Epoch: 2619, cost 27.82 s\n",
      "INFO:44k:====> Epoch: 2620, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2621, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 2622, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2623, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 2624, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 2625, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 2626, cost 16.88 s\n",
      "INFO:44k:====> Epoch: 2627, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 2628, cost 16.17 s\n",
      "INFO:44k:====> Epoch: 2629, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 2630, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2631, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2632, cost 18.11 s\n",
      "INFO:44k:====> Epoch: 2633, cost 20.89 s\n",
      "INFO:44k:====> Epoch: 2634, cost 19.58 s\n",
      "INFO:44k:====> Epoch: 2635, cost 19.39 s\n",
      "INFO:44k:====> Epoch: 2636, cost 19.43 s\n",
      "INFO:44k:Train Epoch: 2637 [36%]\n",
      "INFO:44k:Losses: [2.151738405227661, 2.9677188396453857, 16.686738967895508, 17.017667770385742, 0.7878446578979492], step: 29000, lr: 7.192684713568938e-05\n",
      "INFO:44k:====> Epoch: 2637, cost 18.40 s\n",
      "INFO:44k:====> Epoch: 2638, cost 17.06 s\n",
      "INFO:44k:====> Epoch: 2639, cost 17.08 s\n",
      "INFO:44k:====> Epoch: 2640, cost 17.21 s\n",
      "INFO:44k:====> Epoch: 2641, cost 17.38 s\n",
      "INFO:44k:====> Epoch: 2642, cost 17.65 s\n",
      "INFO:44k:====> Epoch: 2643, cost 18.67 s\n",
      "INFO:44k:====> Epoch: 2644, cost 21.21 s\n",
      "INFO:44k:====> Epoch: 2645, cost 19.16 s\n",
      "INFO:44k:====> Epoch: 2646, cost 19.53 s\n",
      "INFO:44k:====> Epoch: 2647, cost 19.05 s\n",
      "INFO:44k:====> Epoch: 2648, cost 16.85 s\n",
      "INFO:44k:====> Epoch: 2649, cost 16.98 s\n",
      "INFO:44k:====> Epoch: 2650, cost 16.93 s\n",
      "INFO:44k:====> Epoch: 2651, cost 17.22 s\n",
      "INFO:44k:====> Epoch: 2652, cost 16.85 s\n",
      "INFO:44k:====> Epoch: 2653, cost 17.48 s\n",
      "INFO:44k:====> Epoch: 2654, cost 18.88 s\n",
      "INFO:44k:Train Epoch: 2655 [55%]\n",
      "INFO:44k:Losses: [2.1803507804870605, 3.051154613494873, 17.3195743560791, 17.349685668945312, 0.4908316433429718], step: 29200, lr: 7.176518356517328e-05\n",
      "INFO:44k:====> Epoch: 2655, cost 22.64 s\n",
      "INFO:44k:====> Epoch: 2656, cost 19.20 s\n",
      "INFO:44k:====> Epoch: 2657, cost 19.36 s\n",
      "INFO:44k:====> Epoch: 2658, cost 19.01 s\n",
      "INFO:44k:====> Epoch: 2659, cost 16.98 s\n",
      "INFO:44k:====> Epoch: 2660, cost 17.16 s\n",
      "INFO:44k:====> Epoch: 2661, cost 16.84 s\n",
      "INFO:44k:====> Epoch: 2662, cost 17.15 s\n",
      "INFO:44k:====> Epoch: 2663, cost 17.64 s\n",
      "INFO:44k:====> Epoch: 2664, cost 17.39 s\n",
      "INFO:44k:====> Epoch: 2665, cost 19.80 s\n",
      "INFO:44k:====> Epoch: 2666, cost 20.35 s\n",
      "INFO:44k:====> Epoch: 2667, cost 19.50 s\n",
      "INFO:44k:====> Epoch: 2668, cost 19.50 s\n",
      "INFO:44k:====> Epoch: 2669, cost 18.91 s\n",
      "INFO:44k:====> Epoch: 2670, cost 17.10 s\n",
      "INFO:44k:====> Epoch: 2671, cost 16.90 s\n",
      "INFO:44k:====> Epoch: 2672, cost 16.77 s\n",
      "INFO:44k:Train Epoch: 2673 [73%]\n",
      "INFO:44k:Losses: [1.8911188840866089, 3.4151570796966553, 18.510499954223633, 17.065515518188477, 0.5636153817176819], step: 29400, lr: 7.160388335147137e-05\n",
      "INFO:44k:====> Epoch: 2673, cost 18.09 s\n",
      "INFO:44k:====> Epoch: 2674, cost 17.12 s\n",
      "INFO:44k:====> Epoch: 2675, cost 16.77 s\n",
      "INFO:44k:====> Epoch: 2676, cost 16.89 s\n",
      "INFO:44k:====> Epoch: 2677, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2678, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2679, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2680, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2681, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2682, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2683, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 2684, cost 16.72 s\n",
      "INFO:44k:====> Epoch: 2685, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 2686, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2687, cost 16.71 s\n",
      "INFO:44k:====> Epoch: 2688, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2689, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2690, cost 16.55 s\n",
      "INFO:44k:Train Epoch: 2691 [91%]\n",
      "INFO:44k:Losses: [2.046036958694458, 2.7838268280029297, 13.663865089416504, 11.634400367736816, 0.8062894344329834], step: 29600, lr: 7.144294567789892e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 2691 to ./logs/44k/G_29600.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 2691 to ./logs/44k/D_29600.pth\n",
      "INFO:44k:====> Epoch: 2691, cost 27.49 s\n",
      "INFO:44k:====> Epoch: 2692, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2693, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 2694, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2695, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 2696, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2697, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2698, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 2699, cost 16.93 s\n",
      "INFO:44k:====> Epoch: 2700, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 2701, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2702, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 2703, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 2704, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2705, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 2706, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 2707, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 2708, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2709, cost 16.45 s\n",
      "INFO:44k:Train Epoch: 2710 [9%]\n",
      "INFO:44k:Losses: [2.0885584354400635, 2.436708450317383, 12.332357406616211, 11.229243278503418, 0.5544724464416504], step: 29800, lr: 7.127345943339056e-05\n",
      "INFO:44k:====> Epoch: 2710, cost 17.56 s\n",
      "INFO:44k:====> Epoch: 2711, cost 16.74 s\n",
      "INFO:44k:====> Epoch: 2712, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2713, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2714, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 2715, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 2716, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2717, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 2718, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 2719, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 2720, cost 16.19 s\n",
      "INFO:44k:====> Epoch: 2721, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2722, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 2723, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2724, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 2725, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 2726, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2727, cost 16.47 s\n",
      "INFO:44k:Train Epoch: 2728 [27%]\n",
      "INFO:44k:Losses: [1.7336008548736572, 2.8755269050598145, 16.04238510131836, 12.42519760131836, 0.9670604467391968], step: 30000, lr: 7.111326442424049e-05\n",
      "INFO:44k:====> Epoch: 2728, cost 17.57 s\n",
      "INFO:44k:====> Epoch: 2729, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2730, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 2731, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2732, cost 17.17 s\n",
      "INFO:44k:====> Epoch: 2733, cost 16.16 s\n",
      "INFO:44k:====> Epoch: 2734, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2735, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2736, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2737, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 2738, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2739, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2740, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2741, cost 16.89 s\n",
      "INFO:44k:====> Epoch: 2742, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 2743, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 2744, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2745, cost 16.72 s\n",
      "INFO:44k:Train Epoch: 2746 [45%]\n",
      "INFO:44k:Losses: [1.636310338973999, 3.314974784851074, 21.045244216918945, 17.493619918823242, 1.0450061559677124], step: 30200, lr: 7.095342947115003e-05\n",
      "INFO:44k:====> Epoch: 2746, cost 17.53 s\n",
      "INFO:44k:====> Epoch: 2747, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2748, cost 16.19 s\n",
      "INFO:44k:====> Epoch: 2749, cost 16.99 s\n",
      "INFO:44k:====> Epoch: 2750, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2751, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 2752, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 2753, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 2754, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 2755, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2756, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 2757, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2758, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 2759, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2760, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 2761, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2762, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2763, cost 16.61 s\n",
      "INFO:44k:Train Epoch: 2764 [64%]\n",
      "INFO:44k:Losses: [1.752730369567871, 3.3149261474609375, 16.90095329284668, 16.91309356689453, 0.7627688050270081], step: 30400, lr: 7.079395376485317e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 2764 to ./logs/44k/G_30400.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 2764 to ./logs/44k/D_30400.pth\n",
      "INFO:44k:====> Epoch: 2764, cost 27.27 s\n",
      "INFO:44k:====> Epoch: 2765, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 2766, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 2767, cost 16.12 s\n",
      "INFO:44k:====> Epoch: 2768, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 2769, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2770, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 2771, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2772, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 2773, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 2774, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2775, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2776, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2777, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2778, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2779, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2780, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 2781, cost 16.50 s\n",
      "INFO:44k:Train Epoch: 2782 [82%]\n",
      "INFO:44k:Losses: [2.1924052238464355, 2.6635637283325195, 11.341224670410156, 12.829891204833984, 0.5209479331970215], step: 30600, lr: 7.063483649790294e-05\n",
      "INFO:44k:====> Epoch: 2782, cost 17.68 s\n",
      "INFO:44k:====> Epoch: 2783, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 2784, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2785, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 2786, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 2787, cost 15.95 s\n",
      "INFO:44k:====> Epoch: 2788, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2789, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 2790, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 2791, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 2792, cost 16.06 s\n",
      "INFO:44k:====> Epoch: 2793, cost 16.11 s\n",
      "INFO:44k:====> Epoch: 2794, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 2795, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2796, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 2797, cost 16.10 s\n",
      "INFO:44k:====> Epoch: 2798, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 2799, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 2800, cost 16.05 s\n",
      "INFO:44k:Train Epoch: 2801 [0%]\n",
      "INFO:44k:Losses: [1.331559419631958, 3.4925713539123535, 23.100074768066406, 15.69924545288086, 0.21758772432804108], step: 30800, lr: 7.046726735505902e-05\n",
      "INFO:44k:====> Epoch: 2801, cost 17.41 s\n",
      "INFO:44k:====> Epoch: 2802, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2803, cost 16.13 s\n",
      "INFO:44k:====> Epoch: 2804, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 2805, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 2806, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2807, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 2808, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 2809, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 2810, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2811, cost 16.17 s\n",
      "INFO:44k:====> Epoch: 2812, cost 16.10 s\n",
      "INFO:44k:====> Epoch: 2813, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2814, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 2815, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 2816, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 2817, cost 16.14 s\n",
      "INFO:44k:====> Epoch: 2818, cost 16.06 s\n",
      "INFO:44k:Train Epoch: 2819 [18%]\n",
      "INFO:44k:Losses: [2.1741504669189453, 2.7742481231689453, 13.274069786071777, 12.71902847290039, 0.3547464907169342], step: 31000, lr: 7.030888435206653e-05\n",
      "INFO:44k:====> Epoch: 2819, cost 17.37 s\n",
      "INFO:44k:====> Epoch: 2820, cost 16.10 s\n",
      "INFO:44k:====> Epoch: 2821, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 2822, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 2823, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 2824, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 2825, cost 16.10 s\n",
      "INFO:44k:====> Epoch: 2826, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 2827, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 2828, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2829, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 2830, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2831, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2832, cost 16.14 s\n",
      "INFO:44k:====> Epoch: 2833, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2834, cost 16.13 s\n",
      "INFO:44k:====> Epoch: 2835, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 2836, cost 16.27 s\n",
      "INFO:44k:Train Epoch: 2837 [36%]\n",
      "INFO:44k:Losses: [2.1843557357788086, 2.886388063430786, 15.56852912902832, 13.383636474609375, 0.8539659976959229], step: 31200, lr: 7.015085733244871e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 2837 to ./logs/44k/G_31200.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 2837 to ./logs/44k/D_31200.pth\n",
      "INFO:44k:====> Epoch: 2837, cost 27.87 s\n",
      "INFO:44k:====> Epoch: 2838, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 2839, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 2840, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 2841, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 2842, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 2843, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 2844, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2845, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 2846, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2847, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2848, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 2849, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2850, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2851, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2852, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2853, cost 16.74 s\n",
      "INFO:44k:====> Epoch: 2854, cost 16.46 s\n",
      "INFO:44k:Train Epoch: 2855 [55%]\n",
      "INFO:44k:Losses: [2.3262691497802734, 2.5777554512023926, 11.375585556030273, 12.722478866577148, 0.7304819822311401], step: 31400, lr: 6.99931854960934e-05\n",
      "INFO:44k:====> Epoch: 2855, cost 17.55 s\n",
      "INFO:44k:====> Epoch: 2856, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 2857, cost 16.82 s\n",
      "INFO:44k:====> Epoch: 2858, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2859, cost 16.10 s\n",
      "INFO:44k:====> Epoch: 2860, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2861, cost 16.88 s\n",
      "INFO:44k:====> Epoch: 2862, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2863, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 2864, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2865, cost 16.17 s\n",
      "INFO:44k:====> Epoch: 2866, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2867, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2868, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2869, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 2870, cost 16.19 s\n",
      "INFO:44k:====> Epoch: 2871, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 2872, cost 16.22 s\n",
      "INFO:44k:Train Epoch: 2873 [73%]\n",
      "INFO:44k:Losses: [1.5055510997772217, 3.688369035720825, 18.146358489990234, 11.925517082214355, 0.27984800934791565], step: 31600, lr: 6.983586804468683e-05\n",
      "INFO:44k:====> Epoch: 2873, cost 17.20 s\n",
      "INFO:44k:====> Epoch: 2874, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2875, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 2876, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 2877, cost 16.74 s\n",
      "INFO:44k:====> Epoch: 2878, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 2879, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 2880, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2881, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2882, cost 16.06 s\n",
      "INFO:44k:====> Epoch: 2883, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 2884, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 2885, cost 15.94 s\n",
      "INFO:44k:====> Epoch: 2886, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 2887, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 2888, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 2889, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2890, cost 16.31 s\n",
      "INFO:44k:Train Epoch: 2891 [91%]\n",
      "INFO:44k:Losses: [2.2190208435058594, 2.7873899936676025, 16.820343017578125, 15.546344757080078, 0.7179755568504333], step: 31800, lr: 6.96789041817095e-05\n",
      "INFO:44k:====> Epoch: 2891, cost 17.56 s\n",
      "INFO:44k:====> Epoch: 2892, cost 17.05 s\n",
      "INFO:44k:====> Epoch: 2893, cost 20.94 s\n",
      "INFO:44k:====> Epoch: 2894, cost 19.09 s\n",
      "INFO:44k:====> Epoch: 2895, cost 19.22 s\n",
      "INFO:44k:====> Epoch: 2896, cost 19.20 s\n",
      "INFO:44k:====> Epoch: 2897, cost 17.84 s\n",
      "INFO:44k:====> Epoch: 2898, cost 16.93 s\n",
      "INFO:44k:====> Epoch: 2899, cost 16.94 s\n",
      "INFO:44k:====> Epoch: 2900, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 2901, cost 17.14 s\n",
      "INFO:44k:====> Epoch: 2902, cost 16.84 s\n",
      "INFO:44k:====> Epoch: 2903, cost 17.67 s\n",
      "INFO:44k:====> Epoch: 2904, cost 20.54 s\n",
      "INFO:44k:====> Epoch: 2905, cost 19.12 s\n",
      "INFO:44k:====> Epoch: 2906, cost 19.15 s\n",
      "INFO:44k:====> Epoch: 2907, cost 18.84 s\n",
      "INFO:44k:====> Epoch: 2908, cost 17.25 s\n",
      "INFO:44k:====> Epoch: 2909, cost 16.84 s\n",
      "INFO:44k:Train Epoch: 2910 [9%]\n",
      "INFO:44k:Losses: [1.9444622993469238, 3.1471965312957764, 17.43512725830078, 14.085704803466797, 0.6985528469085693], step: 32000, lr: 6.951360282579315e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 2910 to ./logs/44k/G_32000.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 2910 to ./logs/44k/D_32000.pth\n",
      "INFO:44k:====> Epoch: 2910, cost 28.03 s\n",
      "INFO:44k:====> Epoch: 2911, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 2912, cost 16.81 s\n",
      "INFO:44k:====> Epoch: 2913, cost 16.99 s\n",
      "INFO:44k:====> Epoch: 2914, cost 21.15 s\n",
      "INFO:44k:====> Epoch: 2915, cost 19.34 s\n",
      "INFO:44k:====> Epoch: 2916, cost 19.21 s\n",
      "INFO:44k:====> Epoch: 2917, cost 19.40 s\n",
      "INFO:44k:====> Epoch: 2918, cost 16.97 s\n",
      "INFO:44k:====> Epoch: 2919, cost 17.14 s\n",
      "INFO:44k:====> Epoch: 2920, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 2921, cost 17.00 s\n",
      "INFO:44k:====> Epoch: 2922, cost 16.76 s\n",
      "INFO:44k:====> Epoch: 2923, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 2924, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 2925, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 2926, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2927, cost 16.73 s\n",
      "INFO:44k:Train Epoch: 2928 [27%]\n",
      "INFO:44k:Losses: [1.6105198860168457, 3.3565526008605957, 19.225048065185547, 13.873636245727539, 0.4147253632545471], step: 32200, lr: 6.935736328965642e-05\n",
      "INFO:44k:====> Epoch: 2928, cost 17.73 s\n",
      "INFO:44k:====> Epoch: 2929, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 2930, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2931, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2932, cost 16.14 s\n",
      "INFO:44k:====> Epoch: 2933, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 2934, cost 16.20 s\n",
      "INFO:44k:====> Epoch: 2935, cost 16.10 s\n",
      "INFO:44k:====> Epoch: 2936, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2937, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2938, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 2939, cost 16.20 s\n",
      "INFO:44k:====> Epoch: 2940, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2941, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 2942, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2943, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 2944, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2945, cost 16.55 s\n",
      "INFO:44k:Train Epoch: 2946 [45%]\n",
      "INFO:44k:Losses: [1.6560978889465332, 3.753241777420044, 20.48497772216797, 16.45564842224121, 0.775571346282959], step: 32400, lr: 6.920147491921476e-05\n",
      "INFO:44k:====> Epoch: 2946, cost 17.55 s\n",
      "INFO:44k:====> Epoch: 2947, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2948, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2949, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2950, cost 16.74 s\n",
      "INFO:44k:====> Epoch: 2951, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 2952, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2953, cost 15.97 s\n",
      "INFO:44k:====> Epoch: 2954, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 2955, cost 16.20 s\n",
      "INFO:44k:====> Epoch: 2956, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2957, cost 16.13 s\n",
      "INFO:44k:====> Epoch: 2958, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2959, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 2960, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 2961, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 2962, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2963, cost 16.31 s\n",
      "INFO:44k:Train Epoch: 2964 [64%]\n",
      "INFO:44k:Losses: [1.8997821807861328, 3.113769292831421, 19.005285263061523, 16.54012680053711, 1.205626130104065], step: 32600, lr: 6.904593692518426e-05\n",
      "INFO:44k:====> Epoch: 2964, cost 17.47 s\n",
      "INFO:44k:====> Epoch: 2965, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 2966, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 2967, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 2968, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 2969, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 2970, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2971, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2972, cost 16.17 s\n",
      "INFO:44k:====> Epoch: 2973, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 2974, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 2975, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 2976, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 2977, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 2978, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2979, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 2980, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 2981, cost 16.61 s\n",
      "INFO:44k:Train Epoch: 2982 [82%]\n",
      "INFO:44k:Losses: [1.928807258605957, 3.1901745796203613, 15.283260345458984, 16.395771026611328, 0.520547091960907], step: 32800, lr: 6.889074852005511e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 2982 to ./logs/44k/G_32800.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 2982 to ./logs/44k/D_32800.pth\n",
      "INFO:44k:====> Epoch: 2982, cost 27.34 s\n",
      "INFO:44k:====> Epoch: 2983, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 2984, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 2985, cost 16.07 s\n",
      "INFO:44k:====> Epoch: 2986, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 2987, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 2988, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 2989, cost 16.08 s\n",
      "INFO:44k:====> Epoch: 2990, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 2991, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 2992, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 2993, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 2994, cost 15.96 s\n",
      "INFO:44k:====> Epoch: 2995, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 2996, cost 17.03 s\n",
      "INFO:44k:====> Epoch: 2997, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 2998, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 2999, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 3000, cost 16.31 s\n",
      "INFO:44k:Train Epoch: 3001 [0%]\n",
      "INFO:44k:Losses: [1.6790143251419067, 3.107936143875122, 15.418896675109863, 14.639289855957031, 0.680026650428772], step: 33000, lr: 6.872731692947266e-05\n",
      "INFO:44k:====> Epoch: 3001, cost 17.80 s\n",
      "INFO:44k:====> Epoch: 3002, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 3003, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 3004, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 3005, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 3006, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 3007, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 3008, cost 16.17 s\n",
      "INFO:44k:====> Epoch: 3009, cost 16.17 s\n",
      "INFO:44k:====> Epoch: 3010, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 3011, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 3012, cost 15.90 s\n",
      "INFO:44k:====> Epoch: 3013, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 3014, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 3015, cost 16.13 s\n",
      "INFO:44k:====> Epoch: 3016, cost 16.10 s\n",
      "INFO:44k:====> Epoch: 3017, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 3018, cost 16.24 s\n",
      "INFO:44k:Train Epoch: 3019 [18%]\n",
      "INFO:44k:Losses: [2.0915279388427734, 2.6173272132873535, 14.569640159606934, 9.620640754699707, 1.0588865280151367], step: 33200, lr: 6.857284465814052e-05\n",
      "INFO:44k:====> Epoch: 3019, cost 17.80 s\n",
      "INFO:44k:====> Epoch: 3020, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 3021, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 3022, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 3023, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 3024, cost 16.13 s\n",
      "INFO:44k:====> Epoch: 3025, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 3026, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 3027, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 3028, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 3029, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 3030, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 3031, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 3032, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 3033, cost 16.09 s\n",
      "INFO:44k:====> Epoch: 3034, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 3035, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 3036, cost 16.44 s\n",
      "INFO:44k:Train Epoch: 3037 [36%]\n",
      "INFO:44k:Losses: [1.7125625610351562, 3.676635980606079, 18.445146560668945, 16.248119354248047, 0.8889120817184448], step: 33400, lr: 6.841871958037964e-05\n",
      "INFO:44k:====> Epoch: 3037, cost 17.73 s\n",
      "INFO:44k:====> Epoch: 3038, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 3039, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 3040, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 3041, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 3042, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 3043, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 3044, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 3045, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 3046, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 3047, cost 16.00 s\n",
      "INFO:44k:====> Epoch: 3048, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 3049, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 3050, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 3051, cost 16.14 s\n",
      "INFO:44k:====> Epoch: 3052, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 3053, cost 17.05 s\n",
      "INFO:44k:====> Epoch: 3054, cost 16.21 s\n",
      "INFO:44k:Train Epoch: 3055 [55%]\n",
      "INFO:44k:Losses: [2.1901469230651855, 2.603649139404297, 11.10145092010498, 12.586109161376953, 0.5050051808357239], step: 33600, lr: 6.826494091583403e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 3055 to ./logs/44k/G_33600.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 3055 to ./logs/44k/D_33600.pth\n",
      "INFO:44k:====> Epoch: 3055, cost 27.43 s\n",
      "INFO:44k:====> Epoch: 3056, cost 16.02 s\n",
      "INFO:44k:====> Epoch: 3057, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 3058, cost 16.84 s\n",
      "INFO:44k:====> Epoch: 3059, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 3060, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 3061, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 3062, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 3063, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 3064, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 3065, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 3066, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 3067, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 3068, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 3069, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 3070, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 3071, cost 16.16 s\n",
      "INFO:44k:====> Epoch: 3072, cost 16.25 s\n",
      "INFO:44k:Train Epoch: 3073 [73%]\n",
      "INFO:44k:Losses: [2.0889673233032227, 2.908386468887329, 20.56656837463379, 14.914448738098145, 0.5679481029510498], step: 33800, lr: 6.811150788590149e-05\n",
      "INFO:44k:====> Epoch: 3073, cost 18.13 s\n",
      "INFO:44k:====> Epoch: 3074, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 3075, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 3076, cost 15.88 s\n",
      "INFO:44k:====> Epoch: 3077, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 3078, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 3079, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 3080, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 3081, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 3082, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 3083, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 3084, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 3085, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 3086, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 3087, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 3088, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 3089, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 3090, cost 16.25 s\n",
      "INFO:44k:Train Epoch: 3091 [91%]\n",
      "INFO:44k:Losses: [2.4266276359558105, 2.4038655757904053, 12.429686546325684, 12.800294876098633, 0.5070819854736328], step: 34000, lr: 6.795841971372985e-05\n",
      "INFO:44k:====> Epoch: 3091, cost 17.59 s\n",
      "INFO:44k:====> Epoch: 3092, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 3093, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 3094, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 3095, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 3096, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 3097, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 3098, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 3099, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 3100, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 3101, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 3102, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 3103, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 3104, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 3105, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 3106, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 3107, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 3108, cost 16.15 s\n",
      "INFO:44k:====> Epoch: 3109, cost 16.43 s\n",
      "INFO:44k:Train Epoch: 3110 [9%]\n",
      "INFO:44k:Losses: [2.0923635959625244, 2.866272211074829, 10.043839454650879, 11.279818534851074, 0.3899272680282593], step: 34200, lr: 6.779719991476001e-05\n",
      "INFO:44k:====> Epoch: 3110, cost 17.96 s\n",
      "INFO:44k:====> Epoch: 3111, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 3112, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 3113, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 3114, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 3115, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 3116, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 3117, cost 16.01 s\n",
      "INFO:44k:====> Epoch: 3118, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 3119, cost 16.87 s\n",
      "INFO:44k:====> Epoch: 3120, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 3121, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 3122, cost 16.12 s\n",
      "INFO:44k:====> Epoch: 3123, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 3124, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 3125, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 3126, cost 16.14 s\n",
      "INFO:44k:====> Epoch: 3127, cost 16.43 s\n",
      "INFO:44k:Train Epoch: 3128 [27%]\n",
      "INFO:44k:Losses: [1.665853500366211, 3.257990837097168, 16.768383026123047, 12.894083976745605, 1.021443486213684], step: 34400, lr: 6.764481818463165e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 3128 to ./logs/44k/G_34400.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 3128 to ./logs/44k/D_34400.pth\n",
      "INFO:44k:====> Epoch: 3128, cost 27.05 s\n",
      "INFO:44k:====> Epoch: 3129, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 3130, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 3131, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 3132, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 3133, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 3134, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 3135, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 3136, cost 16.15 s\n",
      "INFO:44k:====> Epoch: 3137, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 3138, cost 16.16 s\n",
      "INFO:44k:====> Epoch: 3139, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 3140, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 3141, cost 17.07 s\n",
      "INFO:44k:====> Epoch: 3142, cost 20.78 s\n",
      "INFO:44k:====> Epoch: 3143, cost 19.64 s\n",
      "INFO:44k:====> Epoch: 3144, cost 18.84 s\n",
      "INFO:44k:====> Epoch: 3145, cost 19.44 s\n",
      "INFO:44k:Train Epoch: 3146 [45%]\n",
      "INFO:44k:Losses: [1.8496156930923462, 2.823679208755493, 18.507183074951172, 15.843754768371582, 0.29035496711730957], step: 34600, lr: 6.749277894935125e-05\n",
      "INFO:44k:====> Epoch: 3146, cost 19.38 s\n",
      "INFO:44k:====> Epoch: 3147, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 3148, cost 16.91 s\n",
      "INFO:44k:====> Epoch: 3149, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 3150, cost 17.15 s\n",
      "INFO:44k:====> Epoch: 3151, cost 16.88 s\n",
      "INFO:44k:====> Epoch: 3152, cost 16.93 s\n",
      "INFO:44k:====> Epoch: 3153, cost 21.02 s\n",
      "INFO:44k:====> Epoch: 3154, cost 19.17 s\n",
      "INFO:44k:====> Epoch: 3155, cost 19.21 s\n",
      "INFO:44k:====> Epoch: 3156, cost 19.16 s\n",
      "INFO:44k:====> Epoch: 3157, cost 18.19 s\n",
      "INFO:44k:====> Epoch: 3158, cost 16.98 s\n",
      "INFO:44k:====> Epoch: 3159, cost 16.99 s\n",
      "INFO:44k:====> Epoch: 3160, cost 16.79 s\n",
      "INFO:44k:====> Epoch: 3161, cost 17.08 s\n",
      "INFO:44k:====> Epoch: 3162, cost 17.50 s\n",
      "INFO:44k:====> Epoch: 3163, cost 17.50 s\n",
      "INFO:44k:Train Epoch: 3164 [64%]\n",
      "INFO:44k:Losses: [1.8604891300201416, 3.0552260875701904, 15.76782512664795, 14.606938362121582, 0.7846009731292725], step: 34800, lr: 6.734108143912365e-05\n",
      "INFO:44k:====> Epoch: 3164, cost 21.92 s\n",
      "INFO:44k:====> Epoch: 3165, cost 18.78 s\n",
      "INFO:44k:====> Epoch: 3166, cost 18.90 s\n",
      "INFO:44k:====> Epoch: 3167, cost 18.94 s\n",
      "INFO:44k:====> Epoch: 3168, cost 17.27 s\n",
      "INFO:44k:====> Epoch: 3169, cost 17.11 s\n",
      "INFO:44k:====> Epoch: 3170, cost 16.91 s\n",
      "INFO:44k:====> Epoch: 3171, cost 16.86 s\n",
      "INFO:44k:====> Epoch: 3172, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 3173, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 3174, cost 17.72 s\n",
      "INFO:44k:====> Epoch: 3175, cost 21.12 s\n",
      "INFO:44k:====> Epoch: 3176, cost 19.76 s\n",
      "INFO:44k:====> Epoch: 3177, cost 18.87 s\n",
      "INFO:44k:====> Epoch: 3178, cost 18.99 s\n",
      "INFO:44k:====> Epoch: 3179, cost 17.07 s\n",
      "INFO:44k:====> Epoch: 3180, cost 17.13 s\n",
      "INFO:44k:====> Epoch: 3181, cost 16.82 s\n",
      "INFO:44k:Train Epoch: 3182 [82%]\n",
      "INFO:44k:Losses: [1.9529203176498413, 2.772857904434204, 17.007976531982422, 17.36073875427246, 0.7674940824508667], step: 35000, lr: 6.718972488588385e-05\n",
      "INFO:44k:====> Epoch: 3182, cost 18.25 s\n",
      "INFO:44k:====> Epoch: 3183, cost 16.98 s\n",
      "INFO:44k:====> Epoch: 3184, cost 17.21 s\n",
      "INFO:44k:====> Epoch: 3185, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 3186, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 3187, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 3188, cost 16.19 s\n",
      "INFO:44k:====> Epoch: 3189, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 3190, cost 16.13 s\n",
      "INFO:44k:====> Epoch: 3191, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 3192, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 3193, cost 21.05 s\n",
      "INFO:44k:====> Epoch: 3194, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 3195, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 3196, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 3197, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 3198, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 3199, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 3200, cost 16.16 s\n",
      "INFO:44k:Train Epoch: 3201 [0%]\n",
      "INFO:44k:Losses: [1.4324653148651123, 3.480741500854492, 24.2632999420166, 15.268242835998535, 0.7804574370384216], step: 35200, lr: 6.703032868472774e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 3201 to ./logs/44k/G_35200.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 3201 to ./logs/44k/D_35200.pth\n",
      "INFO:44k:====> Epoch: 3201, cost 27.35 s\n",
      "INFO:44k:====> Epoch: 3202, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 3203, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 3204, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 3205, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 3206, cost 16.86 s\n",
      "INFO:44k:====> Epoch: 3207, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 3208, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 3209, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 3210, cost 16.19 s\n",
      "INFO:44k:====> Epoch: 3211, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 3212, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 3213, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 3214, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 3215, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 3216, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 3217, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 3218, cost 16.40 s\n",
      "INFO:44k:Train Epoch: 3219 [18%]\n",
      "INFO:44k:Losses: [2.012049436569214, 3.0505874156951904, 16.8599796295166, 14.932954788208008, 0.34326502680778503], step: 35400, lr: 6.687967058278703e-05\n",
      "INFO:44k:====> Epoch: 3219, cost 17.28 s\n",
      "INFO:44k:====> Epoch: 3220, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 3221, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 3222, cost 16.79 s\n",
      "INFO:44k:====> Epoch: 3223, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 3224, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 3225, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 3226, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 3227, cost 16.16 s\n",
      "INFO:44k:====> Epoch: 3228, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 3229, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 3230, cost 16.85 s\n",
      "INFO:44k:====> Epoch: 3231, cost 16.12 s\n",
      "INFO:44k:====> Epoch: 3232, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 3233, cost 16.20 s\n",
      "INFO:44k:====> Epoch: 3234, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 3235, cost 16.15 s\n",
      "INFO:44k:====> Epoch: 3236, cost 16.06 s\n",
      "INFO:44k:Train Epoch: 3237 [36%]\n",
      "INFO:44k:Losses: [1.7951033115386963, 3.5074424743652344, 14.811914443969727, 13.91331958770752, 1.1502711772918701], step: 35600, lr: 6.672935110164868e-05\n",
      "INFO:44k:====> Epoch: 3237, cost 17.44 s\n",
      "INFO:44k:====> Epoch: 3238, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 3239, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 3240, cost 16.14 s\n",
      "INFO:44k:====> Epoch: 3241, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 3242, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 3243, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 3244, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 3245, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 3246, cost 16.05 s\n",
      "INFO:44k:====> Epoch: 3247, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 3248, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 3249, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 3250, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 3251, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 3252, cost 16.19 s\n",
      "INFO:44k:====> Epoch: 3253, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 3254, cost 16.55 s\n",
      "INFO:44k:Train Epoch: 3255 [55%]\n",
      "INFO:44k:Losses: [2.1580028533935547, 2.883613109588623, 17.96299171447754, 18.398921966552734, 0.6213428974151611], step: 35800, lr: 6.657936948022482e-05\n",
      "INFO:44k:====> Epoch: 3255, cost 17.41 s\n",
      "INFO:44k:====> Epoch: 3256, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 3257, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 3258, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 3259, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 3260, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 3261, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 3262, cost 16.19 s\n",
      "INFO:44k:====> Epoch: 3263, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 3264, cost 16.10 s\n",
      "INFO:44k:====> Epoch: 3265, cost 16.10 s\n",
      "INFO:44k:====> Epoch: 3266, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 3267, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 3268, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 3269, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 3270, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 3271, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 3272, cost 16.57 s\n",
      "INFO:44k:Train Epoch: 3273 [73%]\n",
      "INFO:44k:Losses: [2.055079460144043, 2.931137800216675, 14.006379127502441, 13.121164321899414, 0.6145986318588257], step: 36000, lr: 6.642972495913828e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 3273 to ./logs/44k/G_36000.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 3273 to ./logs/44k/D_36000.pth\n",
      "INFO:44k:====> Epoch: 3273, cost 27.61 s\n",
      "INFO:44k:====> Epoch: 3274, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 3275, cost 17.08 s\n",
      "INFO:44k:====> Epoch: 3276, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 3277, cost 16.77 s\n",
      "INFO:44k:====> Epoch: 3278, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 3279, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 3280, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 3281, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 3282, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 3283, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 3284, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 3285, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 3286, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 3287, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 3288, cost 16.21 s\n",
      "INFO:44k:====> Epoch: 3289, cost 16.79 s\n",
      "INFO:44k:====> Epoch: 3290, cost 16.46 s\n",
      "INFO:44k:Train Epoch: 3291 [91%]\n",
      "INFO:44k:Losses: [1.790686011314392, 3.0205512046813965, 17.194690704345703, 13.153695106506348, 0.4545779526233673], step: 36200, lr: 6.628041678071863e-05\n",
      "INFO:44k:====> Epoch: 3291, cost 17.63 s\n",
      "INFO:44k:====> Epoch: 3292, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 3293, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 3294, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 3295, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 3296, cost 16.12 s\n",
      "INFO:44k:====> Epoch: 3297, cost 16.13 s\n",
      "INFO:44k:====> Epoch: 3298, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 3299, cost 16.91 s\n",
      "INFO:44k:====> Epoch: 3300, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 3301, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 3302, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 3303, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 3304, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 3305, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 3306, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 3307, cost 16.81 s\n",
      "INFO:44k:====> Epoch: 3308, cost 16.19 s\n",
      "INFO:44k:====> Epoch: 3309, cost 16.18 s\n",
      "INFO:44k:Train Epoch: 3310 [9%]\n",
      "INFO:44k:Losses: [2.114183187484741, 2.5661447048187256, 7.213949203491211, 7.349850654602051, 0.7653732299804688], step: 36400, lr: 6.612317775847476e-05\n",
      "INFO:44k:====> Epoch: 3310, cost 17.58 s\n",
      "INFO:44k:====> Epoch: 3311, cost 16.16 s\n",
      "INFO:44k:====> Epoch: 3312, cost 16.87 s\n",
      "INFO:44k:====> Epoch: 3313, cost 16.16 s\n",
      "INFO:44k:====> Epoch: 3314, cost 16.84 s\n",
      "INFO:44k:====> Epoch: 3315, cost 16.69 s\n",
      "INFO:44k:====> Epoch: 3316, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 3317, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 3318, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 3319, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 3320, cost 16.10 s\n",
      "INFO:44k:====> Epoch: 3321, cost 16.79 s\n",
      "INFO:44k:====> Epoch: 3322, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 3323, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 3324, cost 16.13 s\n",
      "INFO:44k:====> Epoch: 3325, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 3326, cost 16.16 s\n",
      "INFO:44k:====> Epoch: 3327, cost 16.29 s\n",
      "INFO:44k:Train Epoch: 3328 [27%]\n",
      "INFO:44k:Losses: [1.5708138942718506, 3.6186575889587402, 17.3949031829834, 13.572341918945312, 0.2472325563430786], step: 36600, lr: 6.597455857890554e-05\n",
      "INFO:44k:====> Epoch: 3328, cost 17.77 s\n",
      "INFO:44k:====> Epoch: 3329, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 3330, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 3331, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 3332, cost 16.09 s\n",
      "INFO:44k:====> Epoch: 3333, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 3334, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 3335, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 3336, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 3337, cost 16.08 s\n",
      "INFO:44k:====> Epoch: 3338, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 3339, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 3340, cost 16.06 s\n",
      "INFO:44k:====> Epoch: 3341, cost 15.95 s\n",
      "INFO:44k:====> Epoch: 3342, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 3343, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 3344, cost 16.10 s\n",
      "INFO:44k:====> Epoch: 3345, cost 16.07 s\n",
      "INFO:44k:Train Epoch: 3346 [45%]\n",
      "INFO:44k:Losses: [1.8023781776428223, 3.239605665206909, 18.30508804321289, 15.05548095703125, 0.2271193563938141], step: 36800, lr: 6.58262734374344e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 3346 to ./logs/44k/G_36800.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 3346 to ./logs/44k/D_36800.pth\n",
      "INFO:44k:====> Epoch: 3346, cost 27.28 s\n",
      "INFO:44k:====> Epoch: 3347, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 3348, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 3349, cost 16.11 s\n",
      "INFO:44k:====> Epoch: 3350, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 3351, cost 16.17 s\n",
      "INFO:44k:====> Epoch: 3352, cost 16.82 s\n",
      "INFO:44k:====> Epoch: 3353, cost 16.80 s\n",
      "INFO:44k:====> Epoch: 3354, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 3355, cost 16.16 s\n",
      "INFO:44k:====> Epoch: 3356, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 3357, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 3358, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 3359, cost 16.53 s\n",
      "INFO:44k:====> Epoch: 3360, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 3361, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 3362, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 3363, cost 16.19 s\n",
      "INFO:44k:Train Epoch: 3364 [64%]\n",
      "INFO:44k:Losses: [1.9099984169006348, 3.1836280822753906, 18.740779876708984, 16.70484161376953, 0.5148957371711731], step: 37000, lr: 6.567832158327362e-05\n",
      "INFO:44k:====> Epoch: 3364, cost 17.61 s\n",
      "INFO:44k:====> Epoch: 3365, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 3366, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 3367, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 3368, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 3369, cost 17.04 s\n",
      "INFO:44k:====> Epoch: 3370, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 3371, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 3372, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 3373, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 3374, cost 16.10 s\n",
      "INFO:44k:====> Epoch: 3375, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 3376, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 3377, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 3378, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 3379, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 3380, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 3381, cost 16.31 s\n",
      "INFO:44k:Train Epoch: 3382 [82%]\n",
      "INFO:44k:Losses: [1.8759053945541382, 3.554208278656006, 14.478429794311523, 15.171842575073242, 0.609167218208313], step: 37200, lr: 6.553070226732298e-05\n",
      "INFO:44k:====> Epoch: 3382, cost 17.16 s\n",
      "INFO:44k:====> Epoch: 3383, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 3384, cost 16.16 s\n",
      "INFO:44k:====> Epoch: 3385, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 3386, cost 17.08 s\n",
      "INFO:44k:====> Epoch: 3387, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 3388, cost 16.01 s\n",
      "INFO:44k:====> Epoch: 3389, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 3390, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 3391, cost 16.16 s\n",
      "INFO:44k:====> Epoch: 3392, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 3393, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 3394, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 3395, cost 16.16 s\n",
      "INFO:44k:====> Epoch: 3396, cost 16.81 s\n",
      "INFO:44k:====> Epoch: 3397, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 3398, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 3399, cost 16.68 s\n",
      "INFO:44k:====> Epoch: 3400, cost 16.49 s\n",
      "INFO:44k:Train Epoch: 3401 [0%]\n",
      "INFO:44k:Losses: [1.4419399499893188, 3.3139381408691406, 17.147716522216797, 14.279789924621582, 0.5464171767234802], step: 37400, lr: 6.537524181532318e-05\n",
      "INFO:44k:====> Epoch: 3401, cost 17.78 s\n",
      "INFO:44k:====> Epoch: 3402, cost 21.34 s\n",
      "INFO:44k:====> Epoch: 3403, cost 19.21 s\n",
      "INFO:44k:====> Epoch: 3404, cost 18.80 s\n",
      "INFO:44k:====> Epoch: 3405, cost 18.66 s\n",
      "INFO:44k:====> Epoch: 3406, cost 18.30 s\n",
      "INFO:44k:====> Epoch: 3407, cost 16.96 s\n",
      "INFO:44k:====> Epoch: 3408, cost 17.29 s\n",
      "INFO:44k:====> Epoch: 3409, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 3410, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 3411, cost 16.84 s\n",
      "INFO:44k:====> Epoch: 3412, cost 17.89 s\n",
      "INFO:44k:====> Epoch: 3413, cost 21.38 s\n",
      "INFO:44k:====> Epoch: 3414, cost 19.66 s\n",
      "INFO:44k:====> Epoch: 3415, cost 19.35 s\n",
      "INFO:44k:====> Epoch: 3416, cost 19.10 s\n",
      "INFO:44k:====> Epoch: 3417, cost 16.95 s\n",
      "INFO:44k:====> Epoch: 3418, cost 17.01 s\n",
      "INFO:44k:Train Epoch: 3419 [18%]\n",
      "INFO:44k:Losses: [2.052165985107422, 2.9694325923919678, 22.509153366088867, 16.61577033996582, 0.7071566581726074], step: 37600, lr: 6.522830370478316e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 3419 to ./logs/44k/G_37600.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 3419 to ./logs/44k/D_37600.pth\n",
      "INFO:44k:====> Epoch: 3419, cost 27.87 s\n",
      "INFO:44k:====> Epoch: 3420, cost 16.87 s\n",
      "INFO:44k:====> Epoch: 3421, cost 17.05 s\n",
      "INFO:44k:====> Epoch: 3422, cost 17.20 s\n",
      "INFO:44k:====> Epoch: 3423, cost 21.31 s\n",
      "INFO:44k:====> Epoch: 3424, cost 18.94 s\n",
      "INFO:44k:====> Epoch: 3425, cost 18.73 s\n",
      "INFO:44k:====> Epoch: 3426, cost 18.90 s\n",
      "INFO:44k:====> Epoch: 3427, cost 18.14 s\n",
      "INFO:44k:====> Epoch: 3428, cost 17.59 s\n",
      "INFO:44k:====> Epoch: 3429, cost 17.32 s\n",
      "INFO:44k:====> Epoch: 3430, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 3431, cost 16.79 s\n",
      "INFO:44k:====> Epoch: 3432, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 3433, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 3434, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 3435, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 3436, cost 16.13 s\n",
      "INFO:44k:Train Epoch: 3437 [36%]\n",
      "INFO:44k:Losses: [2.073909282684326, 3.211594343185425, 14.894599914550781, 12.534650802612305, 0.7312037348747253], step: 37800, lr: 6.508169585395201e-05\n",
      "INFO:44k:====> Epoch: 3437, cost 17.73 s\n",
      "INFO:44k:====> Epoch: 3438, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 3439, cost 16.38 s\n",
      "INFO:44k:====> Epoch: 3440, cost 16.14 s\n",
      "INFO:44k:====> Epoch: 3441, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 3442, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 3443, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 3444, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 3445, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 3446, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 3447, cost 16.08 s\n",
      "INFO:44k:====> Epoch: 3448, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 3449, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 3450, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 3451, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 3452, cost 16.09 s\n",
      "INFO:44k:====> Epoch: 3453, cost 16.42 s\n",
      "INFO:44k:====> Epoch: 3454, cost 16.37 s\n",
      "INFO:44k:Train Epoch: 3455 [55%]\n",
      "INFO:44k:Losses: [1.9544117450714111, 3.164639472961426, 12.6126708984375, 11.431253433227539, 0.47776922583580017], step: 38000, lr: 6.493541752053436e-05\n",
      "INFO:44k:====> Epoch: 3455, cost 17.14 s\n",
      "INFO:44k:====> Epoch: 3456, cost 16.19 s\n",
      "INFO:44k:====> Epoch: 3457, cost 16.13 s\n",
      "INFO:44k:====> Epoch: 3458, cost 16.02 s\n",
      "INFO:44k:====> Epoch: 3459, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 3460, cost 16.03 s\n",
      "INFO:44k:====> Epoch: 3461, cost 16.37 s\n",
      "INFO:44k:====> Epoch: 3462, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 3463, cost 16.08 s\n",
      "INFO:44k:====> Epoch: 3464, cost 16.18 s\n",
      "INFO:44k:====> Epoch: 3465, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 3466, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 3467, cost 16.78 s\n",
      "INFO:44k:====> Epoch: 3468, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 3469, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 3470, cost 16.58 s\n",
      "INFO:44k:====> Epoch: 3471, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 3472, cost 16.28 s\n",
      "INFO:44k:Train Epoch: 3473 [73%]\n",
      "INFO:44k:Losses: [2.026822566986084, 3.0837693214416504, 15.07344913482666, 10.694572448730469, 0.5667798519134521], step: 38200, lr: 6.478946796390328e-05\n",
      "INFO:44k:====> Epoch: 3473, cost 17.54 s\n",
      "INFO:44k:====> Epoch: 3474, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 3475, cost 16.32 s\n",
      "INFO:44k:====> Epoch: 3476, cost 16.22 s\n",
      "INFO:44k:====> Epoch: 3477, cost 16.04 s\n",
      "INFO:44k:====> Epoch: 3478, cost 16.17 s\n",
      "INFO:44k:====> Epoch: 3479, cost 16.31 s\n",
      "INFO:44k:====> Epoch: 3480, cost 16.57 s\n",
      "INFO:44k:====> Epoch: 3481, cost 16.41 s\n",
      "INFO:44k:====> Epoch: 3482, cost 16.50 s\n",
      "INFO:44k:====> Epoch: 3483, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 3484, cost 16.25 s\n",
      "INFO:44k:====> Epoch: 3485, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 3486, cost 15.99 s\n",
      "INFO:44k:====> Epoch: 3487, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 3488, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 3489, cost 16.60 s\n",
      "INFO:44k:====> Epoch: 3490, cost 16.39 s\n",
      "INFO:44k:Train Epoch: 3491 [91%]\n",
      "INFO:44k:Losses: [1.6989160776138306, 3.561835289001465, 15.570475578308105, 12.518266677856445, 0.49512138962745667], step: 38400, lr: 6.464384644509648e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 3491 to ./logs/44k/G_38400.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 3491 to ./logs/44k/D_38400.pth\n",
      "INFO:44k:====> Epoch: 3491, cost 28.14 s\n",
      "INFO:44k:====> Epoch: 3492, cost 16.24 s\n",
      "INFO:44k:====> Epoch: 3493, cost 16.33 s\n",
      "INFO:44k:====> Epoch: 3494, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 3495, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 3496, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 3497, cost 16.11 s\n",
      "INFO:44k:====> Epoch: 3498, cost 16.39 s\n",
      "INFO:44k:====> Epoch: 3499, cost 16.20 s\n",
      "INFO:44k:====> Epoch: 3500, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 3501, cost 16.05 s\n",
      "INFO:44k:====> Epoch: 3502, cost 16.13 s\n",
      "INFO:44k:====> Epoch: 3503, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 3504, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 3505, cost 16.59 s\n",
      "INFO:44k:====> Epoch: 3506, cost 16.29 s\n",
      "INFO:44k:====> Epoch: 3507, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 3508, cost 16.35 s\n",
      "INFO:44k:====> Epoch: 3509, cost 16.25 s\n",
      "INFO:44k:Train Epoch: 3510 [9%]\n",
      "INFO:44k:Losses: [2.3721251487731934, 2.7362558841705322, 14.2462158203125, 13.539173126220703, 0.4458775520324707], step: 38600, lr: 6.449048990778414e-05\n",
      "INFO:44k:====> Epoch: 3510, cost 17.93 s\n",
      "INFO:44k:====> Epoch: 3511, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 3512, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 3513, cost 16.62 s\n",
      "INFO:44k:====> Epoch: 3514, cost 16.23 s\n",
      "INFO:44k:====> Epoch: 3515, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 3516, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 3517, cost 16.27 s\n",
      "INFO:44k:====> Epoch: 3518, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 3519, cost 16.61 s\n",
      "INFO:44k:====> Epoch: 3520, cost 16.46 s\n",
      "INFO:44k:====> Epoch: 3521, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 3522, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 3523, cost 16.63 s\n",
      "INFO:44k:====> Epoch: 3524, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 3525, cost 16.52 s\n",
      "INFO:44k:====> Epoch: 3526, cost 16.75 s\n",
      "INFO:44k:====> Epoch: 3527, cost 16.33 s\n",
      "INFO:44k:Train Epoch: 3528 [27%]\n",
      "INFO:44k:Losses: [1.7106444835662842, 3.2522010803222656, 16.204105377197266, 11.757555961608887, 0.8757127523422241], step: 38800, lr: 6.434554037533547e-05\n",
      "INFO:44k:====> Epoch: 3528, cost 17.66 s\n",
      "INFO:44k:====> Epoch: 3529, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 3530, cost 16.26 s\n",
      "INFO:44k:====> Epoch: 3531, cost 16.34 s\n",
      "INFO:44k:====> Epoch: 3532, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 3533, cost 16.93 s\n",
      "INFO:44k:====> Epoch: 3534, cost 16.44 s\n",
      "INFO:44k:====> Epoch: 3535, cost 16.51 s\n",
      "INFO:44k:====> Epoch: 3536, cost 16.66 s\n",
      "INFO:44k:====> Epoch: 3537, cost 16.65 s\n",
      "INFO:44k:====> Epoch: 3538, cost 16.64 s\n",
      "INFO:44k:====> Epoch: 3539, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 3540, cost 16.40 s\n",
      "INFO:44k:====> Epoch: 3541, cost 16.43 s\n",
      "INFO:44k:====> Epoch: 3542, cost 16.73 s\n",
      "INFO:44k:====> Epoch: 3543, cost 16.49 s\n",
      "INFO:44k:====> Epoch: 3544, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 3545, cost 16.76 s\n",
      "INFO:44k:Train Epoch: 3546 [45%]\n",
      "INFO:44k:Losses: [1.6572867631912231, 3.4285941123962402, 19.490650177001953, 13.53592586517334, 0.7236233949661255], step: 39000, lr: 6.420091663304574e-05\n",
      "INFO:44k:====> Epoch: 3546, cost 17.32 s\n",
      "INFO:44k:====> Epoch: 3547, cost 16.48 s\n",
      "INFO:44k:====> Epoch: 3548, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 3549, cost 16.54 s\n",
      "INFO:44k:====> Epoch: 3550, cost 16.70 s\n",
      "INFO:44k:====> Epoch: 3551, cost 16.28 s\n",
      "INFO:44k:====> Epoch: 3552, cost 16.97 s\n",
      "INFO:44k:====> Epoch: 3553, cost 16.45 s\n",
      "INFO:44k:====> Epoch: 3554, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 3555, cost 16.30 s\n",
      "INFO:44k:====> Epoch: 3556, cost 16.36 s\n",
      "INFO:44k:====> Epoch: 3557, cost 16.67 s\n",
      "INFO:44k:====> Epoch: 3558, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 3559, cost 16.47 s\n",
      "INFO:44k:====> Epoch: 3560, cost 16.85 s\n",
      "INFO:44k:====> Epoch: 3561, cost 16.55 s\n",
      "INFO:44k:====> Epoch: 3562, cost 16.56 s\n",
      "INFO:44k:====> Epoch: 3563, cost 16.52 s\n",
      "INFO:44k:Train Epoch: 3564 [64%]\n",
      "INFO:44k:Losses: [1.9301252365112305, 3.09128737449646, 18.795799255371094, 16.293066024780273, 0.6034507155418396], step: 39200, lr: 6.405661794866542e-05\n",
      "INFO:44k:Saving model and optimizer state at iteration 3564 to ./logs/44k/G_39200.pth\n",
      "INFO:44k:Saving model and optimizer state at iteration 3564 to ./logs/44k/D_39200.pth\n",
      "INFO:44k:====> Epoch: 3564, cost 27.35 s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/so-vits-svc/train.py\", line 310, in <module>\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    main()\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/root/so-vits-svc/train.py\", line 51, in main\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 125, in _main\n",
      "    mp.spawn(run, nprocs=n_gpus, args=(n_gpus, hps,))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 239, in spawn\n",
      "    prepare(preparation_data)\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 236, in prepare\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 197, in start_processes\n",
      "Traceback (most recent call last):\n",
      "    while not context.join():\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 109, in join\n",
      "    ready = multiprocessing.connection.wait(\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 125, in _main\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n",
      "    prepare(preparation_data)\n",
      "Error in generated code:\n",
      "    main_content = runpy.run_path(main_path,\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 236, in prepare\n",
      "def cgs(A, b, x0, tol, maxiter, M, callback, atol):\n",
      "    return _call_(_func_, A, b, x0, tol, maxiter, M, callback, atol)\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 289, in run_path\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "\n",
      "KeyboardInterrupt\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n",
      "    main_content = runpy.run_path(main_path,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 289, in run_path\n",
      "    return _run_module_code(code, init_globals, run_name,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 96, in _run_module_code\n",
      "    return _run_module_code(code, init_globals, run_name,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 96, in _run_module_code\n",
      "    _run_code(code, mod_globals, init_globals,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    _run_code(code, mod_globals, init_globals,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exec(code, run_globals)\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/so-vits-svc/train.py\", line 22, in <module>\n",
      "  File \"/root/so-vits-svc/train.py\", line 22, in <module>\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 125, in _main\n",
      "    import utils\n",
      "  File \"/root/so-vits-svc/utils.py\", line 11, in <module>\n",
      "    import utils\n",
      "  File \"/root/so-vits-svc/utils.py\", line 11, in <module>\n",
      "    prepare(preparation_data)\n",
      "    import librosa\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/__init__.py\", line 209, in <module>\n",
      "    import librosa\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/__init__.py\", line 208, in <module>\n",
      "    from . import core\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/core/__init__.py\", line 5, in <module>\n",
      "    from .convert import *  # pylint: disable=wildcard-import\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/core/convert.py\", line 7, in <module>\n",
      "    from . import notation\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/core/notation.py\", line 8, in <module>\n",
      "    from ._cache import cache\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/_cache.py\", line 6, in <module>\n",
      "    from ..util.exceptions import ParameterError\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/util/__init__.py\", line 77, in <module>\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 236, in prepare\n",
      "    from .utils import *  # pylint: disable=wildcard-import\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/util/utils.py\", line 5, in <module>\n",
      "    from joblib import Memory\n",
      "    import scipy.ndimage\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/joblib/__init__.py\", line 120, in <module>\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/ndimage/__init__.py\", line 154, in <module>\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n",
      "    from .parallel import Parallel\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\", line 26, in <module>\n",
      "    from ._interpolation import *  # noqa: F401 F403\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/ndimage/_interpolation.py\", line 37, in <module>\n",
      "    main_content = runpy.run_path(main_path,\n",
      "    from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n",
      "    from scipy import special\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/joblib/_parallel_backends.py\", line 13, in <module>\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 289, in run_path\n",
      "    from .my_exceptions import WorkerInterrupt\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/joblib/my_exceptions.py\", line 3, in <module>\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/__init__.py\", line 200, in __getattr__\n",
      "    from . import _deprecated_my_exceptions\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/joblib/_deprecated_my_exceptions.py\", line 115, in <module>\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _run_module_code(code, init_globals, run_name,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 96, in _run_module_code\n",
      "    locals().update(_mk_common_exceptions())\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/joblib/_deprecated_my_exceptions.py\", line 108, in _mk_common_exceptions\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/special/__init__.py\", line 663, in <module>\n",
      "    _run_code(code, mod_globals, init_globals,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    this_obj, this_name = _mk_exception(obj, name=name)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/joblib/_deprecated_my_exceptions.py\", line 85, in _mk_exception\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/so-vits-svc/train.py\", line 22, in <module>\n",
      "    this_exception = type(\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "    import utils\n",
      "    from . import _ufuncs\n",
      "  File \"/root/so-vits-svc/utils.py\", line 11, in <module>\n",
      "  File \"scipy/special/_ufuncs.pyx\", line 1, in init scipy.special._ufuncs\n",
      "    import librosa\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/__init__.py\", line 209, in <module>\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 945, in _find_spec\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    from . import core\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1439, in find_spec\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 125, in _main\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/core/__init__.py\", line 5, in <module>\n",
      "    from .convert import *  # pylint: disable=wildcard-import\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1411, in _get_spec\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/core/convert.py\", line 7, in <module>\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1544, in find_spec\n",
      "    from . import notation\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/core/notation.py\", line 8, in <module>\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 147, in _path_stat\n",
      "KeyboardInterrupt\n",
      "    from ..util.exceptions import ParameterError\n",
      "    prepare(preparation_data)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 236, in prepare\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/util/__init__.py\", line 77, in <module>\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "    from .utils import *  # pylint: disable=wildcard-import\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/util/utils.py\", line 5, in <module>\n",
      "    import scipy.ndimage\n",
      "    main_content = runpy.run_path(main_path,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/ndimage/__init__.py\", line 154, in <module>\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 289, in run_path\n",
      "    from ._interpolation import *  # noqa: F401 F403\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/ndimage/_interpolation.py\", line 37, in <module>\n",
      "    return _run_module_code(code, init_globals, run_name,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 96, in _run_module_code\n",
      "    from scipy import special\n",
      "    _run_code(code, mod_globals, init_globals,\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/__init__.py\", line 200, in __getattr__\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/so-vits-svc/train.py\", line 22, in <module>\n",
      "    import utils\n",
      "  File \"/root/so-vits-svc/utils.py\", line 11, in <module>\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    import librosa\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/__init__.py\", line 209, in <module>\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/special/__init__.py\", line 671, in <module>\n",
      "    from . import core\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/core/__init__.py\", line 5, in <module>\n",
      "    from .convert import *  # pylint: disable=wildcard-import\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/core/convert.py\", line 7, in <module>\n",
      "    from . import notation\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/core/notation.py\", line 8, in <module>\n",
      "    from ..util.exceptions import ParameterError\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/util/__init__.py\", line 77, in <module>\n",
      "    from . import _orthogonal\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/special/_orthogonal.py\", line 81, in <module>\n",
      "    from .utils import *  # pylint: disable=wildcard-import\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/util/utils.py\", line 5, in <module>\n",
      "    import scipy.ndimage\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/ndimage/__init__.py\", line 154, in <module>\n",
      "    from ._interpolation import *  # noqa: F401 F403\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/ndimage/_interpolation.py\", line 37, in <module>\n",
      "    from scipy import special\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/__init__.py\", line 200, in __getattr__\n",
      "Traceback (most recent call last):\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "  File \"<string>\", line 1, in <module>\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/special/__init__.py\", line 671, in <module>\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    from . import _orthogonal\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/special/_orthogonal.py\", line 81, in <module>\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 125, in _main\n",
      "    from scipy import linalg\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/__init__.py\", line 200, in __getattr__\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "    prepare(preparation_data)\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 236, in prepare\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/__init__.py\", line 208, in <module>\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "    from ._decomp_polar import *\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n",
      "  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 945, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1439, in find_spec\n",
      "    from scipy import linalg\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1411, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1572, in find_spec\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/__init__.py\", line 200, in __getattr__\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 128, in _path_join\n",
      "KeyboardInterrupt\n",
      "    main_content = runpy.run_path(main_path,\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 289, in run_path\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/__init__.py\", line 216, in <module>\n",
      "    return _run_module_code(code, init_globals, run_name,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 96, in _run_module_code\n",
      "    _run_code(code, mod_globals, init_globals,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/so-vits-svc/train.py\", line 22, in <module>\n",
      "    import utils\n",
      "  File \"/root/so-vits-svc/utils.py\", line 11, in <module>\n",
      "    import librosa\n",
      "    from ._sketches import *\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/__init__.py\", line 208, in <module>\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_sketches.py\", line 9, in <module>\n",
      "    from ._cache import cache\n",
      "    from scipy.sparse import csc_matrix\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/__init__.py\", line 283, in <module>\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/librosa/_cache.py\", line 6, in <module>\n",
      "    from joblib import Memory\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/joblib/__init__.py\", line 120, in <module>\n",
      "    from .parallel import Parallel\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\", line 26, in <module>\n",
      "    from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n",
      "    from . import csgraph\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/joblib/_parallel_backends.py\", line 17, in <module>\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/csgraph/__init__.py\", line 185, in <module>\n",
      "    from .pool import MemmappingPool\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/joblib/pool.py\", line 31, in <module>\n",
      "    from ._memmapping_reducer import get_memmapping_reducers\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/joblib/_memmapping_reducer.py\", line 37, in <module>\n",
      "    from .externals.loky.backend import resource_tracker\n",
      "    from ._laplacian import laplacian\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/__init__.py\", line 18, in <module>\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/csgraph/_laplacian.py\", line 7, in <module>\n",
      "    from .backend.context import cpu_count\n",
      "    from scipy.sparse.linalg import LinearOperator\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/__init__.py\", line 120, in <module>\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/__init__.py\", line 2, in <module>\n",
      "    from ._isolve import *\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/_isolve/__init__.py\", line 4, in <module>\n",
      "    from multiprocessing import synchronize\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "    from .iterative import *\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/_isolve/iterative.py\", line 416, in <module>\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1012, in get_code\n",
      "    def cgs(A, b, x0=None, tol=1e-5, maxiter=None, M=None, callback=None, atol=None):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/_lib/_threadsafety.py\", line 57, in decorator\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 672, in _compile_bytecode\n",
      "    return lock.decorate(func)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/_lib/_threadsafety.py\", line 45, in decorate\n",
      "KeyboardInterrupt\n",
      "    return scipy._lib.decorator.decorate(func, caller)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/_lib/decorator.py\", line 207, in decorate\n",
      "    fun = FunctionMaker.create(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/_lib/decorator.py\", line 196, in create\n",
      "    return self.make('def %(name)s(%(signature)s):\\n' + ibody,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scipy/_lib/decorator.py\", line 165, in make\n",
      "    code = compile(src, filename, 'single')\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py -c configs/config.json -m 44k"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
